{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type Markdown and LaTeX:  𝛼^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import modules.\n",
    "import os, random, string, zipfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from urllib.request import urlretrieve\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_root = './dataset/'\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "class TqdmUpTo(tqdm):\n",
    "    def update_to(self, count=1, blockSize=1, totalSize=None):\n",
    "        if totalSize is not None:\n",
    "            self.total = totalSize\n",
    "        # It will also set self.n = count * blockSize\n",
    "        self.update(count * blockSize - self.n)\n",
    "\n",
    "def download_file(filename, expected_bytes=None, force=False):\n",
    "    dest_filename = os.path.join(data_root, filename)\n",
    "    if force or not os.path.exists(dest_filename):\n",
    "        print('Download: %s' % filename)\n",
    "        with TqdmUpTo(unit='B', unit_scale=True, unit_divisor=1024, miniters=1) as t:\n",
    "            dest_filename, _ = urlretrieve(url+filename, dest_filename,\n",
    "                                 reporthook=t.update_to, data=None)\n",
    "        print('\\n%s Download Complete!' % filename)\n",
    "        \n",
    "    if expected_bytes:\n",
    "        statinfo = os.stat(dest_filename)\n",
    "        not_expected_bytes_error = 'Failed to verify ' + dest_filename + '. Can you get to it with a browser?'\n",
    "        assert statinfo.st_size == expected_bytes, not_expected_bytes_error\n",
    "        \n",
    "    return dest_filename\n",
    "\n",
    "file = download_file('text8.zip', force=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_file(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0]),\n",
    "                                encoding='utf-8')\n",
    "    return data\n",
    "\n",
    "text = read_file(file)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "    \n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "[' a']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_unrollings = 10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    \n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "        \n",
    "    def _next_batch(self):\n",
    "        '''Generate a single batch from the current cursor position in the data.'''\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        ''' Generate the next array of batches from the data. The array\n",
    "        consists of the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        '''\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "    \n",
    "def characters(probabilities):\n",
    "    ''' Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\n",
    "    '''\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    ''' \n",
    "    Convert a sequence of batches back into their (most likely) string representation.\n",
    "    '''\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing interpretations of what this means anarchism also refers to related social movements that advocate the elimination of authoritarian institutions particularly the state the word anarchy as most anarchists use it does not imply chaos nihilism or anomie but rather a harmonious anti authoritarian society in place of what are regarded as authoritarian political structures and coercive economic instituti'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 27)\n",
      "1562484\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "print(train_batches.next()[1].shape)\n",
    "print(len(train_text) // batch_size)\n",
    "print(len(string.ascii_lowercase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    ''' Log probability of the true labels in a predicted batch.'''\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    ''' Sample one element from a distribution assumed to be an array of\n",
    "    normalized probabilities.\n",
    "    '''\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    ''' Turn a (column) prediction into 1-hot encoded samples.'''\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    ''' Generate a random column of probability.'''\n",
    "    b = np.random.uniform(0., 1., size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0906 17:39:49.529324 140651331675968 deprecation.py:323] From /home/commaai-03/.local/lib/python3.6/site-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Forget gate:\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Memory cell:\n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Output gate:\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases:\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        ''' Create a LSTM cell. See eg: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates\n",
    "        '''\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]\n",
    "        \n",
    "    # Unrolled LSTM loop\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "        \n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        # tf.nn.xw_plus_b -> y = w*x + b\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        _loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                logits=logits, labels=tf.concat(train_labels, 0)))\n",
    "        \n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    # tf.train.exponential_decay: \n",
    "    # decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)\n",
    "    _learning_rate = tf.train.exponential_decay(10., global_step, 10000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(_learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(_loss))\n",
    "    # tf.clip_by_global_norm: args:(t_list, clip_norm, use_norm=None, name=None)\n",
    "    # t_list[i] = t_list[i] * clip_norm / max(global_norm, clip_norm)\n",
    "    # global_norm = sqrt(sum([l2norm(t)**2 for t in t_list]))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, \n",
    "        saved_sample_output,\n",
    "        saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output), \n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tensorflow]: Initialized!\n",
      "Average loss at step 100: 2.593915 learning_rate: 10.000000\n",
      "Minibatch perplexity: 9.97\n",
      "Validation set perplexity: 11.04\n",
      "Average loss at step 200: 2.247275 learning_rate: 10.000000\n",
      "Minibatch perplexity: 9.17\n",
      "Validation set perplexity: 8.57\n",
      "Average loss at step 300: 2.105411 learning_rate: 10.000000\n",
      "Minibatch perplexity: 7.56\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 400: 2.004295 learning_rate: 10.000000\n",
      "Minibatch perplexity: 7.70\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 500: 1.945324 learning_rate: 10.000000\n",
      "Minibatch perplexity: 6.61\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 600: 1.913403 learning_rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 700: 1.861726 learning_rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 800: 1.823177 learning_rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 900: 1.833401 learning_rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 6.21\n",
      "Average loss at step 1000: 1.831762 learning_rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "--------------------------------------------------------------------------------\n",
      "quiluters see beable that its aidistionss voully to recours zero spotharklagiqua\n",
      "dez center and as nats that an heading lapers one nine nine mermung that lifmoli\n",
      "owhemage cenrusdict hubuil two dityons are ulegity of a saxkensing mplieap i bin\n",
      "utoby severaniv asarfer bozrdy from seven two the cased the lapke apone is us en\n",
      "raly appariations and  from decneplape an ining convunings from one nine seven t\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 1100: 1.779509 learning_rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 1200: 1.760148 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 1300: 1.734659 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1400: 1.749475 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1500: 1.738191 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1600: 1.744848 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1700: 1.712262 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 1800: 1.673837 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 1900: 1.646677 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2000: 1.694894 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "--------------------------------------------------------------------------------\n",
      " for icono to the howen the mosting selveman to alsourcha freft from deefte batt\n",
      "based one three furlations production image on banghborogd ecorn remaired in not\n",
      "ners mea bottic teaps com of in tanners come uts that dachite dinca blshm althou\n",
      "quan also drantdoos varchine acting boff ywark notilian its greate but out helo \n",
      "on wark whia defives footk fab exulters genemence in the new five a browe lakg t\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2100: 1.684105 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 2200: 1.674578 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 2300: 1.642534 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 2400: 1.653614 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 2500: 1.676404 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 2600: 1.650968 learning_rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2700: 1.654314 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2800: 1.647193 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2900: 1.650316 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3000: 1.649043 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "--------------------------------------------------------------------------------\n",
      "verationmentation and mutorey positizar one nine netersturing the ler mays the a\n",
      "cede one nine six one one nine three one five rany on a normed as they apposnds \n",
      "nel lifimer interved the chisce is romedic ficthon existentis six s europ to any\n",
      "d is also coin of the pluaformon pait is use form are nory motter s underrating \n",
      "ment and part with the five wie in the encomblatide ending sciting foch whishet \n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 3100: 1.627643 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3200: 1.645084 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3300: 1.644710 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 3400: 1.664728 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3500: 1.655762 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3600: 1.670650 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3700: 1.646593 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3800: 1.642075 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3900: 1.634842 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4000: 1.647609 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "--------------------------------------------------------------------------------\n",
      "by parifo usiphed internateraf one one two be remained injirate intermal was ter\n",
      "zers crad four hore also ternally extablical ju medurism df list of the fames an\n",
      "zed amourion reported to the nater s agos hekbolm will genetical by goer weeces \n",
      "ersovers that aleants the galanary dour crestenal rogor used also avithous for e\n",
      "drag cleact and the consisite carropioging is chilk hold cence assess secree ita\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4100: 1.630594 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 4200: 1.632577 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4300: 1.613167 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4400: 1.608889 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 4500: 1.616825 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4600: 1.610236 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4700: 1.624129 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4800: 1.631371 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4900: 1.630211 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5000: 1.606523 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "--------------------------------------------------------------------------------\n",
      "jorged of ettion in the lincess in pargal drunse liver bo or almoinder was israe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in mid two three d himsertique yeats in he bowo the slove inge be reakire that t\n",
      "hmener one nine six six on eight six zero zero frop one zero zero zero two zero \n",
      "ges all of perption video s narivania acdider time the carry eventive deation bl\n",
      "scard demee rescrees of similal both a centuen ita in calle palxin their day inv\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5100: 1.613933 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5200: 1.599040 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5300: 1.589987 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5400: 1.588237 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5500: 1.574066 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5600: 1.591878 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5700: 1.578951 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5800: 1.584844 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5900: 1.582354 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6000: 1.558101 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.37\n",
      "--------------------------------------------------------------------------------\n",
      "d of the is pose hardwelly congring cordocreus they english fay its finald that \n",
      "d and two zero obl the leugove four smal be drolement by diftended class usid ca\n",
      "ers rosh first from one the ideut troous theme on dequended the leagualist by a \n",
      "quees servers in endies other other is ggmenf simiets distruding is immentik had\n",
      "etry this late other pdinament and supmay thene one zero four flan systepla rens\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 6100: 1.574405 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 6200: 1.548450 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 6300: 1.554548 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6400: 1.547733 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6500: 1.565977 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6600: 1.606199 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6700: 1.583418 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 6800: 1.606456 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6900: 1.581977 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 7000: 1.571499 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "--------------------------------------------------------------------------------\n",
      "wly two zero zero zero zero six four three eight on ex and the toughtional coppr\n",
      "hige usitures tyte often impubland by presidence s the four populard gould compu\n",
      "x as a four the toners in secular chiet for t produces tiels all the pworegine i\n",
      "by as the clare on which or of every one nine four two one four one brechaule on\n",
      "x by prossions often complective posing heer one jorned mocession and the includ\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 7100: 1.575386 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 7200: 1.573126 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 7300: 1.567543 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 7400: 1.579055 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 7500: 1.589391 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 7600: 1.547762 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.15\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 7700: 1.545745 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 7800: 1.576854 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 7900: 1.579226 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 8000: 1.608246 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "--------------------------------------------------------------------------------\n",
      "che the wllioner avorton of combi s film rather to and five eight and implay the\n",
      "s a one nine this over bagkhish piarxly read fin sterica war was a an local stil\n",
      "ingla wigh unitional involved are s tegen for standarry wollian dictionary as al\n",
      "ly widely unlutt frevh drought a food claiment veadiviel distributism on the gyl\n",
      "shicketory will ver billen stants zero basie bears age one nine three many its e\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 8100: 1.589143 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 8200: 1.561085 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 8300: 1.565598 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 8400: 1.574198 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 8500: 1.574976 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 8600: 1.576668 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 8700: 1.566253 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 8800: 1.542946 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 8900: 1.553593 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 9000: 1.547523 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "--------------------------------------------------------------------------------\n",
      "time leof geer to is public to peoso gave wire wire stalls one four used be aids\n",
      "twentirataly one the parch an experomerd for exestlond one nine nine two zero pe\n",
      "o temporally mecrovent redomare that whale we seen or binnic may r med of the fo\n",
      "je overome by chail where aidfore which abodigir in electility countrie was see \n",
      "toctos for head known it see ruls alal seize by sporce lows of the gaged miks cm\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 9100: 1.561468 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 9200: 1.581745 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.19\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 9300: 1.586662 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 9400: 1.578535 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 9500: 1.581641 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 9600: 1.567673 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.43\n",
      "Average loss at step 9700: 1.583194 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 9800: 1.584433 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 9900: 1.576926 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 10000: 1.597612 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "--------------------------------------------------------------------------------\n",
      "cas was more tillite lernazing hound reach a legid of beth phons cari slands yin\n",
      "war amact creft and time drufings airct japa cufferio fish ot n has were to two \n",
      "hani ackanian four zero five zero zero zero zero zero zero zero zero zero four o\n",
      "ur have ground have ir antical the adm for two five midor began kominaryly and a\n",
      "quare external machile time five ravily air putper mribers at the collets sign a\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 10100: 1.552740 learning_rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 10200: 1.541416 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 10300: 1.552196 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 10400: 1.548118 learning_rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 10500: 1.580231 learning_rate: 1.000000\n",
      "Minibatch perplexity: 3.98\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 10600: 1.561753 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 10700: 1.562445 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 10800: 1.565809 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 10900: 1.567944 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 11000: 1.557061 learning_rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "--------------------------------------------------------------------------------\n",
      "s least ice scombant such as he with long the effects that might ming in one of \n",
      "y consamedicating of differenceting foredon projobile programming wing from curf\n",
      "om arts a sreado will presentity one nine seven zero zero eight hor the predired\n",
      "ver god pofe asennes the palariss spray to there israilio fox infuber fixe disco\n",
      "le materal repluter as engines g the interraitanls of the heb standall form tear\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 11100: 1.542675 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 11200: 1.536565 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 11300: 1.537962 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 11400: 1.535436 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 11500: 1.562848 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 11600: 1.547308 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 11700: 1.555235 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 11800: 1.538107 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 11900: 1.545174 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 12000: 1.548619 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "--------------------------------------------------------------------------------\n",
      "by staate its bemain nat important of the nor the generally rappecies is all as \n",
      " attelent if kermica eationary accustell if yston cader isl near liver ni shille\n",
      "ed the percenty at tave was a toecirthel antist concepts vaid billabre when to g\n",
      "manter world arthnytic a which offation other force on one four two two zero eng\n",
      "b or instratives three and area of musimith language web sarstronslow are broadc\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 12100: 1.538834 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 12200: 1.538920 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 12300: 1.539687 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 12400: 1.540317 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 12500: 1.536133 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 12600: 1.518932 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 12700: 1.530423 learning_rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.12\n",
      "Average loss at step 12800: 1.553876 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 12900: 1.534948 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.14\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 13000: 1.535024 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "--------------------------------------------------------------------------------\n",
      "funi inclumii artacement he there of computer bloburion of the carina sented tha\n",
      " is there and bassey expreen portiruted a fivenere see the time peader sominhy d\n",
      "habt motal in needer united all negehaza and is alexaley that anari s and alexar\n",
      "itors are protaa sea one one one zero n nistrate furtheran moves this war aritar\n",
      "an mardetirent bewherlantar queegents one nine true it lenghout treast of breanc\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.12\n",
      "Average loss at step 13100: 1.529067 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 13200: 1.542108 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 13300: 1.539743 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 13400: 1.532428 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.08\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 13500: 1.505360 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 13600: 1.547404 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 13700: 1.519879 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 13800: 1.533362 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 13900: 1.524731 learning_rate: 1.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 14000: 1.545282 learning_rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "--------------------------------------------------------------------------------\n",
      "x go soup cie averary its baticods this mothory noveincon part and govel aborgg \n",
      "real it where many an snamber was pajaczerquane to countritus date his compinist\n",
      "lite support see legity several is specicific and also west into the munde it ar\n",
      "on directs it it south elegturario like if that in mush adding the baskities wer\n",
      "vanges upam romatic instrument they the pofemance is setary and basic ravization\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 14100: 1.546360 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.18\n",
      "Average loss at step 14200: 1.525234 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.18\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 14300: 1.512345 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.18\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 14400: 1.516355 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 14500: 1.547681 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 14600: 1.526814 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 14700: 1.533858 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 14800: 1.548777 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 14900: 1.569987 learning_rate: 1.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 15000: 1.497474 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.28\n",
      "--------------------------------------------------------------------------------\n",
      "hand and schidg of defaires in glosware purter four zero repoctinises of elitate\n",
      "adias righ and other of stork haken it addity in the magnetica cancusive book on\n",
      "duess and sease induptimotish stretut showaque is recembed in underspulg to agot\n",
      "linian felts formation for the s s their to initiats s ciphese one award largue \n",
      "king only one four servable in the cammas of one nine six three fro two one four\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 15100: 1.494055 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 15200: 1.544319 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 15300: 1.521314 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 15400: 1.518756 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 15500: 1.503865 learning_rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 15600: 1.530286 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 15700: 1.560553 learning_rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 15800: 1.524069 learning_rate: 1.000000\n",
      "Minibatch perplexity: 3.82\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 15900: 1.533284 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 16000: 1.551981 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "--------------------------------------------------------------------------------\n",
      "x of a states have accession in the in internitor rast to in their superate prov\n",
      "d to diaback so wn an or afternation as corritec whas undaging iious technbalse \n",
      "ks times mithod of botper in the veryal sugid american buse on the bl anthiv pre\n",
      "dicated time including kar disea of howsulba the mellays although khou every of \n",
      "a clad the calen nalt in isthly by unit georcments as ave hosel the houlton the \n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 16100: 1.552450 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 16200: 1.515885 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 16300: 1.535605 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 16400: 1.528792 learning_rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 16500: 1.566350 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 16600: 1.540492 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 16700: 1.559704 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 16800: 1.525635 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 16900: 1.526841 learning_rate: 1.000000\n",
      "Minibatch perplexity: 3.99\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 17000: 1.538744 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.22\n",
      "--------------------------------------------------------------------------------\n",
      "usting world his lie for even of revert is which arman speads routh who examedus\n",
      "nine for isa intervert or johronine including ettient bobs constitution which te\n",
      "elliess rommentaticula in volume upuiment econtay of the five zero seven septica\n",
      "both into the coll of meat is usmon is cruss of the two zero zero zero the euper\n",
      "quannic to that as two three redowd with the english the fornies percapily the u\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 17100: 1.538329 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 17200: 1.533313 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 17300: 1.523420 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.12\n",
      "Average loss at step 17400: 1.522636 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.11\n",
      "Average loss at step 17500: 1.520897 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 17600: 1.520945 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 17700: 1.530879 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 4.11\n",
      "Average loss at step 17800: 1.523616 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.02\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 17900: 1.513337 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.12\n",
      "Average loss at step 18000: 1.528115 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "--------------------------------------------------------------------------------\n",
      "jemash one zero one three zero one k head on the woten zero two zero eight six z\n",
      "chan space act and the general kined assci four two by fatuomen in anai been sel\n",
      "gudgel minoprosourres and saltam younsite of calestant of pacialised syguig this\n",
      " of hladber selections to the flomic and less knadhit no mutphempitor computer i\n",
      "jumy normal for crueds two zero one four one nine three and a see lack asvant by\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 18100: 1.525583 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 18200: 1.515627 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 18300: 1.501600 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 18400: 1.535496 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 18500: 1.541809 learning_rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.12\n",
      "Average loss at step 18600: 1.544835 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 18700: 1.515577 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 18800: 1.511217 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.12\n",
      "Average loss at step 18900: 1.514816 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 19000: 1.538229 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "--------------------------------------------------------------------------------\n",
      "x engleidy beever of khilles uchian of these golds about his also blue when musi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zers alize the law were citshing the ch cruss gold a three seven instires friens\n",
      "would offersh major summuters shoshest of commonly pater it maanolitically the b\n",
      "jels and be goods compression of under as the mex introebium it is must be a nic\n",
      "chotation of toliment in shate one nine six zing indian engineers american fath \n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.12\n",
      "Average loss at step 19100: 1.574572 learning_rate: 1.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.12\n",
      "Average loss at step 19200: 1.568804 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.14\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 19300: 1.531537 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 19400: 1.531057 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 19500: 1.527132 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 19600: 1.550783 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 19700: 1.514637 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.15\n",
      "Validation set perplexity: 4.12\n",
      "Average loss at step 19800: 1.512257 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 19900: 1.533787 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 20000: 1.500320 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "--------------------------------------------------------------------------------\n",
      " skin typall empisiar consishogert rest some to develop to ort of so jubable in \n",
      "justion with revengion wire was and problem human social upast of political a ne\n",
      "unical perhoress workom cocces are the term in be five nine the inform was reach\n",
      "ined and lorgy come in being on amas propishay in the prience thehe appears for \n",
      "vers  winnists viete one five seven zero three in the sagesbembered malabely dur\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 20100: 1.526907 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 20200: 1.548062 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 20300: 1.550303 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 20400: 1.597762 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 20500: 1.588623 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 20600: 1.554055 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 20700: 1.543494 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 20800: 1.532904 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 20900: 1.525494 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 21000: 1.542366 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.42\n",
      "--------------------------------------------------------------------------------\n",
      "ment laws the ruppillism links at a combaciate cullini makoons there hetle cancr\n",
      "ines and methonal himming lannocle roer catle pheals the data in the cangura s c\n",
      "ergern it construct he founda range u access c isrg in the trees in root afrivat\n",
      "fow onetzerneage tone fran ith leader the is between as published on the one fiv\n",
      "zer anghen in reference jowage hero one one me alfound of a mics of major tried \n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 21100: 1.512030 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 21200: 1.550692 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 21300: 1.551023 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 21400: 1.533720 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 21500: 1.519361 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 21600: 1.534179 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 21700: 1.532980 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 21800: 1.537019 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 21900: 1.545946 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 22000: 1.520206 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.51\n",
      "--------------------------------------------------------------------------------\n",
      " of the lor discoveries actualic signs social one joeth mending first of simple \n",
      "th the organial after using serge ato operases and the are generes in variouary \n",
      "ry one nine four yfarder known of poew elector of three zero one nine nine one n\n",
      "very four computer of university belalities alacing to councaltog one nine five \n",
      "wert without his the first varf compare to statuh and gover two seven front both\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 22100: 1.517855 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.07\n",
      "Average loss at step 22200: 1.541568 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.22\n",
      "Validation set perplexity: 4.07\n",
      "Average loss at step 22300: 1.545310 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.07\n",
      "Average loss at step 22400: 1.524852 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.07\n",
      "Average loss at step 22500: 1.532097 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.07\n",
      "Average loss at step 22600: 1.509001 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 22700: 1.519708 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 22800: 1.549655 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 22900: 1.536961 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.05\n",
      "Average loss at step 23000: 1.547265 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.62\n",
      "--------------------------------------------------------------------------------\n",
      "ver konie comministant champters the rike them londs also metjover fathled later\n",
      "univelys to descrising coresuran language in ashtas lachnic early charts two fou\n",
      "us though the dellected theue hond ruin beswances europe can beliebly basong one\n",
      "throvers rite uping of the flubern one five zero zero one nine five eight four o\n",
      "d gover sequence a bas in two fier set and from a a sprint sbanger rac then guip\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.05\n",
      "Average loss at step 23100: 1.553752 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.05\n",
      "Average loss at step 23200: 1.558788 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.04\n",
      "Average loss at step 23300: 1.554219 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.04\n",
      "Average loss at step 23400: 1.532634 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 4.04\n",
      "Average loss at step 23500: 1.544183 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 23600: 1.550658 learning_rate: 0.100000\n",
      "Minibatch perplexity: 3.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.04\n",
      "Average loss at step 23700: 1.535647 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 23800: 1.540849 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 23900: 1.542542 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 24000: 1.562620 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.95\n",
      "--------------------------------------------------------------------------------\n",
      "pholey been squrush life the him poence as sucufaws of the large nut aid it a hi\n",
      "jo for the moral found in one eight five john for the one zero doe usque was two\n",
      "ates halkress from the businy deffwar and montry the densutured one of the germa\n",
      "abich fount more great have theicical later was blood than one eight nine tent t\n",
      "ust when be reacred on oxymay a reasons steak to the thomb and the studies accre\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 24100: 1.525240 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 24200: 1.544672 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.01\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 24300: 1.532881 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 24400: 1.537076 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 24500: 1.520507 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 24600: 1.543093 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 24700: 1.517575 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 24800: 1.510426 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 24900: 1.514787 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 25000: 1.540659 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.63\n",
      "--------------------------------------------------------------------------------\n",
      "za physijles citie these bransed compression septes qualmans of litk z states be\n",
      "d two zero they ad ip in this two zelopy britz reage outcities this a composed a\n",
      "pende hih oby three zero briment one anselte with inside government was one clim\n",
      "harks the john settle intellebp the such assemension s ruth in the to the adkisa\n",
      "s is a shipsles from piace auti the prigme of the odiculation fortbress and mena\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 25100: 1.540987 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 25200: 1.538095 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 25300: 1.500473 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 25400: 1.515623 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 25500: 1.513719 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.09\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 25600: 1.509076 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 25700: 1.522861 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 25800: 1.531897 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 25900: 1.559656 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.11\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 26000: 1.553651 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.69\n",
      "--------------------------------------------------------------------------------\n",
      "adam the two tavo an early when a majume can retore alexanies lower of mortiels \n",
      "erie other cusseed two zero two ruth to as antimith ends busined than infloched \n",
      "red and way over the terriqually s higher two seven three prines the boausebia a\n",
      "y migst river angoverination of includine neartialians publisas years over the g\n",
      "jutuh meteanomc lesssist b iv expionition of where the musist six two six one on\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 26100: 1.527614 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 26200: 1.550934 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.02\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 26300: 1.522684 learning_rate: 0.100000\n",
      "Minibatch perplexity: 3.98\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 26400: 1.517922 learning_rate: 0.100000\n",
      "Minibatch perplexity: 3.93\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 26500: 1.510345 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 26600: 1.504337 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 26700: 1.536498 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.05\n",
      "Validation set perplexity: 4.00\n",
      "Average loss at step 26800: 1.527261 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 3.99\n",
      "Average loss at step 26900: 1.540374 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.00\n",
      "Average loss at step 27000: 1.501846 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.43\n",
      "--------------------------------------------------------------------------------\n",
      "th the businal conshippa ight to achian experimet eight five timal but of reside\n",
      "ersation united for the garks s to futue call doe europy scasisks in begneus loo\n",
      "d transleve with seven one eight seven three son arganis a renew iving cropt zir\n",
      "x nationally whose of asabrished meditical of theon bactias were oza belish comp\n",
      "nent and extramens lamic sloge birrcagent a villeisuit a night the operation mar\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.00\n",
      "Average loss at step 27100: 1.546652 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.00\n",
      "Average loss at step 27200: 1.547834 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.00\n",
      "Average loss at step 27300: 1.532482 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.00\n",
      "Average loss at step 27400: 1.543258 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 27500: 1.553745 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 27600: 1.532795 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 27700: 1.532600 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 27800: 1.537716 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 27900: 1.582132 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 28000: 1.557696 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.67\n",
      "--------------------------------------------------------------------------------\n",
      "quess of the large the coyers and angil arthon if of system of set in resing nir\n",
      "trice at this two two epared peaning that marton are some monomian electembern f\n",
      "or young time trant from gramped two you thing other instruction with electroply\n",
      "th the followlynsing indappilly buads the protectionshing rockboing s trials tux\n",
      "sorna poyern carricas and a and peace is all bug hamo ob on mused and cities of \n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 28100: 1.540435 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.02\n",
      "Average loss at step 28200: 1.518078 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 28300: 1.528663 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 28400: 1.540534 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 28500: 1.542642 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 28600: 1.519566 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 28700: 1.541741 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 28800: 1.523173 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 28900: 1.540260 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 29000: 1.537073 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.75\n",
      "--------------------------------------------------------------------------------\n",
      "on the lamper one nine four grad doreing missify without that indiving to on zer\n",
      "mung kalf eight six one nine two estarted gave right fortuall first three by zha\n",
      "a to oschs oelhea and sumber with agrem that coul each of new greemed harkiageat\n",
      "queen geern science which d zero one nine five scaut of period its standri lew n\n",
      "wards of formates withoulaol towable which the polish movese would faioin orial \n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 29100: 1.527450 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 29200: 1.525480 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 29300: 1.522467 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.17\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 29400: 1.521910 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 29500: 1.551816 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 29600: 1.540356 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 29700: 1.518842 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 29800: 1.558672 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 29900: 1.567748 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 30000: 1.523590 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.87\n",
      "--------------------------------------------------------------------------------\n",
      "suall one nine nine six mribefary a stroction in nittery claumery serfician prov\n",
      "quest mast directly uses called by mellers the invasino play the generally of wh\n",
      "t the prisonia wis the one nine seven zero three eight one seven one two th bell\n",
      "vidate menqueing tilled mety repultograph general webseprocemelleds of his for r\n",
      "k would generator and c from eritant to two eight five gases popular was meetuhi\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.02\n"
     ]
    }
   ],
   "source": [
    "steps = 30000\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('[Tensorflow]: Initialized!')\n",
    "    mean_loss = 0\n",
    "    for step in range(steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, loss, predictions, learning_rate = session.run(\n",
    "            [optimizer, _loss, train_prediction, _learning_rate], feed_dict=feed_dict)\n",
    "        # print('Loss: %f' % loss)\n",
    "        mean_loss += loss\n",
    "        if (step+1) % summary_frequency == 0:\n",
    "            mean_loss /= summary_frequency\n",
    "            print('Average loss at step %d: %f learning_rate: %f' % (step+1, mean_loss, learning_rate))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if (step+1) % (summary_frequency * 10) == 0:\n",
    "                # Generate some sample.\n",
    "                print('-' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('-' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob += logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "            valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Forget gate:\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Memory cell:\n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Output gate:\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Concatenate parameters\n",
    "    sx = tf.concat([ix, fx, cx, ox], 1)\n",
    "    sm = tf.concat([im, fm, cm, om], 1)\n",
    "    sb = tf.concat([ib, fb, cb, ob], 1)\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases:\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        ''' Create a LSTM cell. See eg: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates\n",
    "        '''\n",
    "        \n",
    "        smatmul = tf.matmul(i, sx) + tf.matmul(o, sm) + sb\n",
    "        # tf.split: arg(value, num_or_size_splits, axis=0, num=None)\n",
    "        smatmul_input, smatmul_forget, update, smatmul_output = tf.split(smatmul, 4, 1)\n",
    "        \n",
    "        input_gate = tf.sigmoid(smatmul_input)\n",
    "        forget_gate = tf.sigmoid(smatmul_forget)\n",
    "        output_gate = tf.sigmoid(smatmul_output)\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]\n",
    "        \n",
    "    # Unrolled LSTM loop\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "        \n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        # tf.nn.xw_plus_b -> y = w*x + b\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        _loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                logits=logits, labels=tf.concat(train_labels, 0)))\n",
    "        \n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    # tf.train.exponential_decay: \n",
    "    # decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)\n",
    "    _learning_rate = tf.train.exponential_decay(10., global_step, 10000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(_learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(_loss))\n",
    "    # tf.clip_by_global_norm: args:(t_list, clip_norm, use_norm=None, name=None)\n",
    "    # t_list[i] = t_list[i] * clip_norm / max(global_norm, clip_norm)\n",
    "    # global_norm = sqrt(sum([l2norm(t)**2 for t in t_list]))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, \n",
    "        saved_sample_output,\n",
    "        saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output), \n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tensorflow]: Initialized!\n",
      "Average loss at step 100: 2.605803 learning_rate: 10.000000\n",
      "Minibatch perplexity: 11.36\n",
      "Validation set perplexity: 11.36\n",
      "Average loss at step 200: 2.240572 learning_rate: 10.000000\n",
      "Minibatch perplexity: 7.86\n",
      "Validation set perplexity: 9.26\n",
      "Average loss at step 300: 2.118685 learning_rate: 10.000000\n",
      "Minibatch perplexity: 8.10\n",
      "Validation set perplexity: 8.20\n",
      "Average loss at step 400: 2.021401 learning_rate: 10.000000\n",
      "Minibatch perplexity: 7.43\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 500: 1.952224 learning_rate: 10.000000\n",
      "Minibatch perplexity: 7.08\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 600: 1.900028 learning_rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 700: 1.868704 learning_rate: 10.000000\n",
      "Minibatch perplexity: 6.71\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 800: 1.863541 learning_rate: 10.000000\n",
      "Minibatch perplexity: 6.78\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 900: 1.802325 learning_rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 1000: 1.795136 learning_rate: 10.000000\n",
      "Minibatch perplexity: 6.78\n",
      "--------------------------------------------------------------------------------\n",
      "lugioutain groduitian bittes this the mamation airi or of decure throunk buting \n",
      "s lethed the replatio s avcent the hen paendain hait  hairmexnispord in the guil\n",
      "d to which prolowonts proplace eymaliet bapery the must of etoull seftifularimoc\n",
      "fuchaliw four ald bun aunolatic four fich for two one zero gech repeinism refeur\n",
      "veb ius suider prycemeto and zowo yersy bestikeob diandatucoram is incebmchturs \n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 1100: 1.811024 learning_rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 1200: 1.791875 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 1300: 1.768767 learning_rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 6.12\n",
      "Average loss at step 1400: 1.764921 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 1500: 1.748523 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1600: 1.706362 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 1700: 1.720688 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1800: 1.710709 learning_rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 1900: 1.734278 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 2000: 1.698527 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "--------------------------------------------------------------------------------\n",
      "predectemed the daun frened vance seartion x cussof and bood kama eight one nine\n",
      "ble have suinth emputand of been inkided frop wes form boinn nec landined dision\n",
      "ment was laber bione stated and finaliswere importops of was and subate the goul\n",
      "pends which hundon aregatuet other vaodayic electran glathing of the mavibodrage\n",
      "hention wate of the gow they madree inaensies also mock to been there emenbl of \n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 2100: 1.708035 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 2200: 1.664982 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 2300: 1.681813 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 2400: 1.679179 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2500: 1.670491 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2600: 1.658488 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2700: 1.655900 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2800: 1.664191 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2900: 1.651851 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 3000: 1.655509 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "--------------------------------------------------------------------------------\n",
      "ctionen dut hagh for caniam dances ghrace is mublent the kireqand of two zero ze\n",
      "eloss that works after jussationallish poheswater under pbanta and comes espens \n",
      "y comics to storilly recorded are apperiea to reberially in angenta access not w\n",
      "barized icalative aecrarmanb fan in suzputandiane mashetiful used huroniouh lang\n",
      "colard like incritacs of scar lina but insovilscent vis the curiforble ica all o\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 3100: 1.657827 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 3200: 1.643950 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 3300: 1.650209 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 3400: 1.625349 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 3500: 1.617107 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3600: 1.632229 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3700: 1.630648 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3800: 1.602634 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3900: 1.621270 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 4000: 1.624817 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "--------------------------------------------------------------------------------\n",
      "kone the chirgernis af wey well there sleement definies replitition ary hurders \n",
      "he line fume thene sourciefically of the batting usels darian radiogrsnence brol\n",
      "quping on ruzoria yoal challe sounce subures france icoftatonoproliaine i the in\n",
      "tiporipainds neginmon furnow world finanistracted five seven one zero five pen l\n",
      "k decolagos wikh on the severye cleeke and will to that their gekergate lindom h\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4100: 1.644789 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 4200: 1.617088 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4300: 1.625752 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 4400: 1.637688 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4500: 1.659835 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4600: 1.623266 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 4700: 1.621486 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4800: 1.610179 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 4900: 1.569245 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 5000: 1.613042 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "--------------------------------------------------------------------------------\n",
      "ed langu or mumor ghannessally ver in lasclagee yorf b stries was for madopches \n",
      "gran from frod ravife breing rattury underge one nine zero zero d duy thriks sur\n",
      "quarration one may over a greek edmanciect callind to the bandidgoalists the tax\n",
      "x nun carrele absondist legautons as are sade in grandent systems in parter or h\n",
      " varues from the yalchy seel than materes calter of legut of has be othe the nam\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.95\n",
      "Average loss at step 5100: 1.609643 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 5200: 1.603761 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5300: 1.615446 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 5400: 1.610595 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 5500: 1.596451 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 5600: 1.579964 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5700: 1.599081 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5800: 1.612227 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5900: 1.611515 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 6000: 1.614871 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "--------------------------------------------------------------------------------\n",
      "ure rolaries elf the chreastion bebeelly martiors words was mecord d one of the \n",
      " sign and he low to vision suiiulia namalter such for the resises and a what beh\n",
      "z regelm works california calcolory and two made also athers and volation arviau\n",
      " rae lear plabizational for ade meanwi ale ligement language causelie are rerieo\n",
      " doritome some was and sear aroum but this ists crouper instruct reldard diolard\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6100: 1.585413 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 6200: 1.583306 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6300: 1.602110 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 6400: 1.593198 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6500: 1.607662 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 6600: 1.613277 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6700: 1.599077 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 6800: 1.599977 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 6900: 1.589125 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 7000: 1.593960 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "--------------------------------------------------------------------------------\n",
      "les accures zurm status playen lack feor hunding court with famous externd on th\n",
      "f seculal lex deccmpities called trorted these univer was into stilly beedhang s\n",
      "ques his untal kerumatics of bound and publicy of hust in unionely upes are for \n",
      "jox pance on the anna milip indided that arbetts the phenyaing peruliation state\n",
      "steresland qualitory the coure of the there muchoposic lovall reliegio those har\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 7100: 1.602033 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 7200: 1.618549 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 7300: 1.604615 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 7400: 1.588661 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 7500: 1.605805 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 7600: 1.590451 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 7700: 1.597542 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 7800: 1.605350 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 7900: 1.568904 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 8000: 1.583210 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "--------------------------------------------------------------------------------\n",
      "cipti instage poption kun cou with a bostry as warl de verd latina not make cymp\n",
      "orramisioning bionas be as reford infuly s bettional nering with are has one s r\n",
      "lifes a schorld kyously of his it teppoing princtions typ not ledent sevene of t\n",
      "y devel a dreen will many eajost is city batk of regived a many history updes in\n",
      "al partrous in continuanial skillifyc mumb or commoners of contracts resoveblem \n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 8100: 1.604402 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 8200: 1.585396 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 8300: 1.583301 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 8400: 1.594342 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 8500: 1.614547 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 8600: 1.549960 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 8700: 1.563599 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 8800: 1.567651 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 8900: 1.585167 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 9000: 1.560576 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "--------------------------------------------------------------------------------\n",
      "x or to corpition after spoyus caes adjound often nequing the eastere who pearly\n",
      "h original moved that that hold there word have dispublishus stochers risperring\n",
      "y and who hardon to in took the actrument is l engineret recoors of etric most t\n",
      "ut into sometimes to the stept theology the population from then anoodocraes and\n",
      "ques two two a reasing supage hardshould fiction suiscripted to chrion official \n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 9100: 1.545353 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 9200: 1.543664 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 9300: 1.536068 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 9400: 1.541777 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 9500: 1.550709 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 9600: 1.532283 learning_rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 9700: 1.513336 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.01\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 9800: 1.539871 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 9900: 1.536597 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 10000: 1.516559 learning_rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "--------------------------------------------------------------------------------\n",
      "or contempher has no that it slangual harp signed can been stylen umpled roracti\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nessible birs of agent hand altertina say namiler of dy in the name one wif s su\n",
      "cleas to the quiter had contrary in the chrish our to studuing and only a monsap\n",
      "z people calls one nine nine five but owdoral preswere into whol to i well stric\n",
      "vers external horiz milliande in desplacembem baokaally ocabliander which of the\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 10100: 1.502186 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 10200: 1.528728 learning_rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 10300: 1.514551 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 10400: 1.556879 learning_rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 10500: 1.523859 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 10600: 1.513410 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.07\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 10700: 1.516190 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 10800: 1.527294 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.00\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 10900: 1.552673 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 11000: 1.521436 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "--------------------------------------------------------------------------------\n",
      "rannical narch shootabks scriftificated by regions and the when itrichstain grea\n",
      "netrad hid an ording often with the commons one pxscate in a polycs which by chr\n",
      "jam it aulled equate earliered on a s priclent dating the bethe segatual the nee\n",
      "vosed by gux they magrence me and least one b one one nine six zero mests althou\n",
      "sivate until minerary counsering and has may the below hemble consedstus were es\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 11100: 1.523646 learning_rate: 1.000000\n",
      "Minibatch perplexity: 3.98\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 11200: 1.565553 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 11300: 1.540842 learning_rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 11400: 1.550392 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 11500: 1.554518 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.12\n",
      "Average loss at step 11600: 1.546814 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 11700: 1.503849 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 11800: 1.488901 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 11900: 1.513452 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 12000: 1.522440 learning_rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "--------------------------------------------------------------------------------\n",
      "ne mustal dows jains reform is it was rocksessing baid first he proved is mark p\n",
      "greeching them wish in the rappility and way most the tegrates propronge human i\n",
      "were thereas in enotyre but national mids one nine nine two s artist treate jey \n",
      "plato preyoled blue cantin are tends schakatian he which remanders into the vari\n",
      "pits as three diskong that mogn in a notions in pod in lacked fianestatics entle\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 12100: 1.529642 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 12200: 1.536831 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 12300: 1.534120 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 12400: 1.539615 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 12500: 1.572119 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 12600: 1.540594 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 12700: 1.557143 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 12800: 1.541691 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 12900: 1.545551 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 13000: 1.534470 learning_rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "--------------------------------------------------------------------------------\n",
      "phosition on the other it at modelling the book in killed edification rate from \n",
      "hakax in book of night three zero zero of high military which of commond collect\n",
      "ore chory orgen consider impics of one nine gen and in sovery crime of ackogist \n",
      "quest two one seven pecury he codula lack engineer in cases of tradia racka to t\n",
      " while but them new secry the sopenne alams was device afceng to however with tw\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 13100: 1.519517 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 13200: 1.541917 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 13300: 1.507908 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.22\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 13400: 1.522665 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 13500: 1.483578 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.02\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 13600: 1.504064 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 13700: 1.492155 learning_rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 13800: 1.481934 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 13900: 1.500854 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.11\n",
      "Average loss at step 14000: 1.512107 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "--------------------------------------------------------------------------------\n",
      "x with parlia book english compended this is jual relatively the do chies walts \n",
      "negized by zero cities in the may ls profit eitherrn a change ham to econoum in \n",
      "bert of both underd off dusi europenic annon to require sometimes ory incolpadar\n",
      "c of the lofe be a division of that bom thingom accede the g eng propp s of aler\n",
      "ones cutuin and lath zerve to its law is a have seels transmann lan of native en\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 14100: 1.510133 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 14200: 1.475238 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 14300: 1.461712 learning_rate: 1.000000\n",
      "Minibatch perplexity: 3.86\n",
      "Validation set perplexity: 4.12\n",
      "Average loss at step 14400: 1.496098 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 14500: 1.515894 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 14600: 1.511887 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.16\n",
      "Average loss at step 14700: 1.534939 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 14800: 1.498872 learning_rate: 1.000000\n",
      "Minibatch perplexity: 3.96\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 14900: 1.513227 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 15000: 1.520637 learning_rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "--------------------------------------------------------------------------------\n",
      " and contra as seriestencies the amsish though at in adeken political concempule\n",
      "quantlis india a devicise litting as bay tindhown production of rade two zero ze\n",
      "ble its long wind his view carksn a book one one th compolesion rable in rnsanne\n",
      "ors themselves and school suctian communities as nother fight for one seven eigh\n",
      "m investifies given yonsallige the war frand starder believe ir kan joonng sumce\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 15100: 1.508454 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 15200: 1.526027 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 15300: 1.543793 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 15400: 1.582628 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 15500: 1.559822 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 15600: 1.570616 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 15700: 1.573046 learning_rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 15800: 1.543091 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 15900: 1.540546 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 16000: 1.512776 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "--------------------------------------------------------------------------------\n",
      "ch perled to the world marunism were were cluding so album g p been againgly juh\n",
      "phrold division contin blomsabbridoc common one nine nine seven one two th centu\n",
      "bularwin bus two two crion and ill innanced they familips gensas statted block w\n",
      "vertionma if a bir the entery the one kmp the rightor the rrangalear assect is a\n",
      "bould and gracklis i into hold lade the nations begensive long center soperas an\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 16100: 1.506152 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 16200: 1.531095 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 16300: 1.530481 learning_rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 16400: 1.565429 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 16500: 1.557104 learning_rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 16600: 1.569102 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 16700: 1.560656 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 16800: 1.566587 learning_rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 16900: 1.544791 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 17000: 1.600005 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "--------------------------------------------------------------------------------\n",
      "kize the aogner some fan zero six zero zero zero zero zero freewh via notil see \n",
      "rot or a science on andmy is the chyman seven the sext in did asowing provitice \n",
      "ods production in reath well scientifinis of molisogy scloys gold to foresiveif \n",
      " two zero zero two teburo beensespartry betwefe a become a curres in zenows asse\n",
      "x max chyma known lioth the kench so hadpuarary committeel jud in began have vit\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 17100: 1.575763 learning_rate: 1.000000\n",
      "Minibatch perplexity: 3.94\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 17200: 1.551909 learning_rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 17300: 1.557654 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 17400: 1.533173 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 17500: 1.527386 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 17600: 1.537506 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.16\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 17700: 1.548806 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 17800: 1.529315 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 17900: 1.542779 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.18\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 18000: 1.513976 learning_rate: 1.000000\n",
      "Minibatch perplexity: 3.84\n",
      "--------------------------------------------------------------------------------\n",
      "k in lafthaesprophan reassian basic famitrihated set methons on and the diskred \n",
      "versed westherohian in in several ftc storian and chusgacawan one nine nine six \n",
      "z meises it apy wan howstry in recenters film terreghs during accordes partic li\n",
      "ple comprese from the perled hyllennes comprequing a clinglands r fiction in a i\n",
      "lization presided entiticipe such an sinated productly in alshable one eight and\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 18100: 1.520565 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 18200: 1.527263 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 18300: 1.532743 learning_rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 18400: 1.548260 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 18500: 1.549547 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 18600: 1.514004 learning_rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 18700: 1.498733 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 18800: 1.515265 learning_rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 18900: 1.512920 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 19000: 1.515784 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "--------------------------------------------------------------------------------\n",
      "way it deer american chibza leading chief inheas me holed the recent the three t\n",
      "atigan eight two yolo prefering bydard in one nine two ord latin one zero s jupa\n",
      "feine subpluctasting brithy had the desplatt in toits time history in footm with\n",
      " american oreance same sien do in one zero s two two t role againa doic changed \n",
      "stangt theory lessing through the will the ism was fress information a bux reath\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 19100: 1.520013 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.20\n",
      "Average loss at step 19200: 1.519959 learning_rate: 1.000000\n",
      "Minibatch perplexity: 3.96\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 19300: 1.518123 learning_rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 19400: 1.551327 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 19500: 1.527535 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 19600: 1.548040 learning_rate: 1.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 19700: 1.544647 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 19800: 1.543935 learning_rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 19900: 1.542849 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 20000: 1.509559 learning_rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "--------------------------------------------------------------------------------\n",
      "y unlv digary and islam zero zero zero zero zero s water program ja christment p\n",
      "jawer batinz musics and editive aw to first music birul his mitrism with ying i \n",
      "jon eight eight seven sermin of two and thouldhes machines on the areas which re\n",
      "quithe disalogorys with when are origina westmic and station shougherme rol pans\n",
      "quess hel member of statation wathing ago extension in rodges who u of old and t\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 20100: 1.537594 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.01\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 20200: 1.504620 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 20300: 1.517957 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 20400: 1.526034 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 20500: 1.526472 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 20600: 1.551168 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 20700: 1.588755 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 20800: 1.563574 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 20900: 1.577960 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 21000: 1.566348 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.68\n",
      "--------------------------------------------------------------------------------\n",
      " developing umples arabaning distrord arguarist at was claim and and to ringths \n",
      "tion mountar destroying grabet the in some a democes in one of poel on fewramom \n",
      "ved enagoutary the end p e e up anomics scate which wike nowsstemophensfured to \n",
      "ann poration wounco memoon of political finder deather informplizence key albull\n",
      "questing one roman its agmisfications verys wost when work and in well aid munic\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 21100: 1.563424 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 21200: 1.572311 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 21300: 1.580262 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.19\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 21400: 1.557422 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 21500: 1.574738 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 21600: 1.550994 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 21700: 1.579991 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 21800: 1.585893 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 21900: 1.564818 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 22000: 1.534369 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.28\n",
      "--------------------------------------------------------------------------------\n",
      "mentations after two thret victorils and on the licencintion of would arts of se\n",
      "aglomizing or see his units appeal sous that been occain bromary without of the \n",
      "res one three zero s that unto former detail lospion will figm the reseas of ele\n",
      "phopushric the greece jeleth function of the one nine nine whicers believe of ja\n",
      "bell is fighting their reachy belief yions to minicas faute as way system in the\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 22100: 1.542637 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 22200: 1.531390 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 22300: 1.539181 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 22400: 1.544896 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 22500: 1.546083 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 22600: 1.540780 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 22700: 1.516608 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 22800: 1.508390 learning_rate: 0.100000\n",
      "Minibatch perplexity: 3.97\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 22900: 1.533213 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 23000: 1.536082 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.77\n",
      "--------------------------------------------------------------------------------\n",
      "an purlinty one nine four the commonerretable were use a jehizitiur a very canad\n",
      "en list homoft flogatly prescoted in faffer and term that diphamer current new n\n",
      "nely eccopy two fty substantial theer called by people one zero one two two thre\n",
      "jostle representures clainlia waider theres davil and traving pkylouroit feld an\n",
      " one assocuse one nine four zero one nine branet would groven as immod stall lex\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 23100: 1.518361 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 23200: 1.553436 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 23300: 1.533070 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 23400: 1.531524 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 23500: 1.496401 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 23600: 1.524501 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 23700: 1.508828 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 23800: 1.510687 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 23900: 1.527321 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 24000: 1.538562 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.06\n",
      "--------------------------------------------------------------------------------\n",
      "ing son one station include after chuckant aspects and have tensity tend of jyla\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "was tind is an a zood mahwilific lankuogestel tigation zeorare drud ranist ameri\n",
      "e or the kowed can prolpcelis is series of back resultural consider usal at move\n",
      "y about the dur of explans that hereist kames joze off annimedufic jamo terr wil\n",
      "quest theorny ehy law is in the entrian to west found anychres green of which am\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 24100: 1.546509 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 24200: 1.511245 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 24300: 1.518750 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 24400: 1.546119 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 24500: 1.541695 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 24600: 1.544884 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 24700: 1.531545 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 24800: 1.542497 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 24900: 1.525844 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 25000: 1.518154 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.23\n",
      "--------------------------------------------------------------------------------\n",
      "questary governments enation in according i appoeded some niga deserge in the im\n",
      "joute ct which term dukludeshiff jesciptive the under televertine the great memb\n",
      "veat forerations figgal said evmous reals however influgnt forlesing deceating a\n",
      " after before the four zero three zero zero zero s decondence of pl d known slap\n",
      "e topici organians if islanger point the men stretarrakes no hep potern most acc\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 25100: 1.535372 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 25200: 1.539878 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 25300: 1.514039 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.10\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 25400: 1.533301 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 25500: 1.547420 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 25600: 1.553560 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 25700: 1.573336 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 25800: 1.560062 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 25900: 1.523367 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 26000: 1.519234 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.45\n",
      "--------------------------------------------------------------------------------\n",
      "jour and the beaths w hartast opans injure very author translated as the differe\n",
      "quers of relative external educkmiralsy polity proffed there and is their beases\n",
      "ments carraphy diesh confrefe were pastes victy wathens and the flections aq som\n",
      "ked programstics cammology banesain steadal of zelight dignials in the tems pres\n",
      "were blam in fok throolfin courchated on afterracus that world liberal rode tend\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 26100: 1.531760 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 26200: 1.532093 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 26300: 1.523814 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 26400: 1.534251 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 26500: 1.538572 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 26600: 1.551942 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 26700: 1.547420 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 26800: 1.527288 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 26900: 1.566487 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 27000: 1.551425 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.10\n",
      "--------------------------------------------------------------------------------\n",
      "bogacules bliend having botal one two seven three negusly the win australia was \n",
      " commended to the bow one of frepylems solt the clomair damn include for nine ze\n",
      "z zero judbochled for a megsonism of other as in one eighting stempt of forceali\n",
      "why usualland to soed hovere sometor folkysor bunniban dus up events of the part\n",
      "n wus no of unserve high large in two zero zero eachingulied the exceensting the\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 27100: 1.551715 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 27200: 1.558834 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 27300: 1.572494 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 27400: 1.528979 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 27500: 1.552610 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 27600: 1.528501 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 27700: 1.530470 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 27800: 1.563480 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 27900: 1.567869 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 28000: 1.545482 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.04\n",
      "--------------------------------------------------------------------------------\n",
      "z p the static and that an itdo died that suffered wam lkstring catalable to can\n",
      "p dayeth his related by a was news rised than s family or wide of of the primary\n",
      "z nefbrt intrappose addchy includian for the mark forma few ism to writers of co\n",
      "jeffer release in the leaders far that the digrant the josbee ox the first of th\n",
      "verk algen time during the alexanded british and power s hudd family the first g\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 28100: 1.557801 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 28200: 1.548407 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 28300: 1.537806 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 28400: 1.524366 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 28500: 1.542576 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 28600: 1.547749 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.15\n",
      "Average loss at step 28700: 1.542665 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 28800: 1.534351 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.18\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 28900: 1.558556 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 29000: 1.553240 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.66\n",
      "--------------------------------------------------------------------------------\n",
      "peody in the preev of the world bakivancins during lettheurour rate film at the \n",
      "x and claimed the clop manifold some near paqued to president wode the two zero \n",
      "use by the uctaint has on estempical verylault cell different window germant tim\n",
      "chmope of centuria and rimench main of war s manciuse the sellyn is college wemb\n",
      "anes publicative coffest by has s may other into r connecture bab the live one n\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 29100: 1.526850 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 29200: 1.532438 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 29300: 1.554978 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 29400: 1.546898 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 29500: 1.543663 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 29600: 1.549421 learning_rate: 0.100000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 29700: 1.559266 learning_rate: 0.100000\n",
      "Minibatch perplexity: 3.99\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 29800: 1.557157 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 29900: 1.557406 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 30000: 1.557398 learning_rate: 0.100000\n",
      "Minibatch perplexity: 4.96\n",
      "--------------------------------------------------------------------------------\n",
      "prom university a self a war the dora this of acivanist cases secretulaging the \n",
      "praton of the non in the unitetarles where where darmaury fall york repreails he\n",
      "word sometimerila brankeling three surver joon dovershic anx eatical earls of ma\n",
      "nce is they a lardorge or bow pirouth from he is notuniinment were shis yfords t\n",
      " operation on canadoll writersins used of much tab volate town dulled there is t\n",
      "--------------------------------------------------------------------------------\n",
      "Validation set perplexity: 4.16\n"
     ]
    }
   ],
   "source": [
    "steps = 30000\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('[Tensorflow]: Initialized!')\n",
    "    mean_loss = 0\n",
    "    for step in range(steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, loss, predictions, learning_rate = session.run(\n",
    "            [optimizer, _loss, train_prediction, _learning_rate], feed_dict=feed_dict)\n",
    "        # print('Loss: %f' % loss)\n",
    "        mean_loss += loss\n",
    "        if (step+1) % summary_frequency == 0:\n",
    "            mean_loss /= summary_frequency\n",
    "            print('Average loss at step %d: %f learning_rate: %f' % (step+1, mean_loss, learning_rate))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if (step+1) % (summary_frequency * 10) == 0:\n",
    "                # Generate some sample.\n",
    "                print('-' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('-' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob += logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "            valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "499px",
    "left": "658px",
    "right": "20px",
    "top": "445px",
    "width": "334px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

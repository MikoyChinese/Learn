{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 5\n",
    "------------\n",
    "\n",
    "The goal of this assignment is to train a Word2Vec skip-gram model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os, zipfile, random, math\n",
    "import collections, random\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from urllib.request import urlretrieve\n",
    "from matplotlib import pylab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = './dataset/'\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "class TqdmUpTo(tqdm):\n",
    "    def update_to(self, count=1, blockSize=1, totalSize=None):\n",
    "        if totalSize is not None:\n",
    "            self.total = totalSize\n",
    "        # It will also set self.n = count * blockSize\n",
    "        self.update(count * blockSize - self.n)\n",
    "\n",
    "def download_file(filename, expected_bytes=None, force=False):\n",
    "    dest_filename = os.path.join(data_root, filename)\n",
    "    if force or not os.path.exists(dest_filename):\n",
    "        print('Download: %s' % filename)\n",
    "        with TqdmUpTo(unit='B', unit_scale=True, unit_divisor=1024, miniters=1) as t:\n",
    "            dest_filename, _ = urlretrieve(url+filename, dest_filename,\n",
    "                                 reporthook=t.update_to, data=None)\n",
    "        print('\\n%s Download Complete!' % filename)\n",
    "        \n",
    "    if expected_bytes:\n",
    "        statinfo = os.stat(dest_filename)\n",
    "        not_expected_bytes_error = 'Failed to verify ' + dest_filename + '. Can you get to it with a browser?'\n",
    "        assert statinfo.st_size == expected_bytes, not_expected_bytes_error\n",
    "        \n",
    "    return dest_filename\n",
    "\n",
    "file = download_file('text8.zip', force=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0]),\n",
    "                                encoding='utf-8').split()\n",
    "    return data\n",
    "\n",
    "words = read_file(file)\n",
    "print('Data size %d' % len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the dictionary and replace rare words with UNK token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 50000\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = [['UNK', -1]]\n",
    "    # Append a new list to the original list.\n",
    "    # collections.Counter:\n",
    "    # eg: l = [a, b, a, a, b, c] -> collections.Counter(l)\n",
    "    # return {'a': 3, 'b': 2, 'c': 1}\n",
    "    # most_common(k): return 最常见的k个元素. ('a', 3)\n",
    "    count.extend(\n",
    "        collections.Counter(words).most_common(vocabulary_size -1))\n",
    "    dictionary = {}\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = []\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    \n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "del words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most common words (+UNK)', count[:5])\n",
    "print('data:', data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window # target label at the center of the buffer.\n",
    "        targets_to_avoid = [ skip_window ]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('data: ', [reverse_dictionary[di] for di in data[:32]])\n",
    "\n",
    "\n",
    "for num_skips, skip_window in [(2, 1), (4, 2)]:\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch(batch_size=16, num_skips=num_skips, skip_window=skip_window)\n",
    "    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n",
    "    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(16)])\n",
    "    \n",
    "for num_skips, skip_window in [(2, 1), (4, 2)]:\n",
    "    data_index = 1\n",
    "    batch, labels = generate_batch(batch_size=16, num_skips=num_skips, skip_window=skip_window)\n",
    "    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n",
    "    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(16)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128\n",
    "skip_window = 1\n",
    "num_skips = 2\n",
    "\n",
    "valid_size = 16\n",
    "valid_window = 100\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/device:GPU:0'):\n",
    "    \n",
    "    # Input data.\n",
    "    # [128,]\n",
    "    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    # [128, 1]\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    # [16,]\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    # Variables\n",
    "    # [50000, 128]\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform(shape=[vocabulary_size, embedding_size],\n",
    "                          minval=-1.0, maxval=1.0))\n",
    "    # [50000, 128]\n",
    "    softmax_weights = tf.Variable(\n",
    "        tf.truncated_normal(shape=[vocabulary_size, embedding_size],\n",
    "                            mean=0.0, stddev=1. / math.sqrt(embedding_size)))\n",
    "    # [50000,]\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Model.\n",
    "    # Look up embeddings for inputs.\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    \n",
    "    # Compute.\n",
    "    _loss = tf.reduce_mean(\n",
    "        tf.nn.sampled_softmax_loss(weights=softmax_weights, \n",
    "                                   biases=softmax_biases,\n",
    "                                   labels=train_labels,\n",
    "                                   inputs=embed,\n",
    "                                   num_sampled=num_sampled,\n",
    "                                   num_classes=vocabulary_size))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdagradOptimizer(1.0).minimize(_loss)\n",
    "    \n",
    "    # compute the similarity between minibatch examles and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(\n",
    "      normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, \n",
    "                           tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 1000000\n",
    "config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "with tf.Session(graph=graph, config=config) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('[Tensorflow]: Initialized!')\n",
    "    average_loss = 0\n",
    "    for step in range(steps):\n",
    "        batch_data, batch_labels = generate_batch(\n",
    "            batch_size=batch_size,\n",
    "            num_skips=num_skips,\n",
    "            skip_window=skip_window)\n",
    "        feed_dict = {train_dataset: batch_data,\n",
    "                    train_labels: batch_labels}\n",
    "        _, loss = session.run([optimizer, _loss], feed_dict=feed_dict)\n",
    "        average_loss += loss\n",
    "        if (step+1) % 2000 == 0:\n",
    "            average_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step %d: %f' % (step+1, average_loss))\n",
    "            average_loss = 0\n",
    "        if (step+1) % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                log = 'Nearest to %s' % valid_word\n",
    "                for k in range(top_k):\n",
    "                    closed_word = reverse_dictionary[nearest[k]]\n",
    "                    log = '%s %s,' % (log, closed_word)\n",
    "                print(log)\n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 400\n",
    "\n",
    "tsne = TSNE(perplexity=30, \n",
    "            n_components=2, \n",
    "            init='pca',\n",
    "            n_iter=5000, method='exact')\n",
    "two_d_embeddings = tsne.fit_transform(final_embeddings[1:num_points])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def plot(embeddings, labels):\n",
    "    assert embeddings.shape[0] >= len(labels), 'More labels than embeddings.'\n",
    "    pylab.figure(figsize=(15,15))\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = embeddings[i,:]\n",
    "        pylab.scatter(x, y)\n",
    "        pylab.annotate(label, xy=(x,y), xytext=(5,2), \n",
    "                       textcoords='offset points',\n",
    "                       ha='right', va='bottom')\n",
    "    pylab.show()\n",
    "        \n",
    "words = [reverse_dictionary[i] for i in range(1, num_points)]\n",
    "plot(two_d_embeddings, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Problem\n",
    "-------\n",
    "\n",
    "An alternative to skip-gram is another Word2Vec model called [CBOW](http://arxiv.org/abs/1301.3781) (Continuous Bag of Words). In the CBOW model, instead of predicting a context word from a word vector, you predict a word from the sum of all the word vectors in its context. Implement and evaluate a CBOW model trained on the text8 dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, bag_window):\n",
    "    global data_index\n",
    "    span =  bag_window * 2 + 1\n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size, span-1), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size):\n",
    "        buffer_list = list(buffer)\n",
    "        labels[i, 0] = buffer_list.pop(bag_window)\n",
    "        batch[i] = buffer_list\n",
    "        \n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data: ', [reverse_dictionary[di] for di in data[:32]])\n",
    "\n",
    "\n",
    "for bag_window in [1, 2]:\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch(batch_size=4, bag_window=bag_window)\n",
    "    print('\\nwith bag_window = %d:' % bag_window)\n",
    "    print('    batch:', [[reverse_dictionary[w] for w in bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128\n",
    "bag_window = 2\n",
    "\n",
    "valid_size = 16\n",
    "valid_window = 100\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/device:GPU:0'):\n",
    "    \n",
    "    # Input data.\n",
    "    # [128,]\n",
    "    train_dataset = tf.placeholder(tf.int32, shape=[batch_size, bag_window * 2])\n",
    "    # [128, 1]\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    # [16,]\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    # Variables\n",
    "    # [50000, 128]\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform(shape=[vocabulary_size, embedding_size],\n",
    "                          minval=-1.0, maxval=1.0))\n",
    "    # [50000, 128]\n",
    "    softmax_weights = tf.Variable(\n",
    "        tf.truncated_normal(shape=[vocabulary_size, embedding_size],\n",
    "                            mean=0.0, stddev=1. / math.sqrt(embedding_size)))\n",
    "    # [50000,]\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Model.\n",
    "    # Look up embeddings for inputs.\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    \n",
    "    # Compute.\n",
    "    _loss = tf.reduce_mean(\n",
    "        tf.nn.sampled_softmax_loss(weights=softmax_weights, \n",
    "                                   biases=softmax_biases,\n",
    "                                   labels=train_labels,\n",
    "                                   inputs=tf.reduce_sum(embed, 1),\n",
    "                                   num_sampled=num_sampled,\n",
    "                                   num_classes=vocabulary_size))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdagradOptimizer(1.0).minimize(_loss)\n",
    "    \n",
    "    # compute the similarity between minibatch examles and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(\n",
    "      normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, \n",
    "                           tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 100000\n",
    "config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "with tf.Session(graph=graph, config=config) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('[Tensorflow]: Initialized!')\n",
    "    average_loss = 0\n",
    "    for step in range(steps):\n",
    "        batch_data, batch_labels = generate_batch(\n",
    "            batch_size=batch_size,\n",
    "            bag_window=bag_window)\n",
    "        feed_dict = {train_dataset: batch_data,\n",
    "                    train_labels: batch_labels}\n",
    "        _, loss = session.run([optimizer, _loss], feed_dict=feed_dict)\n",
    "        average_loss += loss\n",
    "        if (step+1) % 2000 == 0:\n",
    "            average_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step %d: %f' % (step+1, average_loss))\n",
    "            average_loss = 0\n",
    "        if (step+1) % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                log = 'Nearest to %s' % valid_word\n",
    "                for k in range(top_k):\n",
    "                    closed_word = reverse_dictionary[nearest[k]]\n",
    "                    log = '%s %s,' % (log, closed_word)\n",
    "                print(log)\n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 400\n",
    "\n",
    "tsne = TSNE(perplexity=30, \n",
    "            n_components=2, \n",
    "            init='pca',\n",
    "            n_iter=5000, method='exact')\n",
    "two_d_embeddings = tsne.fit_transform(final_embeddings[1:num_points])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def plot(embeddings, labels):\n",
    "    assert embeddings.shape[0] >= len(labels), 'More labels than embeddings.'\n",
    "    pylab.figure(figsize=(15,15))\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = embeddings[i,:]\n",
    "        pylab.scatter(x, y)\n",
    "        pylab.annotate(label, xy=(x,y), xytext=(5,2), \n",
    "                       textcoords='offset points',\n",
    "                       ha='right', va='bottom')\n",
    "    pylab.show()\n",
    "        \n",
    "words = [reverse_dictionary[i] for i in range(1, num_points)]\n",
    "plot(two_d_embeddings, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122,
   "position": {
    "height": "812px",
    "left": "540px",
    "right": "20px",
    "top": "128px",
    "width": "369px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

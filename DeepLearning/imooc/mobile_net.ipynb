{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR_DIR = '/home/commaai-03/Data/dataset/cifar-10-python'\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as f:\n",
    "        data = pickle.load(f, encoding='bytes')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [os.path.join(CIFAR_DIR, file) \n",
    "             for file in os.listdir(CIFAR_DIR)\n",
    "             if '.html' not in file]\n",
    "filenames.sort()\n",
    "meta_file = filenames[0]\n",
    "train_files = filenames[1:-1]\n",
    "test_file = [filenames[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'batch_label'\n",
      "b'labels'\n",
      "b'data'\n",
      "b'filenames'\n"
     ]
    }
   ],
   "source": [
    "_test_data = unpickle(test_file[0])\n",
    "for k, v in _test_data.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9dc144f438>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_arr = _test_data[b'data'][0]\n",
    "# 32 * 32 * 3 (R,G,B)\n",
    "img_arr_reshaped = img_arr.reshape((3, 32, 32))\n",
    "img = img_arr_reshaped.transpose(1, 2, 0)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "%matplotlib inline\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifarData:\n",
    "    \n",
    "    def __init__(self, filenames, need_shuffle):\n",
    "        all_data = []\n",
    "        all_labels = []\n",
    "        for filename in filenames:\n",
    "            data, labels = self.load_data(filename)\n",
    "            all_data.append(data)\n",
    "            all_labels.append(labels)\n",
    "        self._data = np.vstack(all_data)\n",
    "        self._data = self._data / 127.5 - 1\n",
    "        self._labels = np.hstack(all_labels)\n",
    "        print('[CIFAR-10]: Data shape-> %s' % str(self._data.shape))\n",
    "        print('[CIFAR-10]: Label shape-> %s' % str(self._labels.shape))\n",
    "        \n",
    "        self.num_examples = self._data.shape[0]\n",
    "        self._need_shuffle = need_shuffle\n",
    "        self._indicator = 0\n",
    "        if self._need_shuffle:\n",
    "            self._shuffle_data()\n",
    "            \n",
    "    def load_data(self, filename):\n",
    "        import pickle\n",
    "        with open(filename, mode='rb') as f:\n",
    "            data = pickle.load(f, encoding='bytes')\n",
    "        return data[b'data'], data[b'labels']\n",
    "    \n",
    "    def _shuffle_data(self):\n",
    "        index = np.random.permutation(self.num_examples)\n",
    "        self._data = self._data[index]\n",
    "        self._labels = self._labels[index]\n",
    "        \n",
    "    def next_batch(self, batch_size):\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > self.num_examples:\n",
    "            rest_num_examples = self.num_examples - self._indicator\n",
    "            data_rest_part = self._data[self._indicator: self.num_examples]\n",
    "            label_rest_part = self._labels[self._indicator: self.num_examples]\n",
    "            \n",
    "            if self._need_shuffle:\n",
    "                self._shuffle_data()\n",
    "            # For new loop, self._indicator + batch_size = self.num_examples\n",
    "            self._indicator = batch_size - rest_num_examples\n",
    "            end_indicator = self._indicator\n",
    "            data_new_part = self._data[:end_indicator]\n",
    "            label_new_part = self._labels[:end_indicator]\n",
    "            batch_data = np.concatenate((data_rest_part, data_new_part), axis=0)\n",
    "            batch_label = np.concatenate((label_rest_part, label_new_part), axis=0)\n",
    "        else:\n",
    "            batch_data = self._data[self._indicator:end_indicator]\n",
    "            batch_label = self._labels[self._indicator:end_indicator]\n",
    "            self._indicator = end_indicator\n",
    "        \n",
    "        return batch_data, batch_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIFAR-10]: Data shape-> (50000, 3072)\n",
      "[CIFAR-10]: Label shape-> (50000,)\n",
      "[CIFAR-10]: Data shape-> (10000, 3072)\n",
      "[CIFAR-10]: Label shape-> (10000,)\n"
     ]
    }
   ],
   "source": [
    "train_data = CifarData(train_files, need_shuffle=True)\n",
    "test_data = CifarData(test_file, need_shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separable_conv_block(x, \n",
    "                         output_channel_number, name):\n",
    "    '''\n",
    "    Args:\n",
    "    - x:\n",
    "    - output_channel_number:\n",
    "    - name:\n",
    "    '''\n",
    "    with tf.variable_scope(name):\n",
    "        input_channel = x.get_shape().as_list()[-1]\n",
    "        #\n",
    "        channel_wise_x = tf.split(x, input_channel, axis=3)\n",
    "        output_channels = []\n",
    "        for i in range(len(channel_wise_x)):\n",
    "            output_channel = tf.keras.layers.Conv2D(filters=1, \n",
    "                                kernel_size=(3,3),\n",
    "                                strides=(1,1), \n",
    "                                padding='same', \n",
    "                                activation=tf.nn.relu, \n",
    "                                name='conv_%d' % i)(channel_wise_x[i])\n",
    "            output_channels.append(output_channel)\n",
    "        concat_layer = tf.concat(output_channels, axis=3)\n",
    "        conv1_1 = tf.keras.layers.Conv2D(\n",
    "                filters=output_channel_number, \n",
    "                kernel_size=(1,1), strides=(1,1), \n",
    "                padding='same', \n",
    "                activation=tf.nn.relu, name='conv1_1')(concat_layer)\n",
    "    return conv1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0924 11:55:36.233151 140317871028032 deprecation.py:506] From /home/commaai-03/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0924 11:55:37.698521 140317871028032 deprecation.py:323] From /home/commaai-03/.local/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(dtype=tf.float32, shape=[None, 3072])\n",
    "y = tf.placeholder(dtype=tf.int64, shape=[None])\n",
    "\n",
    "x_img = tf.reshape(tensor=x, shape=[-1, 3, 32, 32])\n",
    "# x_img: [None, 32, 32, 3]\n",
    "x_img = tf.transpose(x_img, perm=[0, 2, 3, 1])\n",
    "\n",
    "# conv1: feature map. [None, 32, 32, 32]\n",
    "conv1 = tf.keras.layers.Conv2D(filters=32, \n",
    "                               kernel_size=(3,3), \n",
    "                               padding='same', \n",
    "                               activation=tf.nn.relu, \n",
    "                               name='conv1')(x_img)\n",
    "# pooling1: [None, 16, 16, 32]\n",
    "pooling1 = tf.keras.layers.MaxPooling2D(pool_size=(2,2), \n",
    "                                        strides=(2,2))(conv1)\n",
    "\n",
    "# separable_2a and 2b [None, 16, 16, 32]\n",
    "separable_2a = separable_conv_block(x=pooling1, \n",
    "                                    output_channel_number=32, \n",
    "                                    name='separable_2a')\n",
    "separable_2b = separable_conv_block(x=separable_2a, \n",
    "                                    output_channel_number=32, \n",
    "                                    name='separable_2b')\n",
    "\n",
    "# pooling2: [None, 8, 8, 32]\n",
    "pooling2 = tf.keras.layers.MaxPooling2D(pool_size=(2,2), \n",
    "                                        strides=(2,2))(separable_2b)\n",
    "\n",
    "# separable_3a and 3b [None, 8, 8, 32]\n",
    "separable_3a = separable_conv_block(x=pooling2,\n",
    "                                    output_channel_number=32,\n",
    "                                    name='separable_3a')\n",
    "separable_3b = separable_conv_block(x=separable_3a, \n",
    "                                     output_channel_number=32, \n",
    "                                     name='separable_3b')\n",
    "\n",
    "# pooling3: [None, 4, 4, 32]\n",
    "pooling3 = tf.keras.layers.MaxPooling2D(pool_size=(2,2), \n",
    "                                        strides=(2,2))(separable_3b)\n",
    "\n",
    "# flatten: [512 * None,]\n",
    "flatten = tf.keras.layers.Flatten()(pooling3)\n",
    "# Dense: [10,]\n",
    "y_ = tf.keras.layers.Dense(units=10)(flatten)\n",
    "\n",
    "_loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_)\n",
    "\n",
    "predict = tf.argmax(y_, 1)\n",
    "correct_prediction = tf.equal(predict, y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float64))\n",
    "\n",
    "with tf.name_scope('train_op'):\n",
    "    train_op = tf.train.AdamOptimizer(1e-3).minimize(_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train]: Step: 100, loss: 2.31312 acc: 0.10000\n",
      "[Train]: Step: 200, loss: 2.12169 acc: 0.10000\n",
      "[Train]: Step: 300, loss: 1.85917 acc: 0.35000\n",
      "[Train]: Step: 400, loss: 1.41616 acc: 0.40000\n",
      "[Train]: Step: 500, loss: 2.22816 acc: 0.20000\n",
      "[Train]: Step: 600, loss: 1.76293 acc: 0.40000\n",
      "[Train]: Step: 700, loss: 1.39598 acc: 0.60000\n",
      "[Train]: Step: 800, loss: 1.70677 acc: 0.50000\n",
      "[Train]: Step: 900, loss: 1.33551 acc: 0.55000\n",
      "[Train]: Step: 1000, loss: 1.41636 acc: 0.45000\n",
      "[Test] Step: 1000, acc: 0.47280\n",
      "[Train]: Step: 1100, loss: 1.58659 acc: 0.30000\n",
      "[Train]: Step: 1200, loss: 1.76952 acc: 0.35000\n",
      "[Train]: Step: 1300, loss: 1.45533 acc: 0.45000\n",
      "[Train]: Step: 1400, loss: 1.47931 acc: 0.40000\n",
      "[Train]: Step: 1500, loss: 1.14052 acc: 0.70000\n",
      "[Train]: Step: 1600, loss: 1.65458 acc: 0.45000\n",
      "[Train]: Step: 1700, loss: 1.23242 acc: 0.50000\n",
      "[Train]: Step: 1800, loss: 1.43847 acc: 0.50000\n",
      "[Train]: Step: 1900, loss: 1.19096 acc: 0.55000\n",
      "[Train]: Step: 2000, loss: 1.11585 acc: 0.55000\n",
      "[Test] Step: 2000, acc: 0.49350\n",
      "[Train]: Step: 2100, loss: 1.14894 acc: 0.50000\n",
      "[Train]: Step: 2200, loss: 1.65611 acc: 0.55000\n",
      "[Train]: Step: 2300, loss: 1.22593 acc: 0.55000\n",
      "[Train]: Step: 2400, loss: 1.47681 acc: 0.50000\n",
      "[Train]: Step: 2500, loss: 1.26357 acc: 0.45000\n",
      "[Train]: Step: 2600, loss: 0.89912 acc: 0.65000\n",
      "[Train]: Step: 2700, loss: 1.18524 acc: 0.55000\n",
      "[Train]: Step: 2800, loss: 1.06546 acc: 0.55000\n",
      "[Train]: Step: 2900, loss: 1.72402 acc: 0.40000\n",
      "[Train]: Step: 3000, loss: 1.04238 acc: 0.70000\n",
      "[Test] Step: 3000, acc: 0.53490\n",
      "[Train]: Step: 3100, loss: 1.29070 acc: 0.50000\n",
      "[Train]: Step: 3200, loss: 1.31467 acc: 0.50000\n",
      "[Train]: Step: 3300, loss: 1.34259 acc: 0.55000\n",
      "[Train]: Step: 3400, loss: 1.37219 acc: 0.55000\n",
      "[Train]: Step: 3500, loss: 1.41753 acc: 0.55000\n",
      "[Train]: Step: 3600, loss: 1.16295 acc: 0.60000\n",
      "[Train]: Step: 3700, loss: 0.97495 acc: 0.65000\n",
      "[Train]: Step: 3800, loss: 1.01929 acc: 0.65000\n",
      "[Train]: Step: 3900, loss: 1.24888 acc: 0.60000\n",
      "[Train]: Step: 4000, loss: 0.95797 acc: 0.70000\n",
      "[Test] Step: 4000, acc: 0.57230\n",
      "[Train]: Step: 4100, loss: 1.61317 acc: 0.55000\n",
      "[Train]: Step: 4200, loss: 1.26696 acc: 0.55000\n",
      "[Train]: Step: 4300, loss: 1.26055 acc: 0.60000\n",
      "[Train]: Step: 4400, loss: 1.23670 acc: 0.50000\n",
      "[Train]: Step: 4500, loss: 0.85514 acc: 0.70000\n",
      "[Train]: Step: 4600, loss: 1.09077 acc: 0.50000\n",
      "[Train]: Step: 4700, loss: 0.53322 acc: 0.80000\n",
      "[Train]: Step: 4800, loss: 1.49122 acc: 0.40000\n",
      "[Train]: Step: 4900, loss: 1.31283 acc: 0.55000\n",
      "[Train]: Step: 5000, loss: 1.01066 acc: 0.65000\n",
      "[Test] Step: 5000, acc: 0.58730\n",
      "[Train]: Step: 5100, loss: 1.31680 acc: 0.55000\n",
      "[Train]: Step: 5200, loss: 0.92291 acc: 0.65000\n",
      "[Train]: Step: 5300, loss: 1.13916 acc: 0.70000\n",
      "[Train]: Step: 5400, loss: 1.06948 acc: 0.60000\n",
      "[Train]: Step: 5500, loss: 0.98194 acc: 0.70000\n",
      "[Train]: Step: 5600, loss: 1.25114 acc: 0.55000\n",
      "[Train]: Step: 5700, loss: 1.10306 acc: 0.60000\n",
      "[Train]: Step: 5800, loss: 1.30855 acc: 0.50000\n",
      "[Train]: Step: 5900, loss: 1.00247 acc: 0.70000\n",
      "[Train]: Step: 6000, loss: 1.53035 acc: 0.55000\n",
      "[Test] Step: 6000, acc: 0.58500\n",
      "[Train]: Step: 6100, loss: 1.18002 acc: 0.55000\n",
      "[Train]: Step: 6200, loss: 0.83112 acc: 0.55000\n",
      "[Train]: Step: 6300, loss: 1.38148 acc: 0.55000\n",
      "[Train]: Step: 6400, loss: 1.05294 acc: 0.65000\n",
      "[Train]: Step: 6500, loss: 1.27174 acc: 0.55000\n",
      "[Train]: Step: 6600, loss: 1.45354 acc: 0.45000\n",
      "[Train]: Step: 6700, loss: 1.22920 acc: 0.50000\n",
      "[Train]: Step: 6800, loss: 1.06461 acc: 0.65000\n",
      "[Train]: Step: 6900, loss: 1.61791 acc: 0.45000\n",
      "[Train]: Step: 7000, loss: 1.28554 acc: 0.55000\n",
      "[Test] Step: 7000, acc: 0.61400\n",
      "[Train]: Step: 7100, loss: 1.22996 acc: 0.70000\n",
      "[Train]: Step: 7200, loss: 0.91596 acc: 0.70000\n",
      "[Train]: Step: 7300, loss: 1.11313 acc: 0.55000\n",
      "[Train]: Step: 7400, loss: 0.91828 acc: 0.65000\n",
      "[Train]: Step: 7500, loss: 0.87474 acc: 0.65000\n",
      "[Train]: Step: 7600, loss: 1.09647 acc: 0.55000\n",
      "[Train]: Step: 7700, loss: 1.15958 acc: 0.60000\n",
      "[Train]: Step: 7800, loss: 1.14794 acc: 0.40000\n",
      "[Train]: Step: 7900, loss: 1.03084 acc: 0.65000\n",
      "[Train]: Step: 8000, loss: 1.17821 acc: 0.50000\n",
      "[Test] Step: 8000, acc: 0.61780\n",
      "[Train]: Step: 8100, loss: 1.47393 acc: 0.60000\n",
      "[Train]: Step: 8200, loss: 0.86078 acc: 0.65000\n",
      "[Train]: Step: 8300, loss: 0.77119 acc: 0.65000\n",
      "[Train]: Step: 8400, loss: 1.32876 acc: 0.50000\n",
      "[Train]: Step: 8500, loss: 0.90657 acc: 0.65000\n",
      "[Train]: Step: 8600, loss: 0.54267 acc: 0.85000\n",
      "[Train]: Step: 8700, loss: 1.35724 acc: 0.60000\n",
      "[Train]: Step: 8800, loss: 1.11583 acc: 0.60000\n",
      "[Train]: Step: 8900, loss: 1.09202 acc: 0.70000\n",
      "[Train]: Step: 9000, loss: 1.02330 acc: 0.60000\n",
      "[Test] Step: 9000, acc: 0.62910\n",
      "[Train]: Step: 9100, loss: 1.13560 acc: 0.65000\n",
      "[Train]: Step: 9200, loss: 1.38717 acc: 0.50000\n",
      "[Train]: Step: 9300, loss: 0.96310 acc: 0.65000\n",
      "[Train]: Step: 9400, loss: 1.38959 acc: 0.50000\n",
      "[Train]: Step: 9500, loss: 1.14669 acc: 0.65000\n",
      "[Train]: Step: 9600, loss: 1.27457 acc: 0.70000\n",
      "[Train]: Step: 9700, loss: 0.70761 acc: 0.85000\n",
      "[Train]: Step: 9800, loss: 0.94178 acc: 0.70000\n",
      "[Train]: Step: 9900, loss: 1.17585 acc: 0.60000\n",
      "[Train]: Step: 10000, loss: 1.03039 acc: 0.60000\n",
      "[Test] Step: 10000, acc: 0.58920\n",
      "[Train]: Step: 10100, loss: 0.93257 acc: 0.65000\n",
      "[Train]: Step: 10200, loss: 1.31486 acc: 0.65000\n",
      "[Train]: Step: 10300, loss: 1.44252 acc: 0.55000\n",
      "[Train]: Step: 10400, loss: 1.32146 acc: 0.55000\n",
      "[Train]: Step: 10500, loss: 1.63102 acc: 0.45000\n",
      "[Train]: Step: 10600, loss: 0.93438 acc: 0.70000\n",
      "[Train]: Step: 10700, loss: 0.97560 acc: 0.65000\n",
      "[Train]: Step: 10800, loss: 1.47486 acc: 0.50000\n",
      "[Train]: Step: 10900, loss: 1.04675 acc: 0.50000\n",
      "[Train]: Step: 11000, loss: 1.04528 acc: 0.65000\n",
      "[Test] Step: 11000, acc: 0.63530\n",
      "[Train]: Step: 11100, loss: 1.21481 acc: 0.65000\n",
      "[Train]: Step: 11200, loss: 1.15917 acc: 0.80000\n",
      "[Train]: Step: 11300, loss: 0.70366 acc: 0.65000\n",
      "[Train]: Step: 11400, loss: 1.14340 acc: 0.60000\n",
      "[Train]: Step: 11500, loss: 0.93046 acc: 0.70000\n",
      "[Train]: Step: 11600, loss: 0.91496 acc: 0.55000\n",
      "[Train]: Step: 11700, loss: 1.45263 acc: 0.45000\n",
      "[Train]: Step: 11800, loss: 1.41579 acc: 0.50000\n",
      "[Train]: Step: 11900, loss: 1.18557 acc: 0.65000\n",
      "[Train]: Step: 12000, loss: 0.92461 acc: 0.70000\n",
      "[Test] Step: 12000, acc: 0.64210\n",
      "[Train]: Step: 12100, loss: 1.01975 acc: 0.65000\n",
      "[Train]: Step: 12200, loss: 1.07084 acc: 0.70000\n",
      "[Train]: Step: 12300, loss: 0.83409 acc: 0.60000\n",
      "[Train]: Step: 12400, loss: 1.39739 acc: 0.45000\n",
      "[Train]: Step: 12500, loss: 0.77359 acc: 0.70000\n",
      "[Train]: Step: 12600, loss: 1.03621 acc: 0.75000\n",
      "[Train]: Step: 12700, loss: 0.96765 acc: 0.70000\n",
      "[Train]: Step: 12800, loss: 0.67348 acc: 0.75000\n",
      "[Train]: Step: 12900, loss: 0.66708 acc: 0.75000\n",
      "[Train]: Step: 13000, loss: 1.14679 acc: 0.60000\n",
      "[Test] Step: 13000, acc: 0.64980\n",
      "[Train]: Step: 13100, loss: 1.38488 acc: 0.60000\n",
      "[Train]: Step: 13200, loss: 0.99892 acc: 0.60000\n",
      "[Train]: Step: 13300, loss: 0.75775 acc: 0.75000\n",
      "[Train]: Step: 13400, loss: 1.06008 acc: 0.55000\n",
      "[Train]: Step: 13500, loss: 1.18088 acc: 0.55000\n",
      "[Train]: Step: 13600, loss: 0.67590 acc: 0.70000\n",
      "[Train]: Step: 13700, loss: 0.91814 acc: 0.75000\n",
      "[Train]: Step: 13800, loss: 0.67975 acc: 0.75000\n",
      "[Train]: Step: 13900, loss: 0.73104 acc: 0.70000\n",
      "[Train]: Step: 14000, loss: 0.53788 acc: 0.85000\n",
      "[Test] Step: 14000, acc: 0.65810\n",
      "[Train]: Step: 14100, loss: 1.16769 acc: 0.60000\n",
      "[Train]: Step: 14200, loss: 0.55290 acc: 0.75000\n",
      "[Train]: Step: 14300, loss: 0.75906 acc: 0.70000\n",
      "[Train]: Step: 14400, loss: 0.88566 acc: 0.75000\n",
      "[Train]: Step: 14500, loss: 1.03698 acc: 0.65000\n",
      "[Train]: Step: 14600, loss: 0.46537 acc: 0.90000\n",
      "[Train]: Step: 14700, loss: 1.23101 acc: 0.50000\n",
      "[Train]: Step: 14800, loss: 0.77331 acc: 0.75000\n",
      "[Train]: Step: 14900, loss: 1.26349 acc: 0.65000\n",
      "[Train]: Step: 15000, loss: 0.39285 acc: 0.90000\n",
      "[Test] Step: 15000, acc: 0.66660\n",
      "[Train]: Step: 15100, loss: 1.26948 acc: 0.55000\n",
      "[Train]: Step: 15200, loss: 1.05473 acc: 0.65000\n",
      "[Train]: Step: 15300, loss: 1.19299 acc: 0.70000\n",
      "[Train]: Step: 15400, loss: 1.06689 acc: 0.50000\n",
      "[Train]: Step: 15500, loss: 0.51666 acc: 0.90000\n",
      "[Train]: Step: 15600, loss: 1.23540 acc: 0.60000\n",
      "[Train]: Step: 15700, loss: 0.45807 acc: 0.85000\n",
      "[Train]: Step: 15800, loss: 0.93996 acc: 0.60000\n",
      "[Train]: Step: 15900, loss: 0.66860 acc: 0.65000\n",
      "[Train]: Step: 16000, loss: 0.57064 acc: 0.75000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] Step: 16000, acc: 0.65250\n",
      "[Train]: Step: 16100, loss: 0.98914 acc: 0.65000\n",
      "[Train]: Step: 16200, loss: 0.78500 acc: 0.70000\n",
      "[Train]: Step: 16300, loss: 0.84609 acc: 0.80000\n",
      "[Train]: Step: 16400, loss: 1.11363 acc: 0.55000\n",
      "[Train]: Step: 16500, loss: 0.56735 acc: 0.80000\n",
      "[Train]: Step: 16600, loss: 0.75723 acc: 0.75000\n",
      "[Train]: Step: 16700, loss: 0.82767 acc: 0.65000\n",
      "[Train]: Step: 16800, loss: 1.03974 acc: 0.65000\n",
      "[Train]: Step: 16900, loss: 0.67514 acc: 0.90000\n",
      "[Train]: Step: 17000, loss: 0.95437 acc: 0.65000\n",
      "[Test] Step: 17000, acc: 0.62370\n",
      "[Train]: Step: 17100, loss: 0.62143 acc: 0.85000\n",
      "[Train]: Step: 17200, loss: 1.20546 acc: 0.55000\n",
      "[Train]: Step: 17300, loss: 0.96081 acc: 0.60000\n",
      "[Train]: Step: 17400, loss: 0.67683 acc: 0.80000\n",
      "[Train]: Step: 17500, loss: 0.72604 acc: 0.85000\n",
      "[Train]: Step: 17600, loss: 0.36471 acc: 0.90000\n",
      "[Train]: Step: 17700, loss: 0.89433 acc: 0.55000\n",
      "[Train]: Step: 17800, loss: 1.13587 acc: 0.70000\n",
      "[Train]: Step: 17900, loss: 1.16812 acc: 0.75000\n",
      "[Train]: Step: 18000, loss: 0.83723 acc: 0.70000\n",
      "[Test] Step: 18000, acc: 0.66350\n",
      "[Train]: Step: 18100, loss: 0.69639 acc: 0.75000\n",
      "[Train]: Step: 18200, loss: 0.93830 acc: 0.75000\n",
      "[Train]: Step: 18300, loss: 1.18829 acc: 0.55000\n",
      "[Train]: Step: 18400, loss: 0.95367 acc: 0.70000\n",
      "[Train]: Step: 18500, loss: 1.15666 acc: 0.60000\n",
      "[Train]: Step: 18600, loss: 1.14047 acc: 0.65000\n",
      "[Train]: Step: 18700, loss: 1.01011 acc: 0.60000\n",
      "[Train]: Step: 18800, loss: 1.00910 acc: 0.65000\n",
      "[Train]: Step: 18900, loss: 1.04764 acc: 0.80000\n",
      "[Train]: Step: 19000, loss: 0.66259 acc: 0.75000\n",
      "[Test] Step: 19000, acc: 0.66470\n",
      "[Train]: Step: 19100, loss: 0.91801 acc: 0.75000\n",
      "[Train]: Step: 19200, loss: 0.68329 acc: 0.65000\n",
      "[Train]: Step: 19300, loss: 0.66246 acc: 0.70000\n",
      "[Train]: Step: 19400, loss: 0.90873 acc: 0.70000\n",
      "[Train]: Step: 19500, loss: 0.93700 acc: 0.60000\n",
      "[Train]: Step: 19600, loss: 0.82618 acc: 0.65000\n",
      "[Train]: Step: 19700, loss: 0.57765 acc: 0.85000\n",
      "[Train]: Step: 19800, loss: 0.40339 acc: 0.85000\n",
      "[Train]: Step: 19900, loss: 1.16156 acc: 0.70000\n",
      "[Train]: Step: 20000, loss: 1.01464 acc: 0.70000\n",
      "[Test] Step: 20000, acc: 0.64360\n",
      "[Train]: Step: 20100, loss: 0.61857 acc: 0.80000\n",
      "[Train]: Step: 20200, loss: 0.47958 acc: 0.80000\n",
      "[Train]: Step: 20300, loss: 0.97573 acc: 0.70000\n",
      "[Train]: Step: 20400, loss: 0.58769 acc: 0.80000\n",
      "[Train]: Step: 20500, loss: 0.57925 acc: 0.80000\n",
      "[Train]: Step: 20600, loss: 0.82971 acc: 0.65000\n",
      "[Train]: Step: 20700, loss: 0.79612 acc: 0.70000\n",
      "[Train]: Step: 20800, loss: 0.42060 acc: 0.80000\n",
      "[Train]: Step: 20900, loss: 0.70364 acc: 0.75000\n",
      "[Train]: Step: 21000, loss: 0.91724 acc: 0.70000\n",
      "[Test] Step: 21000, acc: 0.66450\n",
      "[Train]: Step: 21100, loss: 1.07689 acc: 0.60000\n",
      "[Train]: Step: 21200, loss: 0.81545 acc: 0.65000\n",
      "[Train]: Step: 21300, loss: 0.97502 acc: 0.70000\n",
      "[Train]: Step: 21400, loss: 0.95587 acc: 0.65000\n",
      "[Train]: Step: 21500, loss: 0.94083 acc: 0.65000\n",
      "[Train]: Step: 21600, loss: 0.76288 acc: 0.75000\n",
      "[Train]: Step: 21700, loss: 0.53153 acc: 0.85000\n",
      "[Train]: Step: 21800, loss: 0.84670 acc: 0.70000\n",
      "[Train]: Step: 21900, loss: 1.10594 acc: 0.55000\n",
      "[Train]: Step: 22000, loss: 1.29484 acc: 0.75000\n",
      "[Test] Step: 22000, acc: 0.65230\n",
      "[Train]: Step: 22100, loss: 0.66917 acc: 0.85000\n",
      "[Train]: Step: 22200, loss: 0.96160 acc: 0.70000\n",
      "[Train]: Step: 22300, loss: 0.63266 acc: 0.85000\n",
      "[Train]: Step: 22400, loss: 1.42653 acc: 0.40000\n",
      "[Train]: Step: 22500, loss: 1.11597 acc: 0.65000\n",
      "[Train]: Step: 22600, loss: 1.02662 acc: 0.60000\n",
      "[Train]: Step: 22700, loss: 1.10613 acc: 0.60000\n",
      "[Train]: Step: 22800, loss: 0.57227 acc: 0.80000\n",
      "[Train]: Step: 22900, loss: 1.00989 acc: 0.65000\n",
      "[Train]: Step: 23000, loss: 1.02877 acc: 0.65000\n",
      "[Test] Step: 23000, acc: 0.68550\n",
      "[Train]: Step: 23100, loss: 0.51131 acc: 0.85000\n",
      "[Train]: Step: 23200, loss: 0.60494 acc: 0.85000\n",
      "[Train]: Step: 23300, loss: 0.73895 acc: 0.65000\n",
      "[Train]: Step: 23400, loss: 0.64759 acc: 0.70000\n",
      "[Train]: Step: 23500, loss: 0.97444 acc: 0.70000\n",
      "[Train]: Step: 23600, loss: 0.96998 acc: 0.60000\n",
      "[Train]: Step: 23700, loss: 0.66589 acc: 0.75000\n",
      "[Train]: Step: 23800, loss: 0.54363 acc: 0.80000\n",
      "[Train]: Step: 23900, loss: 0.63261 acc: 0.80000\n",
      "[Train]: Step: 24000, loss: 0.71540 acc: 0.70000\n",
      "[Test] Step: 24000, acc: 0.68170\n",
      "[Train]: Step: 24100, loss: 1.00965 acc: 0.70000\n",
      "[Train]: Step: 24200, loss: 1.13666 acc: 0.55000\n",
      "[Train]: Step: 24300, loss: 0.73476 acc: 0.70000\n",
      "[Train]: Step: 24400, loss: 0.61247 acc: 0.70000\n",
      "[Train]: Step: 24500, loss: 0.57120 acc: 0.80000\n",
      "[Train]: Step: 24600, loss: 0.82746 acc: 0.65000\n",
      "[Train]: Step: 24700, loss: 0.53035 acc: 0.80000\n",
      "[Train]: Step: 24800, loss: 1.13370 acc: 0.55000\n",
      "[Train]: Step: 24900, loss: 0.64959 acc: 0.75000\n",
      "[Train]: Step: 25000, loss: 0.92311 acc: 0.65000\n",
      "[Test] Step: 25000, acc: 0.65750\n",
      "[Train]: Step: 25100, loss: 0.45508 acc: 0.85000\n",
      "[Train]: Step: 25200, loss: 1.16010 acc: 0.60000\n",
      "[Train]: Step: 25300, loss: 0.73740 acc: 0.85000\n",
      "[Train]: Step: 25400, loss: 0.59585 acc: 0.80000\n",
      "[Train]: Step: 25500, loss: 1.00123 acc: 0.75000\n",
      "[Train]: Step: 25600, loss: 0.49772 acc: 0.80000\n",
      "[Train]: Step: 25700, loss: 1.41650 acc: 0.50000\n",
      "[Train]: Step: 25800, loss: 0.90738 acc: 0.55000\n",
      "[Train]: Step: 25900, loss: 0.63544 acc: 0.75000\n",
      "[Train]: Step: 26000, loss: 0.66332 acc: 0.70000\n",
      "[Test] Step: 26000, acc: 0.67020\n",
      "[Train]: Step: 26100, loss: 1.09415 acc: 0.70000\n",
      "[Train]: Step: 26200, loss: 0.79010 acc: 0.75000\n",
      "[Train]: Step: 26300, loss: 0.75969 acc: 0.75000\n",
      "[Train]: Step: 26400, loss: 0.49640 acc: 0.85000\n",
      "[Train]: Step: 26500, loss: 1.13760 acc: 0.60000\n",
      "[Train]: Step: 26600, loss: 0.91244 acc: 0.65000\n",
      "[Train]: Step: 26700, loss: 0.93321 acc: 0.65000\n",
      "[Train]: Step: 26800, loss: 0.86107 acc: 0.60000\n",
      "[Train]: Step: 26900, loss: 0.65454 acc: 0.80000\n",
      "[Train]: Step: 27000, loss: 1.18495 acc: 0.55000\n",
      "[Test] Step: 27000, acc: 0.68310\n",
      "[Train]: Step: 27100, loss: 0.58384 acc: 0.80000\n",
      "[Train]: Step: 27200, loss: 0.72499 acc: 0.75000\n",
      "[Train]: Step: 27300, loss: 0.96173 acc: 0.65000\n",
      "[Train]: Step: 27400, loss: 0.49610 acc: 0.85000\n",
      "[Train]: Step: 27500, loss: 0.90727 acc: 0.75000\n",
      "[Train]: Step: 27600, loss: 0.87318 acc: 0.75000\n",
      "[Train]: Step: 27700, loss: 0.59993 acc: 0.80000\n",
      "[Train]: Step: 27800, loss: 0.55395 acc: 0.85000\n",
      "[Train]: Step: 27900, loss: 0.62136 acc: 0.70000\n",
      "[Train]: Step: 28000, loss: 0.48292 acc: 0.80000\n",
      "[Test] Step: 28000, acc: 0.68350\n",
      "[Train]: Step: 28100, loss: 0.87455 acc: 0.75000\n",
      "[Train]: Step: 28200, loss: 0.95523 acc: 0.60000\n",
      "[Train]: Step: 28300, loss: 0.58180 acc: 0.75000\n",
      "[Train]: Step: 28400, loss: 0.67620 acc: 0.70000\n",
      "[Train]: Step: 28500, loss: 0.57789 acc: 0.85000\n",
      "[Train]: Step: 28600, loss: 0.74124 acc: 0.65000\n",
      "[Train]: Step: 28700, loss: 0.62050 acc: 0.70000\n",
      "[Train]: Step: 28800, loss: 0.73419 acc: 0.70000\n",
      "[Train]: Step: 28900, loss: 0.81588 acc: 0.70000\n",
      "[Train]: Step: 29000, loss: 0.46596 acc: 0.85000\n",
      "[Test] Step: 29000, acc: 0.67380\n",
      "[Train]: Step: 29100, loss: 0.99580 acc: 0.50000\n",
      "[Train]: Step: 29200, loss: 0.49471 acc: 0.80000\n",
      "[Train]: Step: 29300, loss: 0.89863 acc: 0.65000\n",
      "[Train]: Step: 29400, loss: 0.89362 acc: 0.75000\n",
      "[Train]: Step: 29500, loss: 0.75797 acc: 0.75000\n",
      "[Train]: Step: 29600, loss: 0.45209 acc: 0.85000\n",
      "[Train]: Step: 29700, loss: 0.60671 acc: 0.75000\n",
      "[Train]: Step: 29800, loss: 1.09221 acc: 0.65000\n",
      "[Train]: Step: 29900, loss: 0.93236 acc: 0.60000\n",
      "[Train]: Step: 30000, loss: 0.48306 acc: 0.85000\n",
      "[Test] Step: 30000, acc: 0.68960\n",
      "[Train]: Step: 30100, loss: 0.78861 acc: 0.85000\n",
      "[Train]: Step: 30200, loss: 0.94387 acc: 0.65000\n",
      "[Train]: Step: 30300, loss: 0.52955 acc: 0.85000\n",
      "[Train]: Step: 30400, loss: 0.81970 acc: 0.70000\n",
      "[Train]: Step: 30500, loss: 0.54758 acc: 0.75000\n",
      "[Train]: Step: 30600, loss: 0.84573 acc: 0.75000\n",
      "[Train]: Step: 30700, loss: 1.03747 acc: 0.80000\n",
      "[Train]: Step: 30800, loss: 0.30012 acc: 0.90000\n",
      "[Train]: Step: 30900, loss: 0.97576 acc: 0.70000\n",
      "[Train]: Step: 31000, loss: 0.63233 acc: 0.80000\n",
      "[Test] Step: 31000, acc: 0.69110\n",
      "[Train]: Step: 31100, loss: 0.44077 acc: 0.90000\n",
      "[Train]: Step: 31200, loss: 0.52862 acc: 0.80000\n",
      "[Train]: Step: 31300, loss: 0.95050 acc: 0.70000\n",
      "[Train]: Step: 31400, loss: 1.14713 acc: 0.65000\n",
      "[Train]: Step: 31500, loss: 0.43242 acc: 0.95000\n",
      "[Train]: Step: 31600, loss: 0.86213 acc: 0.70000\n",
      "[Train]: Step: 31700, loss: 0.71575 acc: 0.80000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train]: Step: 31800, loss: 0.71257 acc: 0.70000\n",
      "[Train]: Step: 31900, loss: 1.12327 acc: 0.55000\n",
      "[Train]: Step: 32000, loss: 0.41855 acc: 0.85000\n",
      "[Test] Step: 32000, acc: 0.67810\n",
      "[Train]: Step: 32100, loss: 1.32286 acc: 0.60000\n",
      "[Train]: Step: 32200, loss: 0.75725 acc: 0.75000\n",
      "[Train]: Step: 32300, loss: 0.59772 acc: 0.80000\n",
      "[Train]: Step: 32400, loss: 0.41685 acc: 0.85000\n",
      "[Train]: Step: 32500, loss: 0.84099 acc: 0.75000\n",
      "[Train]: Step: 32600, loss: 1.03620 acc: 0.75000\n",
      "[Train]: Step: 32700, loss: 0.72760 acc: 0.85000\n",
      "[Train]: Step: 32800, loss: 0.57830 acc: 0.80000\n",
      "[Train]: Step: 32900, loss: 1.24137 acc: 0.40000\n",
      "[Train]: Step: 33000, loss: 0.64537 acc: 0.75000\n",
      "[Test] Step: 33000, acc: 0.68650\n",
      "[Train]: Step: 33100, loss: 0.88770 acc: 0.70000\n",
      "[Train]: Step: 33200, loss: 0.87791 acc: 0.70000\n",
      "[Train]: Step: 33300, loss: 0.91494 acc: 0.60000\n",
      "[Train]: Step: 33400, loss: 0.82586 acc: 0.70000\n",
      "[Train]: Step: 33500, loss: 0.75836 acc: 0.80000\n",
      "[Train]: Step: 33600, loss: 1.01877 acc: 0.65000\n",
      "[Train]: Step: 33700, loss: 1.10838 acc: 0.65000\n",
      "[Train]: Step: 33800, loss: 0.61270 acc: 0.65000\n",
      "[Train]: Step: 33900, loss: 1.01426 acc: 0.70000\n",
      "[Train]: Step: 34000, loss: 1.42674 acc: 0.50000\n",
      "[Test] Step: 34000, acc: 0.68530\n",
      "[Train]: Step: 34100, loss: 1.45012 acc: 0.50000\n",
      "[Train]: Step: 34200, loss: 1.23514 acc: 0.55000\n",
      "[Train]: Step: 34300, loss: 0.47895 acc: 0.80000\n",
      "[Train]: Step: 34400, loss: 0.78720 acc: 0.70000\n",
      "[Train]: Step: 34500, loss: 0.60871 acc: 0.75000\n",
      "[Train]: Step: 34600, loss: 1.09969 acc: 0.60000\n",
      "[Train]: Step: 34700, loss: 0.58475 acc: 0.80000\n",
      "[Train]: Step: 34800, loss: 0.90413 acc: 0.60000\n",
      "[Train]: Step: 34900, loss: 0.72246 acc: 0.65000\n",
      "[Train]: Step: 35000, loss: 1.02901 acc: 0.60000\n",
      "[Test] Step: 35000, acc: 0.69810\n",
      "[Train]: Step: 35100, loss: 0.53913 acc: 0.95000\n",
      "[Train]: Step: 35200, loss: 0.49490 acc: 0.90000\n",
      "[Train]: Step: 35300, loss: 0.69910 acc: 0.80000\n",
      "[Train]: Step: 35400, loss: 0.45647 acc: 0.85000\n",
      "[Train]: Step: 35500, loss: 0.98040 acc: 0.80000\n",
      "[Train]: Step: 35600, loss: 0.71445 acc: 0.75000\n",
      "[Train]: Step: 35700, loss: 0.46832 acc: 0.80000\n",
      "[Train]: Step: 35800, loss: 1.02617 acc: 0.55000\n",
      "[Train]: Step: 35900, loss: 0.65807 acc: 0.70000\n",
      "[Train]: Step: 36000, loss: 0.58391 acc: 0.75000\n",
      "[Test] Step: 36000, acc: 0.68700\n",
      "[Train]: Step: 36100, loss: 0.60920 acc: 0.80000\n",
      "[Train]: Step: 36200, loss: 1.24371 acc: 0.75000\n",
      "[Train]: Step: 36300, loss: 0.40051 acc: 0.90000\n",
      "[Train]: Step: 36400, loss: 0.97459 acc: 0.75000\n",
      "[Train]: Step: 36500, loss: 0.79443 acc: 0.65000\n",
      "[Train]: Step: 36600, loss: 0.65886 acc: 0.75000\n",
      "[Train]: Step: 36700, loss: 0.77999 acc: 0.75000\n",
      "[Train]: Step: 36800, loss: 0.53294 acc: 0.80000\n",
      "[Train]: Step: 36900, loss: 0.75006 acc: 0.75000\n",
      "[Train]: Step: 37000, loss: 0.70578 acc: 0.65000\n",
      "[Test] Step: 37000, acc: 0.67760\n",
      "[Train]: Step: 37100, loss: 0.79030 acc: 0.65000\n",
      "[Train]: Step: 37200, loss: 0.76168 acc: 0.60000\n",
      "[Train]: Step: 37300, loss: 0.80074 acc: 0.65000\n",
      "[Train]: Step: 37400, loss: 0.50538 acc: 0.85000\n",
      "[Train]: Step: 37500, loss: 1.08918 acc: 0.60000\n",
      "[Train]: Step: 37600, loss: 0.51746 acc: 0.90000\n",
      "[Train]: Step: 37700, loss: 0.34210 acc: 0.95000\n",
      "[Train]: Step: 37800, loss: 0.71362 acc: 0.70000\n",
      "[Train]: Step: 37900, loss: 0.61329 acc: 0.85000\n",
      "[Train]: Step: 38000, loss: 0.73438 acc: 0.80000\n",
      "[Test] Step: 38000, acc: 0.69050\n",
      "[Train]: Step: 38100, loss: 0.35846 acc: 0.85000\n",
      "[Train]: Step: 38200, loss: 0.84240 acc: 0.65000\n",
      "[Train]: Step: 38300, loss: 0.55129 acc: 0.80000\n",
      "[Train]: Step: 38400, loss: 1.09936 acc: 0.75000\n",
      "[Train]: Step: 38500, loss: 0.67639 acc: 0.80000\n",
      "[Train]: Step: 38600, loss: 0.67606 acc: 0.75000\n",
      "[Train]: Step: 38700, loss: 0.90635 acc: 0.60000\n",
      "[Train]: Step: 38800, loss: 1.00846 acc: 0.70000\n",
      "[Train]: Step: 38900, loss: 0.41822 acc: 0.95000\n",
      "[Train]: Step: 39000, loss: 0.68488 acc: 0.75000\n",
      "[Test] Step: 39000, acc: 0.69430\n",
      "[Train]: Step: 39100, loss: 0.76241 acc: 0.70000\n",
      "[Train]: Step: 39200, loss: 0.63552 acc: 0.75000\n",
      "[Train]: Step: 39300, loss: 0.62006 acc: 0.85000\n",
      "[Train]: Step: 39400, loss: 0.74648 acc: 0.80000\n",
      "[Train]: Step: 39500, loss: 0.49768 acc: 0.85000\n",
      "[Train]: Step: 39600, loss: 1.02414 acc: 0.65000\n",
      "[Train]: Step: 39700, loss: 0.81342 acc: 0.75000\n",
      "[Train]: Step: 39800, loss: 0.64042 acc: 0.90000\n",
      "[Train]: Step: 39900, loss: 1.13702 acc: 0.55000\n",
      "[Train]: Step: 40000, loss: 0.26618 acc: 0.90000\n",
      "[Test] Step: 40000, acc: 0.68470\n",
      "[Train]: Step: 40100, loss: 0.44417 acc: 0.85000\n",
      "[Train]: Step: 40200, loss: 0.51183 acc: 0.85000\n",
      "[Train]: Step: 40300, loss: 0.94508 acc: 0.80000\n",
      "[Train]: Step: 40400, loss: 0.29704 acc: 1.00000\n",
      "[Train]: Step: 40500, loss: 0.53446 acc: 0.85000\n",
      "[Train]: Step: 40600, loss: 0.72426 acc: 0.70000\n",
      "[Train]: Step: 40700, loss: 0.38484 acc: 0.90000\n",
      "[Train]: Step: 40800, loss: 0.76890 acc: 0.75000\n",
      "[Train]: Step: 40900, loss: 0.67145 acc: 0.75000\n",
      "[Train]: Step: 41000, loss: 0.73926 acc: 0.70000\n",
      "[Test] Step: 41000, acc: 0.68560\n",
      "[Train]: Step: 41100, loss: 1.11735 acc: 0.75000\n",
      "[Train]: Step: 41200, loss: 0.42683 acc: 0.85000\n",
      "[Train]: Step: 41300, loss: 0.36120 acc: 0.90000\n",
      "[Train]: Step: 41400, loss: 0.60232 acc: 0.75000\n",
      "[Train]: Step: 41500, loss: 0.88036 acc: 0.65000\n",
      "[Train]: Step: 41600, loss: 0.92040 acc: 0.65000\n",
      "[Train]: Step: 41700, loss: 0.26278 acc: 1.00000\n",
      "[Train]: Step: 41800, loss: 0.84318 acc: 0.60000\n",
      "[Train]: Step: 41900, loss: 1.13793 acc: 0.65000\n",
      "[Train]: Step: 42000, loss: 0.73931 acc: 0.80000\n",
      "[Test] Step: 42000, acc: 0.70520\n",
      "[Train]: Step: 42100, loss: 0.67338 acc: 0.75000\n",
      "[Train]: Step: 42200, loss: 0.60959 acc: 0.70000\n",
      "[Train]: Step: 42300, loss: 0.62918 acc: 0.80000\n",
      "[Train]: Step: 42400, loss: 0.76593 acc: 0.65000\n",
      "[Train]: Step: 42500, loss: 0.71173 acc: 0.75000\n",
      "[Train]: Step: 42600, loss: 0.81157 acc: 0.80000\n",
      "[Train]: Step: 42700, loss: 0.70977 acc: 0.75000\n",
      "[Train]: Step: 42800, loss: 0.96595 acc: 0.65000\n",
      "[Train]: Step: 42900, loss: 0.73699 acc: 0.80000\n",
      "[Train]: Step: 43000, loss: 0.44324 acc: 0.95000\n",
      "[Test] Step: 43000, acc: 0.69500\n",
      "[Train]: Step: 43100, loss: 0.55904 acc: 0.70000\n",
      "[Train]: Step: 43200, loss: 1.29454 acc: 0.55000\n",
      "[Train]: Step: 43300, loss: 0.38521 acc: 0.95000\n",
      "[Train]: Step: 43400, loss: 0.62538 acc: 0.75000\n",
      "[Train]: Step: 43500, loss: 0.91062 acc: 0.65000\n",
      "[Train]: Step: 43600, loss: 0.70374 acc: 0.70000\n",
      "[Train]: Step: 43700, loss: 1.38146 acc: 0.50000\n",
      "[Train]: Step: 43800, loss: 0.96721 acc: 0.60000\n",
      "[Train]: Step: 43900, loss: 0.64086 acc: 0.70000\n",
      "[Train]: Step: 44000, loss: 0.82539 acc: 0.70000\n",
      "[Test] Step: 44000, acc: 0.69690\n",
      "[Train]: Step: 44100, loss: 0.71822 acc: 0.80000\n",
      "[Train]: Step: 44200, loss: 0.97282 acc: 0.75000\n",
      "[Train]: Step: 44300, loss: 0.72995 acc: 0.65000\n",
      "[Train]: Step: 44400, loss: 0.71417 acc: 0.80000\n",
      "[Train]: Step: 44500, loss: 0.72905 acc: 0.75000\n",
      "[Train]: Step: 44600, loss: 1.07466 acc: 0.60000\n",
      "[Train]: Step: 44700, loss: 0.50878 acc: 0.80000\n",
      "[Train]: Step: 44800, loss: 1.43503 acc: 0.60000\n",
      "[Train]: Step: 44900, loss: 0.77478 acc: 0.65000\n",
      "[Train]: Step: 45000, loss: 0.61430 acc: 0.80000\n",
      "[Test] Step: 45000, acc: 0.69230\n",
      "[Train]: Step: 45100, loss: 0.28644 acc: 0.95000\n",
      "[Train]: Step: 45200, loss: 0.70735 acc: 0.75000\n",
      "[Train]: Step: 45300, loss: 0.30189 acc: 0.90000\n",
      "[Train]: Step: 45400, loss: 0.79699 acc: 0.75000\n",
      "[Train]: Step: 45500, loss: 0.68629 acc: 0.75000\n",
      "[Train]: Step: 45600, loss: 0.39751 acc: 0.90000\n",
      "[Train]: Step: 45700, loss: 0.30281 acc: 0.95000\n",
      "[Train]: Step: 45800, loss: 0.90956 acc: 0.65000\n",
      "[Train]: Step: 45900, loss: 0.76493 acc: 0.80000\n",
      "[Train]: Step: 46000, loss: 0.90507 acc: 0.60000\n",
      "[Test] Step: 46000, acc: 0.69180\n",
      "[Train]: Step: 46100, loss: 0.65641 acc: 0.75000\n",
      "[Train]: Step: 46200, loss: 0.68781 acc: 0.75000\n",
      "[Train]: Step: 46300, loss: 0.97793 acc: 0.65000\n",
      "[Train]: Step: 46400, loss: 0.73317 acc: 0.75000\n",
      "[Train]: Step: 46500, loss: 0.43950 acc: 0.85000\n",
      "[Train]: Step: 46600, loss: 0.49926 acc: 0.85000\n",
      "[Train]: Step: 46700, loss: 1.03555 acc: 0.65000\n",
      "[Train]: Step: 46800, loss: 0.57447 acc: 0.80000\n",
      "[Train]: Step: 46900, loss: 0.61907 acc: 0.75000\n",
      "[Train]: Step: 47000, loss: 0.85065 acc: 0.65000\n",
      "[Test] Step: 47000, acc: 0.68910\n",
      "[Train]: Step: 47100, loss: 0.68436 acc: 0.85000\n",
      "[Train]: Step: 47200, loss: 1.13062 acc: 0.55000\n",
      "[Train]: Step: 47300, loss: 0.69814 acc: 0.65000\n",
      "[Train]: Step: 47400, loss: 0.91338 acc: 0.70000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train]: Step: 47500, loss: 0.82158 acc: 0.65000\n",
      "[Train]: Step: 47600, loss: 0.55418 acc: 0.75000\n",
      "[Train]: Step: 47700, loss: 0.54107 acc: 0.80000\n",
      "[Train]: Step: 47800, loss: 0.65212 acc: 0.80000\n",
      "[Train]: Step: 47900, loss: 1.07867 acc: 0.70000\n",
      "[Train]: Step: 48000, loss: 0.60550 acc: 0.70000\n",
      "[Test] Step: 48000, acc: 0.70600\n",
      "[Train]: Step: 48100, loss: 0.62540 acc: 0.85000\n",
      "[Train]: Step: 48200, loss: 0.59075 acc: 0.90000\n",
      "[Train]: Step: 48300, loss: 0.47019 acc: 0.85000\n",
      "[Train]: Step: 48400, loss: 0.51732 acc: 0.85000\n",
      "[Train]: Step: 48500, loss: 0.84916 acc: 0.65000\n",
      "[Train]: Step: 48600, loss: 1.22105 acc: 0.60000\n",
      "[Train]: Step: 48700, loss: 0.77431 acc: 0.70000\n",
      "[Train]: Step: 48800, loss: 0.58187 acc: 0.90000\n",
      "[Train]: Step: 48900, loss: 0.45699 acc: 0.80000\n",
      "[Train]: Step: 49000, loss: 0.79965 acc: 0.65000\n",
      "[Test] Step: 49000, acc: 0.69350\n",
      "[Train]: Step: 49100, loss: 0.68558 acc: 0.65000\n",
      "[Train]: Step: 49200, loss: 0.96451 acc: 0.70000\n",
      "[Train]: Step: 49300, loss: 0.93605 acc: 0.60000\n",
      "[Train]: Step: 49400, loss: 0.50292 acc: 0.75000\n",
      "[Train]: Step: 49500, loss: 1.04488 acc: 0.70000\n",
      "[Train]: Step: 49600, loss: 0.33993 acc: 0.95000\n",
      "[Train]: Step: 49700, loss: 0.88421 acc: 0.65000\n",
      "[Train]: Step: 49800, loss: 0.95235 acc: 0.60000\n",
      "[Train]: Step: 49900, loss: 0.38579 acc: 0.90000\n",
      "[Train]: Step: 50000, loss: 0.62161 acc: 0.65000\n",
      "[Test] Step: 50000, acc: 0.68450\n",
      "[Train]: Step: 50100, loss: 1.10977 acc: 0.65000\n",
      "[Train]: Step: 50200, loss: 1.16509 acc: 0.60000\n",
      "[Train]: Step: 50300, loss: 0.65568 acc: 0.75000\n",
      "[Train]: Step: 50400, loss: 0.67490 acc: 0.75000\n",
      "[Train]: Step: 50500, loss: 0.60589 acc: 0.70000\n",
      "[Train]: Step: 50600, loss: 0.78349 acc: 0.75000\n",
      "[Train]: Step: 50700, loss: 0.99869 acc: 0.65000\n",
      "[Train]: Step: 50800, loss: 0.91879 acc: 0.70000\n",
      "[Train]: Step: 50900, loss: 0.49561 acc: 0.90000\n",
      "[Train]: Step: 51000, loss: 0.57276 acc: 0.85000\n",
      "[Test] Step: 51000, acc: 0.69440\n",
      "[Train]: Step: 51100, loss: 0.79516 acc: 0.65000\n",
      "[Train]: Step: 51200, loss: 0.50381 acc: 0.90000\n",
      "[Train]: Step: 51300, loss: 0.88361 acc: 0.75000\n",
      "[Train]: Step: 51400, loss: 0.88298 acc: 0.60000\n",
      "[Train]: Step: 51500, loss: 0.63083 acc: 0.70000\n",
      "[Train]: Step: 51600, loss: 0.67446 acc: 0.70000\n",
      "[Train]: Step: 51700, loss: 0.46454 acc: 0.85000\n",
      "[Train]: Step: 51800, loss: 0.60261 acc: 0.90000\n",
      "[Train]: Step: 51900, loss: 0.78040 acc: 0.80000\n",
      "[Train]: Step: 52000, loss: 0.87704 acc: 0.60000\n",
      "[Test] Step: 52000, acc: 0.69340\n",
      "[Train]: Step: 52100, loss: 0.96464 acc: 0.65000\n",
      "[Train]: Step: 52200, loss: 0.74507 acc: 0.85000\n",
      "[Train]: Step: 52300, loss: 0.95069 acc: 0.70000\n",
      "[Train]: Step: 52400, loss: 0.96324 acc: 0.60000\n",
      "[Train]: Step: 52500, loss: 0.30754 acc: 0.85000\n",
      "[Train]: Step: 52600, loss: 0.55816 acc: 0.85000\n",
      "[Train]: Step: 52700, loss: 1.05579 acc: 0.70000\n",
      "[Train]: Step: 52800, loss: 0.71914 acc: 0.70000\n",
      "[Train]: Step: 52900, loss: 0.72262 acc: 0.70000\n",
      "[Train]: Step: 53000, loss: 0.82939 acc: 0.60000\n",
      "[Test] Step: 53000, acc: 0.69860\n",
      "[Train]: Step: 53100, loss: 1.14648 acc: 0.65000\n",
      "[Train]: Step: 53200, loss: 0.86267 acc: 0.60000\n",
      "[Train]: Step: 53300, loss: 0.40844 acc: 0.85000\n",
      "[Train]: Step: 53400, loss: 0.51397 acc: 0.85000\n",
      "[Train]: Step: 53500, loss: 0.62535 acc: 0.75000\n",
      "[Train]: Step: 53600, loss: 0.71385 acc: 0.75000\n",
      "[Train]: Step: 53700, loss: 0.94988 acc: 0.75000\n",
      "[Train]: Step: 53800, loss: 0.42682 acc: 0.85000\n",
      "[Train]: Step: 53900, loss: 0.82758 acc: 0.70000\n",
      "[Train]: Step: 54000, loss: 1.06067 acc: 0.70000\n",
      "[Test] Step: 54000, acc: 0.69390\n",
      "[Train]: Step: 54100, loss: 0.55302 acc: 0.80000\n",
      "[Train]: Step: 54200, loss: 0.75753 acc: 0.70000\n",
      "[Train]: Step: 54300, loss: 1.16578 acc: 0.45000\n",
      "[Train]: Step: 54400, loss: 0.74027 acc: 0.75000\n",
      "[Train]: Step: 54500, loss: 1.20060 acc: 0.70000\n",
      "[Train]: Step: 54600, loss: 1.21445 acc: 0.55000\n",
      "[Train]: Step: 54700, loss: 0.48074 acc: 0.80000\n",
      "[Train]: Step: 54800, loss: 0.45212 acc: 0.80000\n",
      "[Train]: Step: 54900, loss: 0.88705 acc: 0.70000\n",
      "[Train]: Step: 55000, loss: 0.64746 acc: 0.75000\n",
      "[Test] Step: 55000, acc: 0.69770\n",
      "[Train]: Step: 55100, loss: 1.52218 acc: 0.55000\n",
      "[Train]: Step: 55200, loss: 0.89493 acc: 0.70000\n",
      "[Train]: Step: 55300, loss: 0.47021 acc: 0.90000\n",
      "[Train]: Step: 55400, loss: 0.63663 acc: 0.80000\n",
      "[Train]: Step: 55500, loss: 0.54268 acc: 0.80000\n",
      "[Train]: Step: 55600, loss: 0.48902 acc: 0.85000\n",
      "[Train]: Step: 55700, loss: 0.97271 acc: 0.65000\n",
      "[Train]: Step: 55800, loss: 0.62504 acc: 0.80000\n",
      "[Train]: Step: 55900, loss: 0.55019 acc: 0.75000\n",
      "[Train]: Step: 56000, loss: 0.76744 acc: 0.70000\n",
      "[Test] Step: 56000, acc: 0.70580\n",
      "[Train]: Step: 56100, loss: 0.80867 acc: 0.70000\n",
      "[Train]: Step: 56200, loss: 0.91445 acc: 0.70000\n",
      "[Train]: Step: 56300, loss: 0.99460 acc: 0.75000\n",
      "[Train]: Step: 56400, loss: 0.84008 acc: 0.80000\n",
      "[Train]: Step: 56500, loss: 1.12971 acc: 0.60000\n",
      "[Train]: Step: 56600, loss: 0.66735 acc: 0.70000\n",
      "[Train]: Step: 56700, loss: 0.70276 acc: 0.70000\n",
      "[Train]: Step: 56800, loss: 0.78337 acc: 0.65000\n",
      "[Train]: Step: 56900, loss: 0.53036 acc: 0.90000\n",
      "[Train]: Step: 57000, loss: 0.71167 acc: 0.70000\n",
      "[Test] Step: 57000, acc: 0.70310\n",
      "[Train]: Step: 57100, loss: 0.78904 acc: 0.70000\n",
      "[Train]: Step: 57200, loss: 0.73030 acc: 0.65000\n",
      "[Train]: Step: 57300, loss: 0.64500 acc: 0.80000\n",
      "[Train]: Step: 57400, loss: 0.75940 acc: 0.75000\n",
      "[Train]: Step: 57500, loss: 0.51135 acc: 0.75000\n",
      "[Train]: Step: 57600, loss: 0.84555 acc: 0.75000\n",
      "[Train]: Step: 57700, loss: 0.43705 acc: 0.80000\n",
      "[Train]: Step: 57800, loss: 0.37067 acc: 0.85000\n",
      "[Train]: Step: 57900, loss: 0.66710 acc: 0.80000\n",
      "[Train]: Step: 58000, loss: 1.03211 acc: 0.70000\n",
      "[Test] Step: 58000, acc: 0.69500\n",
      "[Train]: Step: 58100, loss: 0.90897 acc: 0.65000\n",
      "[Train]: Step: 58200, loss: 0.62388 acc: 0.75000\n",
      "[Train]: Step: 58300, loss: 0.55218 acc: 0.80000\n",
      "[Train]: Step: 58400, loss: 0.39792 acc: 0.90000\n",
      "[Train]: Step: 58500, loss: 0.84885 acc: 0.65000\n",
      "[Train]: Step: 58600, loss: 0.81278 acc: 0.70000\n",
      "[Train]: Step: 58700, loss: 0.71985 acc: 0.75000\n",
      "[Train]: Step: 58800, loss: 0.64434 acc: 0.80000\n",
      "[Train]: Step: 58900, loss: 0.81200 acc: 0.70000\n",
      "[Train]: Step: 59000, loss: 0.90275 acc: 0.55000\n",
      "[Test] Step: 59000, acc: 0.70390\n",
      "[Train]: Step: 59100, loss: 0.86952 acc: 0.60000\n",
      "[Train]: Step: 59200, loss: 0.52335 acc: 0.75000\n",
      "[Train]: Step: 59300, loss: 0.73621 acc: 0.75000\n",
      "[Train]: Step: 59400, loss: 0.98416 acc: 0.75000\n",
      "[Train]: Step: 59500, loss: 1.07297 acc: 0.60000\n",
      "[Train]: Step: 59600, loss: 0.58938 acc: 0.80000\n",
      "[Train]: Step: 59700, loss: 0.78784 acc: 0.75000\n",
      "[Train]: Step: 59800, loss: 0.73083 acc: 0.85000\n",
      "[Train]: Step: 59900, loss: 0.64986 acc: 0.75000\n",
      "[Train]: Step: 60000, loss: 1.03147 acc: 0.70000\n",
      "[Test] Step: 60000, acc: 0.68360\n",
      "[Train]: Step: 60100, loss: 0.73366 acc: 0.80000\n",
      "[Train]: Step: 60200, loss: 0.76896 acc: 0.65000\n",
      "[Train]: Step: 60300, loss: 0.47709 acc: 0.85000\n",
      "[Train]: Step: 60400, loss: 0.77583 acc: 0.80000\n",
      "[Train]: Step: 60500, loss: 0.79873 acc: 0.70000\n",
      "[Train]: Step: 60600, loss: 0.45315 acc: 0.80000\n",
      "[Train]: Step: 60700, loss: 1.04429 acc: 0.65000\n",
      "[Train]: Step: 60800, loss: 0.85150 acc: 0.70000\n",
      "[Train]: Step: 60900, loss: 0.51701 acc: 0.80000\n",
      "[Train]: Step: 61000, loss: 0.59057 acc: 0.75000\n",
      "[Test] Step: 61000, acc: 0.68970\n",
      "[Train]: Step: 61100, loss: 0.61714 acc: 0.65000\n",
      "[Train]: Step: 61200, loss: 0.65870 acc: 0.80000\n",
      "[Train]: Step: 61300, loss: 1.29613 acc: 0.55000\n",
      "[Train]: Step: 61400, loss: 1.02610 acc: 0.75000\n",
      "[Train]: Step: 61500, loss: 1.01989 acc: 0.65000\n",
      "[Train]: Step: 61600, loss: 0.49085 acc: 0.80000\n",
      "[Train]: Step: 61700, loss: 0.76828 acc: 0.70000\n",
      "[Train]: Step: 61800, loss: 0.58286 acc: 0.70000\n",
      "[Train]: Step: 61900, loss: 0.70449 acc: 0.70000\n",
      "[Train]: Step: 62000, loss: 0.77062 acc: 0.75000\n",
      "[Test] Step: 62000, acc: 0.69140\n",
      "[Train]: Step: 62100, loss: 0.49300 acc: 0.85000\n",
      "[Train]: Step: 62200, loss: 0.94691 acc: 0.70000\n",
      "[Train]: Step: 62300, loss: 1.24762 acc: 0.55000\n",
      "[Train]: Step: 62400, loss: 0.88543 acc: 0.60000\n",
      "[Train]: Step: 62500, loss: 1.02138 acc: 0.60000\n",
      "[Train]: Step: 62600, loss: 0.66826 acc: 0.75000\n",
      "[Train]: Step: 62700, loss: 0.94026 acc: 0.60000\n",
      "[Train]: Step: 62800, loss: 0.44061 acc: 0.75000\n",
      "[Train]: Step: 62900, loss: 0.46507 acc: 0.70000\n",
      "[Train]: Step: 63000, loss: 0.66667 acc: 0.80000\n",
      "[Test] Step: 63000, acc: 0.70160\n",
      "[Train]: Step: 63100, loss: 0.81217 acc: 0.70000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train]: Step: 63200, loss: 0.51676 acc: 0.85000\n",
      "[Train]: Step: 63300, loss: 0.53165 acc: 0.75000\n",
      "[Train]: Step: 63400, loss: 0.45421 acc: 0.80000\n",
      "[Train]: Step: 63500, loss: 0.74046 acc: 0.70000\n",
      "[Train]: Step: 63600, loss: 0.96925 acc: 0.70000\n",
      "[Train]: Step: 63700, loss: 0.49509 acc: 0.85000\n",
      "[Train]: Step: 63800, loss: 0.44101 acc: 0.90000\n",
      "[Train]: Step: 63900, loss: 1.39014 acc: 0.70000\n",
      "[Train]: Step: 64000, loss: 0.47966 acc: 0.75000\n",
      "[Test] Step: 64000, acc: 0.70100\n",
      "[Train]: Step: 64100, loss: 0.35832 acc: 0.90000\n",
      "[Train]: Step: 64200, loss: 0.53211 acc: 0.80000\n",
      "[Train]: Step: 64300, loss: 0.86926 acc: 0.85000\n",
      "[Train]: Step: 64400, loss: 0.75079 acc: 0.90000\n",
      "[Train]: Step: 64500, loss: 0.77680 acc: 0.80000\n",
      "[Train]: Step: 64600, loss: 0.74923 acc: 0.75000\n",
      "[Train]: Step: 64700, loss: 0.74941 acc: 0.65000\n",
      "[Train]: Step: 64800, loss: 0.74451 acc: 0.70000\n",
      "[Train]: Step: 64900, loss: 1.14956 acc: 0.65000\n",
      "[Train]: Step: 65000, loss: 0.68938 acc: 0.70000\n",
      "[Test] Step: 65000, acc: 0.69670\n",
      "[Train]: Step: 65100, loss: 0.30715 acc: 0.85000\n",
      "[Train]: Step: 65200, loss: 0.43031 acc: 0.85000\n",
      "[Train]: Step: 65300, loss: 0.60085 acc: 0.75000\n",
      "[Train]: Step: 65400, loss: 0.75186 acc: 0.85000\n",
      "[Train]: Step: 65500, loss: 1.15584 acc: 0.55000\n",
      "[Train]: Step: 65600, loss: 0.70268 acc: 0.65000\n",
      "[Train]: Step: 65700, loss: 0.66414 acc: 0.90000\n",
      "[Train]: Step: 65800, loss: 0.66768 acc: 0.65000\n",
      "[Train]: Step: 65900, loss: 0.80118 acc: 0.70000\n",
      "[Train]: Step: 66000, loss: 0.78780 acc: 0.65000\n",
      "[Test] Step: 66000, acc: 0.70720\n",
      "[Train]: Step: 66100, loss: 0.56635 acc: 0.80000\n",
      "[Train]: Step: 66200, loss: 0.70140 acc: 0.80000\n",
      "[Train]: Step: 66300, loss: 0.53251 acc: 0.80000\n",
      "[Train]: Step: 66400, loss: 0.31245 acc: 0.90000\n",
      "[Train]: Step: 66500, loss: 0.92303 acc: 0.80000\n",
      "[Train]: Step: 66600, loss: 0.96442 acc: 0.65000\n",
      "[Train]: Step: 66700, loss: 0.64359 acc: 0.75000\n",
      "[Train]: Step: 66800, loss: 0.52210 acc: 0.90000\n",
      "[Train]: Step: 66900, loss: 0.47113 acc: 0.80000\n",
      "[Train]: Step: 67000, loss: 0.65364 acc: 0.70000\n",
      "[Test] Step: 67000, acc: 0.69840\n",
      "[Train]: Step: 67100, loss: 0.97506 acc: 0.65000\n",
      "[Train]: Step: 67200, loss: 0.59657 acc: 0.80000\n",
      "[Train]: Step: 67300, loss: 0.39275 acc: 0.90000\n",
      "[Train]: Step: 67400, loss: 0.86162 acc: 0.65000\n",
      "[Train]: Step: 67500, loss: 0.51809 acc: 0.85000\n",
      "[Train]: Step: 67600, loss: 0.17916 acc: 0.95000\n",
      "[Train]: Step: 67700, loss: 1.12657 acc: 0.70000\n",
      "[Train]: Step: 67800, loss: 0.98454 acc: 0.75000\n",
      "[Train]: Step: 67900, loss: 0.34684 acc: 0.80000\n",
      "[Train]: Step: 68000, loss: 0.90781 acc: 0.70000\n",
      "[Test] Step: 68000, acc: 0.69860\n",
      "[Train]: Step: 68100, loss: 0.34589 acc: 0.85000\n",
      "[Train]: Step: 68200, loss: 0.49954 acc: 0.80000\n",
      "[Train]: Step: 68300, loss: 0.52433 acc: 0.80000\n",
      "[Train]: Step: 68400, loss: 0.49577 acc: 0.85000\n",
      "[Train]: Step: 68500, loss: 0.52727 acc: 0.85000\n",
      "[Train]: Step: 68600, loss: 0.53588 acc: 0.75000\n",
      "[Train]: Step: 68700, loss: 0.37014 acc: 0.90000\n",
      "[Train]: Step: 68800, loss: 1.16274 acc: 0.50000\n",
      "[Train]: Step: 68900, loss: 0.53975 acc: 0.65000\n",
      "[Train]: Step: 69000, loss: 0.74287 acc: 0.85000\n",
      "[Test] Step: 69000, acc: 0.70160\n",
      "[Train]: Step: 69100, loss: 0.45438 acc: 0.80000\n",
      "[Train]: Step: 69200, loss: 0.99316 acc: 0.65000\n",
      "[Train]: Step: 69300, loss: 1.29510 acc: 0.60000\n",
      "[Train]: Step: 69400, loss: 0.71240 acc: 0.75000\n",
      "[Train]: Step: 69500, loss: 0.63538 acc: 0.65000\n",
      "[Train]: Step: 69600, loss: 0.87147 acc: 0.80000\n",
      "[Train]: Step: 69700, loss: 1.54190 acc: 0.55000\n",
      "[Train]: Step: 69800, loss: 0.50632 acc: 0.80000\n",
      "[Train]: Step: 69900, loss: 0.86360 acc: 0.60000\n",
      "[Train]: Step: 70000, loss: 0.62680 acc: 0.65000\n",
      "[Test] Step: 70000, acc: 0.70640\n",
      "[Train]: Step: 70100, loss: 0.84543 acc: 0.60000\n",
      "[Train]: Step: 70200, loss: 0.47803 acc: 0.80000\n",
      "[Train]: Step: 70300, loss: 0.77289 acc: 0.70000\n",
      "[Train]: Step: 70400, loss: 0.67434 acc: 0.75000\n",
      "[Train]: Step: 70500, loss: 0.77567 acc: 0.70000\n",
      "[Train]: Step: 70600, loss: 0.70354 acc: 0.75000\n",
      "[Train]: Step: 70700, loss: 0.63539 acc: 0.80000\n",
      "[Train]: Step: 70800, loss: 1.09960 acc: 0.65000\n",
      "[Train]: Step: 70900, loss: 1.20451 acc: 0.60000\n",
      "[Train]: Step: 71000, loss: 0.65020 acc: 0.75000\n",
      "[Test] Step: 71000, acc: 0.70200\n",
      "[Train]: Step: 71100, loss: 0.79583 acc: 0.75000\n",
      "[Train]: Step: 71200, loss: 0.71245 acc: 0.65000\n",
      "[Train]: Step: 71300, loss: 0.69791 acc: 0.70000\n",
      "[Train]: Step: 71400, loss: 0.55829 acc: 0.85000\n",
      "[Train]: Step: 71500, loss: 0.59475 acc: 0.80000\n",
      "[Train]: Step: 71600, loss: 0.59812 acc: 0.75000\n",
      "[Train]: Step: 71700, loss: 1.04899 acc: 0.65000\n",
      "[Train]: Step: 71800, loss: 0.58957 acc: 0.75000\n",
      "[Train]: Step: 71900, loss: 0.73423 acc: 0.70000\n",
      "[Train]: Step: 72000, loss: 0.76486 acc: 0.75000\n",
      "[Test] Step: 72000, acc: 0.70510\n",
      "[Train]: Step: 72100, loss: 0.43063 acc: 0.90000\n",
      "[Train]: Step: 72200, loss: 0.83078 acc: 0.65000\n",
      "[Train]: Step: 72300, loss: 0.45154 acc: 0.85000\n",
      "[Train]: Step: 72400, loss: 0.86323 acc: 0.80000\n",
      "[Train]: Step: 72500, loss: 0.78963 acc: 0.75000\n",
      "[Train]: Step: 72600, loss: 0.68113 acc: 0.80000\n",
      "[Train]: Step: 72700, loss: 0.49173 acc: 0.75000\n",
      "[Train]: Step: 72800, loss: 0.72358 acc: 0.85000\n",
      "[Train]: Step: 72900, loss: 0.76066 acc: 0.65000\n",
      "[Train]: Step: 73000, loss: 0.51557 acc: 0.75000\n",
      "[Test] Step: 73000, acc: 0.70720\n",
      "[Train]: Step: 73100, loss: 0.38730 acc: 0.90000\n",
      "[Train]: Step: 73200, loss: 0.53857 acc: 0.80000\n",
      "[Train]: Step: 73300, loss: 0.67524 acc: 0.75000\n",
      "[Train]: Step: 73400, loss: 0.79057 acc: 0.85000\n",
      "[Train]: Step: 73500, loss: 0.57065 acc: 0.80000\n",
      "[Train]: Step: 73600, loss: 0.48292 acc: 0.80000\n",
      "[Train]: Step: 73700, loss: 0.45064 acc: 0.80000\n",
      "[Train]: Step: 73800, loss: 0.48718 acc: 0.90000\n",
      "[Train]: Step: 73900, loss: 0.84161 acc: 0.65000\n",
      "[Train]: Step: 74000, loss: 0.57049 acc: 0.85000\n",
      "[Test] Step: 74000, acc: 0.70190\n",
      "[Train]: Step: 74100, loss: 0.57439 acc: 0.75000\n",
      "[Train]: Step: 74200, loss: 0.75316 acc: 0.80000\n",
      "[Train]: Step: 74300, loss: 0.89492 acc: 0.70000\n",
      "[Train]: Step: 74400, loss: 0.74405 acc: 0.75000\n",
      "[Train]: Step: 74500, loss: 0.59550 acc: 0.85000\n",
      "[Train]: Step: 74600, loss: 0.77248 acc: 0.75000\n",
      "[Train]: Step: 74700, loss: 0.58452 acc: 0.75000\n",
      "[Train]: Step: 74800, loss: 0.94614 acc: 0.70000\n",
      "[Train]: Step: 74900, loss: 0.54054 acc: 0.80000\n",
      "[Train]: Step: 75000, loss: 0.82912 acc: 0.60000\n",
      "[Test] Step: 75000, acc: 0.71050\n",
      "[Train]: Step: 75100, loss: 0.35948 acc: 0.90000\n",
      "[Train]: Step: 75200, loss: 0.63670 acc: 0.75000\n",
      "[Train]: Step: 75300, loss: 0.60533 acc: 0.85000\n",
      "[Train]: Step: 75400, loss: 0.37262 acc: 0.90000\n",
      "[Train]: Step: 75500, loss: 0.48858 acc: 0.85000\n",
      "[Train]: Step: 75600, loss: 0.69129 acc: 0.70000\n",
      "[Train]: Step: 75700, loss: 0.79714 acc: 0.65000\n",
      "[Train]: Step: 75800, loss: 0.81500 acc: 0.70000\n",
      "[Train]: Step: 75900, loss: 0.50236 acc: 0.75000\n",
      "[Train]: Step: 76000, loss: 0.92113 acc: 0.65000\n",
      "[Test] Step: 76000, acc: 0.69610\n",
      "[Train]: Step: 76100, loss: 1.14975 acc: 0.45000\n",
      "[Train]: Step: 76200, loss: 0.73818 acc: 0.70000\n",
      "[Train]: Step: 76300, loss: 1.37252 acc: 0.60000\n",
      "[Train]: Step: 76400, loss: 0.64701 acc: 0.80000\n",
      "[Train]: Step: 76500, loss: 0.32477 acc: 0.95000\n",
      "[Train]: Step: 76600, loss: 0.44581 acc: 0.70000\n",
      "[Train]: Step: 76700, loss: 0.50200 acc: 0.75000\n",
      "[Train]: Step: 76800, loss: 0.29974 acc: 0.90000\n",
      "[Train]: Step: 76900, loss: 0.85583 acc: 0.70000\n",
      "[Train]: Step: 77000, loss: 0.73657 acc: 0.75000\n",
      "[Test] Step: 77000, acc: 0.71120\n",
      "[Train]: Step: 77100, loss: 0.87128 acc: 0.65000\n",
      "[Train]: Step: 77200, loss: 0.81540 acc: 0.70000\n",
      "[Train]: Step: 77300, loss: 1.03922 acc: 0.65000\n",
      "[Train]: Step: 77400, loss: 0.23466 acc: 1.00000\n",
      "[Train]: Step: 77500, loss: 0.82001 acc: 0.65000\n",
      "[Train]: Step: 77600, loss: 0.59124 acc: 0.75000\n",
      "[Train]: Step: 77700, loss: 0.57379 acc: 0.80000\n",
      "[Train]: Step: 77800, loss: 0.81659 acc: 0.80000\n",
      "[Train]: Step: 77900, loss: 0.41841 acc: 0.95000\n",
      "[Train]: Step: 78000, loss: 0.60708 acc: 0.80000\n",
      "[Test] Step: 78000, acc: 0.71020\n",
      "[Train]: Step: 78100, loss: 0.69805 acc: 0.75000\n",
      "[Train]: Step: 78200, loss: 0.75304 acc: 0.80000\n",
      "[Train]: Step: 78300, loss: 0.87341 acc: 0.65000\n",
      "[Train]: Step: 78400, loss: 0.66164 acc: 0.80000\n",
      "[Train]: Step: 78500, loss: 1.17403 acc: 0.70000\n",
      "[Train]: Step: 78600, loss: 0.55997 acc: 0.85000\n",
      "[Train]: Step: 78700, loss: 0.87402 acc: 0.70000\n",
      "[Train]: Step: 78800, loss: 0.34806 acc: 0.90000\n",
      "[Train]: Step: 78900, loss: 0.69285 acc: 0.80000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train]: Step: 79000, loss: 1.10421 acc: 0.60000\n",
      "[Test] Step: 79000, acc: 0.70970\n",
      "[Train]: Step: 79100, loss: 0.74132 acc: 0.70000\n",
      "[Train]: Step: 79200, loss: 0.56104 acc: 0.80000\n",
      "[Train]: Step: 79300, loss: 0.86082 acc: 0.60000\n",
      "[Train]: Step: 79400, loss: 0.59912 acc: 0.80000\n",
      "[Train]: Step: 79500, loss: 0.96984 acc: 0.60000\n",
      "[Train]: Step: 79600, loss: 0.66733 acc: 0.65000\n",
      "[Train]: Step: 79700, loss: 0.35168 acc: 0.95000\n",
      "[Train]: Step: 79800, loss: 0.54890 acc: 0.80000\n",
      "[Train]: Step: 79900, loss: 0.84817 acc: 0.70000\n",
      "[Train]: Step: 80000, loss: 1.10721 acc: 0.75000\n",
      "[Test] Step: 80000, acc: 0.70560\n",
      "[Train]: Step: 80100, loss: 0.49094 acc: 0.80000\n",
      "[Train]: Step: 80200, loss: 1.07785 acc: 0.70000\n",
      "[Train]: Step: 80300, loss: 0.40551 acc: 0.85000\n",
      "[Train]: Step: 80400, loss: 0.47165 acc: 0.80000\n",
      "[Train]: Step: 80500, loss: 0.59790 acc: 0.70000\n",
      "[Train]: Step: 80600, loss: 0.75118 acc: 0.70000\n",
      "[Train]: Step: 80700, loss: 0.59679 acc: 0.75000\n",
      "[Train]: Step: 80800, loss: 0.32129 acc: 0.95000\n",
      "[Train]: Step: 80900, loss: 0.66552 acc: 0.80000\n",
      "[Train]: Step: 81000, loss: 0.62279 acc: 0.80000\n",
      "[Test] Step: 81000, acc: 0.69260\n",
      "[Train]: Step: 81100, loss: 0.66344 acc: 0.75000\n",
      "[Train]: Step: 81200, loss: 1.01518 acc: 0.60000\n",
      "[Train]: Step: 81300, loss: 0.73212 acc: 0.75000\n",
      "[Train]: Step: 81400, loss: 0.51330 acc: 0.85000\n",
      "[Train]: Step: 81500, loss: 0.74880 acc: 0.80000\n",
      "[Train]: Step: 81600, loss: 0.51948 acc: 0.75000\n",
      "[Train]: Step: 81700, loss: 0.65984 acc: 0.75000\n",
      "[Train]: Step: 81800, loss: 0.38239 acc: 0.90000\n",
      "[Train]: Step: 81900, loss: 0.43926 acc: 0.80000\n",
      "[Train]: Step: 82000, loss: 0.63397 acc: 0.80000\n",
      "[Test] Step: 82000, acc: 0.70830\n",
      "[Train]: Step: 82100, loss: 0.36866 acc: 0.90000\n",
      "[Train]: Step: 82200, loss: 0.46965 acc: 0.75000\n",
      "[Train]: Step: 82300, loss: 0.25182 acc: 0.90000\n",
      "[Train]: Step: 82400, loss: 0.48955 acc: 0.85000\n",
      "[Train]: Step: 82500, loss: 0.64506 acc: 0.85000\n",
      "[Train]: Step: 82600, loss: 0.37411 acc: 0.90000\n",
      "[Train]: Step: 82700, loss: 0.79505 acc: 0.70000\n",
      "[Train]: Step: 82800, loss: 0.98209 acc: 0.65000\n",
      "[Train]: Step: 82900, loss: 0.74255 acc: 0.70000\n",
      "[Train]: Step: 83000, loss: 0.42525 acc: 0.85000\n",
      "[Test] Step: 83000, acc: 0.70990\n",
      "[Train]: Step: 83100, loss: 0.60159 acc: 0.80000\n",
      "[Train]: Step: 83200, loss: 0.77002 acc: 0.80000\n",
      "[Train]: Step: 83300, loss: 0.50385 acc: 0.75000\n",
      "[Train]: Step: 83400, loss: 0.76088 acc: 0.80000\n",
      "[Train]: Step: 83500, loss: 0.44382 acc: 0.90000\n",
      "[Train]: Step: 83600, loss: 0.35910 acc: 0.85000\n",
      "[Train]: Step: 83700, loss: 0.60311 acc: 0.70000\n",
      "[Train]: Step: 83800, loss: 0.81199 acc: 0.65000\n",
      "[Train]: Step: 83900, loss: 0.41484 acc: 0.95000\n",
      "[Train]: Step: 84000, loss: 0.69937 acc: 0.65000\n",
      "[Test] Step: 84000, acc: 0.70680\n",
      "[Train]: Step: 84100, loss: 0.48986 acc: 0.85000\n",
      "[Train]: Step: 84200, loss: 0.77632 acc: 0.65000\n",
      "[Train]: Step: 84300, loss: 0.62032 acc: 0.80000\n",
      "[Train]: Step: 84400, loss: 0.84259 acc: 0.65000\n",
      "[Train]: Step: 84500, loss: 0.35905 acc: 0.85000\n",
      "[Train]: Step: 84600, loss: 0.56019 acc: 0.80000\n",
      "[Train]: Step: 84700, loss: 0.50939 acc: 0.80000\n",
      "[Train]: Step: 84800, loss: 0.58851 acc: 0.75000\n",
      "[Train]: Step: 84900, loss: 0.85167 acc: 0.60000\n",
      "[Train]: Step: 85000, loss: 0.85809 acc: 0.65000\n",
      "[Test] Step: 85000, acc: 0.70970\n",
      "[Train]: Step: 85100, loss: 0.52775 acc: 0.80000\n",
      "[Train]: Step: 85200, loss: 0.49028 acc: 0.80000\n",
      "[Train]: Step: 85300, loss: 0.43708 acc: 0.80000\n",
      "[Train]: Step: 85400, loss: 0.85303 acc: 0.70000\n",
      "[Train]: Step: 85500, loss: 0.44260 acc: 0.80000\n",
      "[Train]: Step: 85600, loss: 0.45471 acc: 0.85000\n",
      "[Train]: Step: 85700, loss: 0.97777 acc: 0.65000\n",
      "[Train]: Step: 85800, loss: 0.78038 acc: 0.70000\n",
      "[Train]: Step: 85900, loss: 1.19757 acc: 0.50000\n",
      "[Train]: Step: 86000, loss: 0.47447 acc: 0.90000\n",
      "[Test] Step: 86000, acc: 0.70210\n",
      "[Train]: Step: 86100, loss: 0.42670 acc: 0.75000\n",
      "[Train]: Step: 86200, loss: 0.66769 acc: 0.80000\n",
      "[Train]: Step: 86300, loss: 0.78524 acc: 0.65000\n",
      "[Train]: Step: 86400, loss: 0.69660 acc: 0.80000\n",
      "[Train]: Step: 86500, loss: 0.57852 acc: 0.85000\n",
      "[Train]: Step: 86600, loss: 0.88416 acc: 0.65000\n",
      "[Train]: Step: 86700, loss: 0.46562 acc: 0.80000\n",
      "[Train]: Step: 86800, loss: 0.70538 acc: 0.85000\n",
      "[Train]: Step: 86900, loss: 0.41024 acc: 0.80000\n",
      "[Train]: Step: 87000, loss: 0.75057 acc: 0.70000\n",
      "[Test] Step: 87000, acc: 0.70830\n",
      "[Train]: Step: 87100, loss: 0.59504 acc: 0.75000\n",
      "[Train]: Step: 87200, loss: 1.00854 acc: 0.65000\n",
      "[Train]: Step: 87300, loss: 0.29211 acc: 0.95000\n",
      "[Train]: Step: 87400, loss: 0.86221 acc: 0.75000\n",
      "[Train]: Step: 87500, loss: 0.39697 acc: 0.95000\n",
      "[Train]: Step: 87600, loss: 0.95398 acc: 0.75000\n",
      "[Train]: Step: 87700, loss: 0.55960 acc: 0.80000\n",
      "[Train]: Step: 87800, loss: 0.89402 acc: 0.65000\n",
      "[Train]: Step: 87900, loss: 0.95143 acc: 0.75000\n",
      "[Train]: Step: 88000, loss: 0.41842 acc: 0.85000\n",
      "[Test] Step: 88000, acc: 0.70350\n",
      "[Train]: Step: 88100, loss: 0.34422 acc: 0.85000\n",
      "[Train]: Step: 88200, loss: 0.45375 acc: 0.75000\n",
      "[Train]: Step: 88300, loss: 0.54741 acc: 0.70000\n",
      "[Train]: Step: 88400, loss: 0.43165 acc: 0.85000\n",
      "[Train]: Step: 88500, loss: 0.78815 acc: 0.75000\n",
      "[Train]: Step: 88600, loss: 1.29139 acc: 0.60000\n",
      "[Train]: Step: 88700, loss: 0.53533 acc: 0.80000\n",
      "[Train]: Step: 88800, loss: 0.47097 acc: 0.85000\n",
      "[Train]: Step: 88900, loss: 0.43478 acc: 0.80000\n",
      "[Train]: Step: 89000, loss: 0.77503 acc: 0.75000\n",
      "[Test] Step: 89000, acc: 0.70050\n",
      "[Train]: Step: 89100, loss: 0.79105 acc: 0.60000\n",
      "[Train]: Step: 89200, loss: 0.89085 acc: 0.65000\n",
      "[Train]: Step: 89300, loss: 0.94881 acc: 0.70000\n",
      "[Train]: Step: 89400, loss: 0.68958 acc: 0.75000\n",
      "[Train]: Step: 89500, loss: 0.89192 acc: 0.65000\n",
      "[Train]: Step: 89600, loss: 0.48755 acc: 0.85000\n",
      "[Train]: Step: 89700, loss: 0.37214 acc: 0.95000\n",
      "[Train]: Step: 89800, loss: 0.53145 acc: 0.75000\n",
      "[Train]: Step: 89900, loss: 0.44976 acc: 0.80000\n",
      "[Train]: Step: 90000, loss: 0.94310 acc: 0.75000\n",
      "[Test] Step: 90000, acc: 0.71230\n",
      "[Train]: Step: 90100, loss: 0.90849 acc: 0.80000\n",
      "[Train]: Step: 90200, loss: 0.49680 acc: 0.85000\n",
      "[Train]: Step: 90300, loss: 0.79384 acc: 0.70000\n",
      "[Train]: Step: 90400, loss: 0.76657 acc: 0.80000\n",
      "[Train]: Step: 90500, loss: 0.69507 acc: 0.75000\n",
      "[Train]: Step: 90600, loss: 0.55285 acc: 0.80000\n",
      "[Train]: Step: 90700, loss: 0.80286 acc: 0.80000\n",
      "[Train]: Step: 90800, loss: 0.40102 acc: 0.85000\n",
      "[Train]: Step: 90900, loss: 0.21867 acc: 0.95000\n",
      "[Train]: Step: 91000, loss: 0.43763 acc: 0.90000\n",
      "[Test] Step: 91000, acc: 0.70050\n",
      "[Train]: Step: 91100, loss: 1.09169 acc: 0.65000\n",
      "[Train]: Step: 91200, loss: 0.34904 acc: 0.90000\n",
      "[Train]: Step: 91300, loss: 0.84032 acc: 0.70000\n",
      "[Train]: Step: 91400, loss: 0.85834 acc: 0.70000\n",
      "[Train]: Step: 91500, loss: 0.33877 acc: 0.85000\n",
      "[Train]: Step: 91600, loss: 0.86296 acc: 0.70000\n",
      "[Train]: Step: 91700, loss: 1.19644 acc: 0.55000\n",
      "[Train]: Step: 91800, loss: 0.87943 acc: 0.75000\n",
      "[Train]: Step: 91900, loss: 1.12072 acc: 0.55000\n",
      "[Train]: Step: 92000, loss: 0.91398 acc: 0.60000\n",
      "[Test] Step: 92000, acc: 0.70390\n",
      "[Train]: Step: 92100, loss: 0.93129 acc: 0.70000\n",
      "[Train]: Step: 92200, loss: 0.75015 acc: 0.70000\n",
      "[Train]: Step: 92300, loss: 0.77243 acc: 0.70000\n",
      "[Train]: Step: 92400, loss: 0.58522 acc: 0.70000\n",
      "[Train]: Step: 92500, loss: 0.79733 acc: 0.70000\n",
      "[Train]: Step: 92600, loss: 0.36850 acc: 0.85000\n",
      "[Train]: Step: 92700, loss: 0.33112 acc: 0.95000\n",
      "[Train]: Step: 92800, loss: 0.53395 acc: 0.80000\n",
      "[Train]: Step: 92900, loss: 0.75066 acc: 0.80000\n",
      "[Train]: Step: 93000, loss: 0.44459 acc: 0.85000\n",
      "[Test] Step: 93000, acc: 0.70950\n",
      "[Train]: Step: 93100, loss: 0.80387 acc: 0.75000\n",
      "[Train]: Step: 93200, loss: 0.74575 acc: 0.75000\n",
      "[Train]: Step: 93300, loss: 0.67362 acc: 0.75000\n",
      "[Train]: Step: 93400, loss: 0.41726 acc: 0.90000\n",
      "[Train]: Step: 93500, loss: 0.38642 acc: 0.80000\n",
      "[Train]: Step: 93600, loss: 0.71733 acc: 0.65000\n",
      "[Train]: Step: 93700, loss: 0.87312 acc: 0.65000\n",
      "[Train]: Step: 93800, loss: 0.61519 acc: 0.80000\n",
      "[Train]: Step: 93900, loss: 0.86401 acc: 0.75000\n",
      "[Train]: Step: 94000, loss: 1.03702 acc: 0.70000\n",
      "[Test] Step: 94000, acc: 0.70800\n",
      "[Train]: Step: 94100, loss: 0.76371 acc: 0.75000\n",
      "[Train]: Step: 94200, loss: 0.99824 acc: 0.65000\n",
      "[Train]: Step: 94300, loss: 0.33117 acc: 0.85000\n",
      "[Train]: Step: 94400, loss: 0.88976 acc: 0.70000\n",
      "[Train]: Step: 94500, loss: 0.51837 acc: 0.80000\n",
      "[Train]: Step: 94600, loss: 0.63067 acc: 0.75000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train]: Step: 94700, loss: 0.68642 acc: 0.90000\n",
      "[Train]: Step: 94800, loss: 0.88035 acc: 0.65000\n",
      "[Train]: Step: 94900, loss: 0.50002 acc: 0.90000\n",
      "[Train]: Step: 95000, loss: 0.76578 acc: 0.70000\n",
      "[Test] Step: 95000, acc: 0.70250\n",
      "[Train]: Step: 95100, loss: 0.77127 acc: 0.75000\n",
      "[Train]: Step: 95200, loss: 0.73945 acc: 0.80000\n",
      "[Train]: Step: 95300, loss: 0.86051 acc: 0.55000\n",
      "[Train]: Step: 95400, loss: 0.81165 acc: 0.65000\n",
      "[Train]: Step: 95500, loss: 0.58646 acc: 0.70000\n",
      "[Train]: Step: 95600, loss: 0.68866 acc: 0.80000\n",
      "[Train]: Step: 95700, loss: 0.36892 acc: 0.90000\n",
      "[Train]: Step: 95800, loss: 0.67049 acc: 0.75000\n",
      "[Train]: Step: 95900, loss: 0.29269 acc: 0.90000\n",
      "[Train]: Step: 96000, loss: 0.29585 acc: 0.90000\n",
      "[Test] Step: 96000, acc: 0.70310\n",
      "[Train]: Step: 96100, loss: 0.64523 acc: 0.90000\n",
      "[Train]: Step: 96200, loss: 0.78802 acc: 0.65000\n",
      "[Train]: Step: 96300, loss: 0.48411 acc: 0.80000\n",
      "[Train]: Step: 96400, loss: 0.71207 acc: 0.60000\n",
      "[Train]: Step: 96500, loss: 0.73336 acc: 0.80000\n",
      "[Train]: Step: 96600, loss: 0.73327 acc: 0.75000\n",
      "[Train]: Step: 96700, loss: 0.25394 acc: 0.95000\n",
      "[Train]: Step: 96800, loss: 0.94363 acc: 0.70000\n",
      "[Train]: Step: 96900, loss: 0.38282 acc: 0.90000\n",
      "[Train]: Step: 97000, loss: 0.80758 acc: 0.75000\n",
      "[Test] Step: 97000, acc: 0.71990\n",
      "[Train]: Step: 97100, loss: 0.42554 acc: 0.80000\n",
      "[Train]: Step: 97200, loss: 0.97313 acc: 0.60000\n",
      "[Train]: Step: 97300, loss: 0.50211 acc: 0.85000\n",
      "[Train]: Step: 97400, loss: 0.38133 acc: 0.90000\n",
      "[Train]: Step: 97500, loss: 1.04781 acc: 0.60000\n",
      "[Train]: Step: 97600, loss: 1.09633 acc: 0.75000\n",
      "[Train]: Step: 97700, loss: 0.66548 acc: 0.75000\n",
      "[Train]: Step: 97800, loss: 0.64140 acc: 0.75000\n",
      "[Train]: Step: 97900, loss: 0.38996 acc: 0.90000\n",
      "[Train]: Step: 98000, loss: 0.37443 acc: 0.90000\n",
      "[Test] Step: 98000, acc: 0.71730\n",
      "[Train]: Step: 98100, loss: 0.98016 acc: 0.65000\n",
      "[Train]: Step: 98200, loss: 0.86454 acc: 0.70000\n",
      "[Train]: Step: 98300, loss: 1.02284 acc: 0.80000\n",
      "[Train]: Step: 98400, loss: 0.75961 acc: 0.75000\n",
      "[Train]: Step: 98500, loss: 0.83317 acc: 0.65000\n",
      "[Train]: Step: 98600, loss: 0.78221 acc: 0.75000\n",
      "[Train]: Step: 98700, loss: 0.35444 acc: 0.85000\n",
      "[Train]: Step: 98800, loss: 0.44125 acc: 0.90000\n",
      "[Train]: Step: 98900, loss: 0.42447 acc: 0.80000\n",
      "[Train]: Step: 99000, loss: 0.67393 acc: 0.75000\n",
      "[Test] Step: 99000, acc: 0.70600\n",
      "[Train]: Step: 99100, loss: 0.63793 acc: 0.75000\n",
      "[Train]: Step: 99200, loss: 0.64886 acc: 0.70000\n",
      "[Train]: Step: 99300, loss: 1.43574 acc: 0.55000\n",
      "[Train]: Step: 99400, loss: 0.89265 acc: 0.65000\n",
      "[Train]: Step: 99500, loss: 0.65132 acc: 0.75000\n",
      "[Train]: Step: 99600, loss: 0.89921 acc: 0.75000\n",
      "[Train]: Step: 99700, loss: 0.67618 acc: 0.80000\n",
      "[Train]: Step: 99800, loss: 0.26704 acc: 0.85000\n",
      "[Train]: Step: 99900, loss: 0.42696 acc: 0.90000\n",
      "[Train]: Step: 100000, loss: 0.39697 acc: 0.90000\n",
      "[Test] Step: 100000, acc: 0.71060\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "batch_size = 20\n",
    "train_steps = 100000\n",
    "test_steps = 2000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(train_steps):\n",
    "        batch_data, batch_labels = train_data.next_batch(batch_size)\n",
    "        loss, acc, _ = sess.run([_loss, accuracy, train_op], \n",
    "                                feed_dict={x: batch_data, y: batch_labels})\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print('[Train]: Step: %d, loss: %4.5f acc: %4.5f' \n",
    "                  % (i+1, loss, acc))\n",
    "        if (i+1) % 1000 == 0:\n",
    "            all_test_acc_val = []\n",
    "            for j in range(test_steps):\n",
    "                test_batch_data, test_batch_labels = test_data.next_batch(batch_size)\n",
    "                test_acc_val = sess.run([accuracy], \n",
    "                                        feed_dict={x: test_batch_data, y: test_batch_labels})\n",
    "                all_test_acc_val.append(test_acc_val)\n",
    "            test_acc = np.mean(all_test_acc_val)\n",
    "            print('[Test] Step: %d, acc: %4.5f' % (i+1, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

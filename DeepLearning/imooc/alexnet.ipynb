{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load CIFAR-10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR_DIR = '/home/commaai-03/Data/dataset/cifar-10-python'\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as f:\n",
    "        data = pickle.load(f, encoding='bytes')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [os.path.join(CIFAR_DIR, file)\n",
    "             for file in os.listdir(CIFAR_DIR)\n",
    "             if '.html' not in file]\n",
    "filenames.sort()\n",
    "meta_file = filenames[0]\n",
    "train_files = filenames[1:-1]\n",
    "test_file = [filenames[-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Data Handle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'batch_label'\n",
      "b'labels'\n",
      "b'data'\n",
      "b'filenames'\n"
     ]
    }
   ],
   "source": [
    "_test_data = unpickle(test_file[0])\n",
    "for k, v in _test_data.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_size = 32\n",
    "img_arr = _test_data[b'data'][99]\n",
    "# 32 * 32 * 3 (R,G,B)\n",
    "img_arr_reshaped = img_arr.reshape((3, img_size, img_size))\n",
    "img = img_arr_reshaped.transpose(1, 2, 0)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "%matplotlib inline\n",
    "imshow(img)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifarDate:\n",
    "    \n",
    "    def __init__(self, filenames, need_shuffle):\n",
    "        all_data = []\n",
    "        all_label = []\n",
    "        if not isinstance(filenames, (list, tuple)):\n",
    "            raise TypeError('%s are not iterable.' % filenames)\n",
    "        for filename in filenames:\n",
    "            data, labels = self.load_data(filename)\n",
    "            all_data.append(data)\n",
    "            all_label.append(labels)\n",
    "            #all_data.append(data)\n",
    "            #all_label.append(labels)\n",
    "        self._data = np.vstack(all_data) / 127.5 - 1\n",
    "        self._label = np.hstack(all_label)\n",
    "        print('[CIFAR-10]: Data shape-> %s' % str(self._data.shape))\n",
    "        print('[CIFAR-10]: Label shape-> %s' % str(self._label.shape))\n",
    "        \n",
    "        self.num_examples = self._data.shape[0]\n",
    "        self._need_shuffle = need_shuffle\n",
    "        self._indicator = 0\n",
    "        if self._need_shuffle:\n",
    "            self._shuffle_data()\n",
    "        \n",
    "\n",
    "    def load_data(self, filename):\n",
    "        import pickle\n",
    "        with open(filename, mode='rb') as f:\n",
    "            data = pickle.load(f, encoding='bytes')\n",
    "        return data[b'data'], data[b'labels']\n",
    "        \n",
    "    def _shuffle_data(self):\n",
    "        index = np.random.permutation(self.num_examples)\n",
    "        self._data = self._data[index]\n",
    "        self._label = self._label[index]\n",
    "        \n",
    "    def next_batch(self, batch_size):\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > self.num_examples:\n",
    "            rest_num_examples = self.num_examples - self._indicator\n",
    "            data_rest_part = self._data[self._indicator: self.num_examples]\n",
    "            label_rest_part = self._label[self._indicator: self.num_examples]\n",
    "            \n",
    "            if self._need_shuffle:\n",
    "                self._shuffle_data()\n",
    "            # For new loop, self._indicator + batch_size = self.num_examples\n",
    "            self._indicator = batch_size - rest_num_examples\n",
    "            end_indicator = self._indicator\n",
    "            data_new_part = self._data[:end_indicator]\n",
    "            label_new_part = self._label[:end_indicator]\n",
    "            batch_data = np.concatenate((data_rest_part, data_new_part), axis=0)\n",
    "            batch_label = np.concatenate((label_rest_part, label_new_part), axis=0)\n",
    "        else:\n",
    "            batch_data = self._data[self._indicator:end_indicator]\n",
    "            batch_label = self._label[self._indicator:end_indicator]\n",
    "            self._indicator = end_indicator\n",
    "        \n",
    "        return batch_data, batch_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(train_files, list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIFAR-10]: Data shape-> (50000, 3072)\n",
      "[CIFAR-10]: Label shape-> (50000,)\n",
      "[CIFAR-10]: Data shape-> (10000, 3072)\n",
      "[CIFAR-10]: Label shape-> (10000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "__main__.CifarDate"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 10\n",
    "batch_size = 128\n",
    "\n",
    "train_data = CifarDate(train_files, need_shuffle=True)\n",
    "test_data = CifarDate(test_file, need_shuffle=False)\n",
    "\n",
    "type(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AlexNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/device:GPU:0'):\n",
    "    input_data = tf.placeholder(tf.float32,\n",
    "                                shape=[None, img_size*img_size*3],\n",
    "                                name='input_data')\n",
    "\n",
    "    x = tf.reshape(input_data, [-1, 3, 32, 32])\n",
    "    # shape: [None, 32, 32, 3]\n",
    "    x = tf.transpose(x, perm=[0, 2, 3, 1])\n",
    "    ########### First convolution layer ###########\n",
    "    kernel = tf.Variable(tf.truncated_normal([11, 11, 3, 96],\n",
    "                                             dtype=tf.float32,\n",
    "                                             stddev=1e-1),\n",
    "                         name='conv1_kernel')\n",
    "    conv = tf.nn.conv2d(input=x,\n",
    "                        filter=kernel, \n",
    "                        strides=[1, 4, 4, 1], padding='SAME')\n",
    "    bias = tf.Variable(tf.truncated_normal([96]))\n",
    "    conv1 = tf.nn.relu(tf.nn.bias_add(conv, bias), name='conv1')\n",
    "\n",
    "    # Second Convolution Layer\n",
    "    kernel = tf.Variable(tf.truncated_normal([5, 5, 96, 256],\n",
    "                                             dtype=tf.float32,\n",
    "                                             stddev=1e-1), name='conv1_weights')\n",
    "    # Local Response Normalization\n",
    "    lrn1 = tf.nn.lrn(conv1, alpha=1e-4, beta=0.75,\n",
    "                     depth_radius=2, bias=2.0)\n",
    "    # Max Pooling\n",
    "    pooled_conv1 = tf.nn.max_pool(lrn1, \n",
    "                                  ksize=[1,3,3,1],\n",
    "                                  strides=[1,2,2,1],\n",
    "                                  padding='SAME', \n",
    "                                  name='pool1')\n",
    "    ########### Second convolution layer ###########\n",
    "    # shape: [width, height, depth, num_kernel]\n",
    "    kernel = tf.Variable(tf.truncated_normal([5,5,96,256], \n",
    "                                             dtype=tf.float32, \n",
    "                                             stddev=1e-1), name='conv2_weights')\n",
    "    conv = tf.nn.conv2d(input=pooled_conv1, \n",
    "                        filter=kernel, \n",
    "                        strides=[1, 4, 4, 1], padding='SAME')\n",
    "    bias = tf.Variable(tf.truncated_normal([256]), name='conv2_bias')\n",
    "    conv2 = tf.nn.relu(tf.nn.bias_add(conv, bias), name='conv2')\n",
    "    lrn2 = tf.nn.lrn(conv2, \n",
    "                     alpha=1e-4, \n",
    "                     beta=0.75, \n",
    "                     depth_radius=2, \n",
    "                     bias=2.)\n",
    "    pooled_conv2 = tf.nn.max_pool(lrn2, \n",
    "                                  ksize=[1, 3, 3, 1], \n",
    "                                  strides=[1, 2, 2, 1], \n",
    "                                  padding='SAME', \n",
    "                                  name='pool2')\n",
    "\n",
    "    ########### Third convolution layer ###########\n",
    "    # [3, 3, 256] * 384, strides=1\n",
    "    kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 384],\n",
    "                                             dtype=tf.float32,\n",
    "                                             stddev=1e-1),\n",
    "                         name='conv3_weights')\n",
    "    conv = tf.nn.conv2d(input=pooled_conv2, \n",
    "                        filter=kernel,\n",
    "                        strides=[1,1,1,1], padding='SAME')\n",
    "    bias = tf.Variable(tf.truncated_normal([384]), name='conv3_bias')\n",
    "    conv3 = tf.nn.relu(tf.nn.bias_add(conv, bias), name='conv3')\n",
    "\n",
    "    ########### Fourth convolution layer ###########\n",
    "    kernel = tf.Variable(tf.truncated_normal([3, 3, 384, 384], \n",
    "                                             dtype=tf.float32, \n",
    "                                             stddev=1e-3), \n",
    "                         name='conv4_weights')\n",
    "    conv =  tf.nn.conv2d(input=conv3, \n",
    "                         filter=kernel,\n",
    "                         strides=[1,1,1,1], \n",
    "                         padding='SAME')\n",
    "    bias = tf.Variable(tf.truncated_normal([384]), name='conv4_bias')\n",
    "    conv4 = tf.nn.relu(tf.nn.bias_add(conv, bias), name='conv4')\n",
    "\n",
    "    ########### Firth convolution layer ###########\n",
    "    kernel = tf.Variable(tf.truncated_normal([3,3,384,256],\n",
    "                                             dtype=tf.float32,\n",
    "                                             stddev=1e-1), \n",
    "                         name='conv5_weights')\n",
    "    conv = tf.nn.conv2d(input=conv4, \n",
    "                        filter=kernel, \n",
    "                        strides=[1,1,1,1], \n",
    "                        padding='SAME')\n",
    "    bias = tf.Variable(tf.truncated_normal([256]), name='conv5_bias')\n",
    "    conv5 = tf.nn.relu(tf.nn.bias_add(conv, bias), name='conv5')\n",
    "\n",
    "    ########### Fully Connected Layer ###########\n",
    "    fc_size = 256\n",
    "    conv5 = tf.keras.layers.Flatten()(conv5)\n",
    "    weights = tf.Variable(tf.truncated_normal(shape=[fc_size, fc_size]), \n",
    "                          name='fc1_weights')\n",
    "    bias = tf.Variable(tf.truncated_normal([fc_size]),\n",
    "                       name='fc1_bias')\n",
    "    fc1 = tf.matmul(conv5, weights) + bias\n",
    "    fc1 = tf.nn.relu(fc1, name='fc1')\n",
    "\n",
    "    ########### Fully Connected Layer ###########\n",
    "    weights = tf.Variable(tf.truncated_normal([fc_size, fc_size]), \n",
    "                          name='fc2_weights')\n",
    "    bias = tf.Variable(tf.truncated_normal([fc_size]), \n",
    "                       name='fc2_bias')\n",
    "    fc2 = tf.matmul(fc1, weights) + bias\n",
    "    fc2 = tf.nn.relu(fc2, name='fc2')\n",
    "\n",
    "    ########### Output Layer ###########\n",
    "    weights = tf.Variable(tf.zeros([fc_size, num_classes]), \n",
    "                          name='output_weight')\n",
    "    bias = tf.Variable(tf.truncated_normal([num_classes]), \n",
    "                       name='output_bias')\n",
    "    out = tf.matmul(fc2, weights) + bias\n",
    "\n",
    "    y = tf.placeholder(tf.int32, [None])\n",
    "    y_ = tf.one_hot(y, num_classes)\n",
    "    _loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(logits=out,\n",
    "                                                   labels=y_))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(_loss)\n",
    "    correct_pred = tf.equal(tf.argmax(out, axis=1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    #tf.summary.histogram('cost', _loss)\n",
    "    #tf.summary.histogram('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Session Run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Initialized!\n",
      "[Train]: Step: 100, loss: 2.07042, acc: 0.28125\n",
      "[Train]: Step: 200, loss: 1.73298, acc: 0.34375\n",
      "[Train]: Step: 300, loss: 1.65922, acc: 0.40625\n",
      "[Train]: Step: 400, loss: 1.57112, acc: 0.40625\n",
      "[Train]: Step: 500, loss: 1.66807, acc: 0.50000\n",
      "[Train]: Step: 600, loss: 1.59801, acc: 0.46875\n",
      "[Train]: Step: 700, loss: 1.61140, acc: 0.31250\n",
      "[Train]: Step: 800, loss: 1.55696, acc: 0.46875\n",
      "[Train]: Step: 900, loss: 1.56637, acc: 0.34375\n",
      "[Train]: Step: 1000, loss: 1.41659, acc: 0.40625\n",
      "[Train]: Step: 1100, loss: 1.45281, acc: 0.53125\n",
      "[Train]: Step: 1200, loss: 1.48491, acc: 0.40625\n",
      "[Train]: Step: 1300, loss: 1.45973, acc: 0.56250\n",
      "[Train]: Step: 1400, loss: 1.39579, acc: 0.46875\n",
      "[Train]: Step: 1500, loss: 1.43858, acc: 0.50000\n",
      "[Train]: Step: 1600, loss: 1.65826, acc: 0.50000\n",
      "[Train]: Step: 1700, loss: 1.37493, acc: 0.43750\n",
      "[Train]: Step: 1800, loss: 1.61053, acc: 0.46875\n",
      "[Train]: Step: 1900, loss: 1.17247, acc: 0.56250\n",
      "[Train]: Step: 2000, loss: 1.46286, acc: 0.50000\n",
      "[Train]: Step: 2100, loss: 1.35348, acc: 0.59375\n",
      "[Train]: Step: 2200, loss: 1.23951, acc: 0.62500\n",
      "[Train]: Step: 2300, loss: 1.25745, acc: 0.53125\n",
      "[Train]: Step: 2400, loss: 1.37243, acc: 0.62500\n",
      "[Train]: Step: 2500, loss: 1.57448, acc: 0.43750\n",
      "[Train]: Step: 2600, loss: 1.30608, acc: 0.56250\n",
      "[Train]: Step: 2700, loss: 1.02391, acc: 0.68750\n",
      "[Train]: Step: 2800, loss: 1.38685, acc: 0.46875\n",
      "[Train]: Step: 2900, loss: 1.17779, acc: 0.53125\n",
      "[Train]: Step: 3000, loss: 1.34469, acc: 0.50000\n",
      "[Train]: Step: 3100, loss: 1.43403, acc: 0.53125\n",
      "[Train]: Step: 3200, loss: 0.98572, acc: 0.71875\n",
      "[Train]: Step: 3300, loss: 1.44899, acc: 0.56250\n",
      "[Train]: Step: 3400, loss: 1.27413, acc: 0.59375\n",
      "[Train]: Step: 3500, loss: 1.51773, acc: 0.50000\n",
      "[Train]: Step: 3600, loss: 1.56821, acc: 0.46875\n",
      "[Train]: Step: 3700, loss: 1.50522, acc: 0.43750\n",
      "[Train]: Step: 3800, loss: 1.22696, acc: 0.53125\n",
      "[Train]: Step: 3900, loss: 1.36371, acc: 0.46875\n",
      "[Train]: Step: 4000, loss: 0.95352, acc: 0.62500\n",
      "[Train]: Step: 4100, loss: 1.12277, acc: 0.59375\n",
      "[Train]: Step: 4200, loss: 1.54626, acc: 0.40625\n",
      "[Train]: Step: 4300, loss: 1.21552, acc: 0.56250\n",
      "[Train]: Step: 4400, loss: 1.29642, acc: 0.50000\n",
      "[Train]: Step: 4500, loss: 1.24053, acc: 0.59375\n",
      "[Train]: Step: 4600, loss: 1.40112, acc: 0.50000\n",
      "[Train]: Step: 4700, loss: 1.03090, acc: 0.59375\n",
      "[Train]: Step: 4800, loss: 1.56274, acc: 0.50000\n",
      "[Train]: Step: 4900, loss: 1.56170, acc: 0.59375\n",
      "[Train]: Step: 5000, loss: 1.10129, acc: 0.75000\n",
      "[Test]: Step: 5000, acc: 0.65625\n",
      "[Train]: Step: 5100, loss: 1.16061, acc: 0.53125\n",
      "[Train]: Step: 5200, loss: 1.27677, acc: 0.53125\n",
      "[Train]: Step: 5300, loss: 1.36291, acc: 0.59375\n",
      "[Train]: Step: 5400, loss: 1.62718, acc: 0.37500\n",
      "[Train]: Step: 5500, loss: 0.86949, acc: 0.68750\n",
      "[Train]: Step: 5600, loss: 1.21672, acc: 0.53125\n",
      "[Train]: Step: 5700, loss: 0.94192, acc: 0.71875\n",
      "[Train]: Step: 5800, loss: 1.36732, acc: 0.46875\n",
      "[Train]: Step: 5900, loss: 1.42167, acc: 0.62500\n",
      "[Train]: Step: 6000, loss: 1.05703, acc: 0.62500\n",
      "[Train]: Step: 6100, loss: 1.36586, acc: 0.59375\n",
      "[Train]: Step: 6200, loss: 0.98488, acc: 0.59375\n",
      "[Train]: Step: 6300, loss: 1.04354, acc: 0.62500\n",
      "[Train]: Step: 6400, loss: 1.06806, acc: 0.71875\n",
      "[Train]: Step: 6500, loss: 1.83392, acc: 0.37500\n",
      "[Train]: Step: 6600, loss: 0.77541, acc: 0.75000\n",
      "[Train]: Step: 6700, loss: 1.27768, acc: 0.56250\n",
      "[Train]: Step: 6800, loss: 1.12402, acc: 0.56250\n",
      "[Train]: Step: 6900, loss: 0.99424, acc: 0.59375\n",
      "[Train]: Step: 7000, loss: 0.93924, acc: 0.65625\n",
      "[Train]: Step: 7100, loss: 1.06142, acc: 0.68750\n",
      "[Train]: Step: 7200, loss: 1.30434, acc: 0.50000\n",
      "[Train]: Step: 7300, loss: 1.70954, acc: 0.40625\n",
      "[Train]: Step: 7400, loss: 1.28245, acc: 0.56250\n",
      "[Train]: Step: 7500, loss: 1.06762, acc: 0.59375\n",
      "[Train]: Step: 7600, loss: 0.87805, acc: 0.62500\n",
      "[Train]: Step: 7700, loss: 0.73901, acc: 0.78125\n",
      "[Train]: Step: 7800, loss: 1.07906, acc: 0.68750\n",
      "[Train]: Step: 7900, loss: 1.02889, acc: 0.75000\n",
      "[Train]: Step: 8000, loss: 0.90322, acc: 0.65625\n",
      "[Train]: Step: 8100, loss: 1.12851, acc: 0.65625\n",
      "[Train]: Step: 8200, loss: 1.18071, acc: 0.56250\n",
      "[Train]: Step: 8300, loss: 1.20826, acc: 0.59375\n",
      "[Train]: Step: 8400, loss: 0.84084, acc: 0.75000\n",
      "[Train]: Step: 8500, loss: 0.96462, acc: 0.71875\n",
      "[Train]: Step: 8600, loss: 0.79917, acc: 0.75000\n",
      "[Train]: Step: 8700, loss: 0.98460, acc: 0.68750\n",
      "[Train]: Step: 8800, loss: 0.96023, acc: 0.71875\n",
      "[Train]: Step: 8900, loss: 1.12860, acc: 0.68750\n",
      "[Train]: Step: 9000, loss: 0.98313, acc: 0.71875\n",
      "[Train]: Step: 9100, loss: 1.27369, acc: 0.56250\n",
      "[Train]: Step: 9200, loss: 1.75961, acc: 0.56250\n",
      "[Train]: Step: 9300, loss: 1.29696, acc: 0.56250\n",
      "[Train]: Step: 9400, loss: 1.05955, acc: 0.59375\n",
      "[Train]: Step: 9500, loss: 0.63836, acc: 0.78125\n",
      "[Train]: Step: 9600, loss: 0.88902, acc: 0.62500\n",
      "[Train]: Step: 9700, loss: 1.16130, acc: 0.59375\n",
      "[Train]: Step: 9800, loss: 0.91305, acc: 0.59375\n",
      "[Train]: Step: 9900, loss: 1.34103, acc: 0.53125\n",
      "[Train]: Step: 10000, loss: 1.30803, acc: 0.56250\n",
      "[Test]: Step: 10000, acc: 0.50000\n",
      "[Train]: Step: 10100, loss: 0.84839, acc: 0.65625\n",
      "[Train]: Step: 10200, loss: 1.27978, acc: 0.62500\n",
      "[Train]: Step: 10300, loss: 1.14092, acc: 0.65625\n",
      "[Train]: Step: 10400, loss: 0.83838, acc: 0.65625\n",
      "[Train]: Step: 10500, loss: 0.78582, acc: 0.75000\n",
      "[Train]: Step: 10600, loss: 0.82167, acc: 0.68750\n",
      "[Train]: Step: 10700, loss: 1.12088, acc: 0.56250\n",
      "[Train]: Step: 10800, loss: 0.72449, acc: 0.75000\n",
      "[Train]: Step: 10900, loss: 0.99823, acc: 0.65625\n",
      "[Train]: Step: 11000, loss: 0.84904, acc: 0.68750\n",
      "[Train]: Step: 11100, loss: 0.99080, acc: 0.62500\n",
      "[Train]: Step: 11200, loss: 1.01759, acc: 0.62500\n",
      "[Train]: Step: 11300, loss: 0.84912, acc: 0.81250\n",
      "[Train]: Step: 11400, loss: 0.84231, acc: 0.78125\n",
      "[Train]: Step: 11500, loss: 1.35973, acc: 0.71875\n",
      "[Train]: Step: 11600, loss: 1.01564, acc: 0.59375\n",
      "[Train]: Step: 11700, loss: 0.82254, acc: 0.68750\n",
      "[Train]: Step: 11800, loss: 1.15805, acc: 0.65625\n",
      "[Train]: Step: 11900, loss: 1.02262, acc: 0.62500\n",
      "[Train]: Step: 12000, loss: 0.86257, acc: 0.75000\n",
      "[Train]: Step: 12100, loss: 0.70637, acc: 0.71875\n",
      "[Train]: Step: 12200, loss: 1.11475, acc: 0.71875\n",
      "[Train]: Step: 12300, loss: 0.91217, acc: 0.62500\n",
      "[Train]: Step: 12400, loss: 0.84928, acc: 0.71875\n",
      "[Train]: Step: 12500, loss: 0.97038, acc: 0.62500\n",
      "[Train]: Step: 12600, loss: 0.87308, acc: 0.65625\n",
      "[Train]: Step: 12700, loss: 0.80153, acc: 0.81250\n",
      "[Train]: Step: 12800, loss: 0.60410, acc: 0.81250\n",
      "[Train]: Step: 12900, loss: 0.78935, acc: 0.75000\n",
      "[Train]: Step: 13000, loss: 0.65492, acc: 0.78125\n",
      "[Train]: Step: 13100, loss: 1.13011, acc: 0.71875\n",
      "[Train]: Step: 13200, loss: 0.82379, acc: 0.56250\n",
      "[Train]: Step: 13300, loss: 0.96699, acc: 0.71875\n",
      "[Train]: Step: 13400, loss: 0.85598, acc: 0.62500\n",
      "[Train]: Step: 13500, loss: 0.63719, acc: 0.84375\n",
      "[Train]: Step: 13600, loss: 0.86430, acc: 0.65625\n",
      "[Train]: Step: 13700, loss: 0.83031, acc: 0.75000\n",
      "[Train]: Step: 13800, loss: 0.98416, acc: 0.68750\n",
      "[Train]: Step: 13900, loss: 1.09817, acc: 0.65625\n",
      "[Train]: Step: 14000, loss: 0.85555, acc: 0.65625\n",
      "[Train]: Step: 14100, loss: 0.84810, acc: 0.71875\n",
      "[Train]: Step: 14200, loss: 0.85682, acc: 0.71875\n",
      "[Train]: Step: 14300, loss: 0.79742, acc: 0.71875\n",
      "[Train]: Step: 14400, loss: 0.83305, acc: 0.75000\n",
      "[Train]: Step: 14500, loss: 1.50017, acc: 0.56250\n",
      "[Train]: Step: 14600, loss: 0.63120, acc: 0.81250\n",
      "[Train]: Step: 14700, loss: 0.85591, acc: 0.75000\n",
      "[Train]: Step: 14800, loss: 0.79494, acc: 0.68750\n",
      "[Train]: Step: 14900, loss: 0.69164, acc: 0.78125\n",
      "[Train]: Step: 15000, loss: 0.99942, acc: 0.62500\n",
      "[Test]: Step: 15000, acc: 0.71875\n",
      "[Train]: Step: 15100, loss: 0.97234, acc: 0.59375\n",
      "[Train]: Step: 15200, loss: 0.68975, acc: 0.81250\n",
      "[Train]: Step: 15300, loss: 0.69901, acc: 0.71875\n",
      "[Train]: Step: 15400, loss: 1.41167, acc: 0.56250\n",
      "[Train]: Step: 15500, loss: 0.69817, acc: 0.75000\n",
      "[Train]: Step: 15600, loss: 0.62695, acc: 0.84375\n",
      "[Train]: Step: 15700, loss: 1.15720, acc: 0.65625\n",
      "[Train]: Step: 15800, loss: 0.98390, acc: 0.62500\n",
      "[Train]: Step: 15900, loss: 0.82346, acc: 0.78125\n",
      "[Train]: Step: 16000, loss: 1.23049, acc: 0.50000\n",
      "[Train]: Step: 16100, loss: 0.98686, acc: 0.78125\n",
      "[Train]: Step: 16200, loss: 1.06882, acc: 0.62500\n",
      "[Train]: Step: 16300, loss: 0.60815, acc: 0.81250\n",
      "[Train]: Step: 16400, loss: 0.70140, acc: 0.68750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train]: Step: 16500, loss: 1.29314, acc: 0.68750\n",
      "[Train]: Step: 16600, loss: 1.15783, acc: 0.65625\n",
      "[Train]: Step: 16700, loss: 0.81483, acc: 0.71875\n",
      "[Train]: Step: 16800, loss: 1.12818, acc: 0.68750\n",
      "[Train]: Step: 16900, loss: 0.56724, acc: 0.75000\n",
      "[Train]: Step: 17000, loss: 0.79776, acc: 0.78125\n",
      "[Train]: Step: 17100, loss: 0.96293, acc: 0.75000\n",
      "[Train]: Step: 17200, loss: 0.86524, acc: 0.65625\n",
      "[Train]: Step: 17300, loss: 0.71557, acc: 0.78125\n",
      "[Train]: Step: 17400, loss: 0.46583, acc: 0.81250\n",
      "[Train]: Step: 17500, loss: 0.55535, acc: 0.75000\n",
      "[Train]: Step: 17600, loss: 1.04678, acc: 0.65625\n",
      "[Train]: Step: 17700, loss: 1.38371, acc: 0.68750\n",
      "[Train]: Step: 17800, loss: 0.68764, acc: 0.81250\n",
      "[Train]: Step: 17900, loss: 0.82051, acc: 0.71875\n",
      "[Train]: Step: 18000, loss: 0.62007, acc: 0.78125\n",
      "[Train]: Step: 18100, loss: 0.74629, acc: 0.71875\n",
      "[Train]: Step: 18200, loss: 0.94513, acc: 0.62500\n",
      "[Train]: Step: 18300, loss: 0.59779, acc: 0.87500\n",
      "[Train]: Step: 18400, loss: 0.77966, acc: 0.71875\n",
      "[Train]: Step: 18500, loss: 0.69006, acc: 0.78125\n",
      "[Train]: Step: 18600, loss: 1.30783, acc: 0.50000\n",
      "[Train]: Step: 18700, loss: 0.63784, acc: 0.71875\n",
      "[Train]: Step: 18800, loss: 0.57839, acc: 0.78125\n",
      "[Train]: Step: 18900, loss: 1.01266, acc: 0.75000\n",
      "[Train]: Step: 19000, loss: 0.99306, acc: 0.62500\n",
      "[Train]: Step: 19100, loss: 0.97111, acc: 0.71875\n",
      "[Train]: Step: 19200, loss: 0.68060, acc: 0.68750\n",
      "[Train]: Step: 19300, loss: 0.64446, acc: 0.71875\n",
      "[Train]: Step: 19400, loss: 0.60686, acc: 0.87500\n",
      "[Train]: Step: 19500, loss: 0.84068, acc: 0.75000\n",
      "[Train]: Step: 19600, loss: 0.71105, acc: 0.71875\n",
      "[Train]: Step: 19700, loss: 1.08640, acc: 0.62500\n",
      "[Train]: Step: 19800, loss: 0.68915, acc: 0.75000\n",
      "[Train]: Step: 19900, loss: 0.53406, acc: 0.87500\n",
      "[Train]: Step: 20000, loss: 1.02802, acc: 0.65625\n",
      "[Test]: Step: 20000, acc: 0.53125\n",
      "[Train]: Step: 20100, loss: 0.92121, acc: 0.81250\n",
      "[Train]: Step: 20200, loss: 0.81971, acc: 0.68750\n",
      "[Train]: Step: 20300, loss: 0.88414, acc: 0.68750\n",
      "[Train]: Step: 20400, loss: 0.57311, acc: 0.84375\n",
      "[Train]: Step: 20500, loss: 0.62501, acc: 0.81250\n",
      "[Train]: Step: 20600, loss: 0.84133, acc: 0.71875\n",
      "[Train]: Step: 20700, loss: 0.88243, acc: 0.62500\n",
      "[Train]: Step: 20800, loss: 0.78200, acc: 0.68750\n",
      "[Train]: Step: 20900, loss: 0.83346, acc: 0.78125\n",
      "[Train]: Step: 21000, loss: 0.82327, acc: 0.68750\n",
      "[Train]: Step: 21100, loss: 0.67889, acc: 0.75000\n",
      "[Train]: Step: 21200, loss: 0.45141, acc: 0.90625\n",
      "[Train]: Step: 21300, loss: 0.94167, acc: 0.68750\n",
      "[Train]: Step: 21400, loss: 0.83897, acc: 0.75000\n",
      "[Train]: Step: 21500, loss: 0.68691, acc: 0.65625\n",
      "[Train]: Step: 21600, loss: 0.50268, acc: 0.78125\n",
      "[Train]: Step: 21700, loss: 0.62416, acc: 0.81250\n",
      "[Train]: Step: 21800, loss: 0.78698, acc: 0.68750\n",
      "[Train]: Step: 21900, loss: 0.81421, acc: 0.75000\n",
      "[Train]: Step: 22000, loss: 0.60262, acc: 0.75000\n",
      "[Train]: Step: 22100, loss: 0.76813, acc: 0.81250\n",
      "[Train]: Step: 22200, loss: 0.65767, acc: 0.75000\n",
      "[Train]: Step: 22300, loss: 0.68494, acc: 0.78125\n",
      "[Train]: Step: 22400, loss: 0.57248, acc: 0.71875\n",
      "[Train]: Step: 22500, loss: 1.06020, acc: 0.71875\n",
      "[Train]: Step: 22600, loss: 0.56665, acc: 0.81250\n",
      "[Train]: Step: 22700, loss: 0.71771, acc: 0.78125\n",
      "[Train]: Step: 22800, loss: 0.77707, acc: 0.68750\n",
      "[Train]: Step: 22900, loss: 0.86456, acc: 0.68750\n",
      "[Train]: Step: 23000, loss: 0.61521, acc: 0.81250\n",
      "[Train]: Step: 23100, loss: 0.53037, acc: 0.81250\n",
      "[Train]: Step: 23200, loss: 0.84147, acc: 0.68750\n",
      "[Train]: Step: 23300, loss: 0.68793, acc: 0.78125\n",
      "[Train]: Step: 23400, loss: 0.53484, acc: 0.75000\n",
      "[Train]: Step: 23500, loss: 0.50455, acc: 0.84375\n",
      "[Train]: Step: 23600, loss: 0.28188, acc: 0.90625\n",
      "[Train]: Step: 23700, loss: 0.82938, acc: 0.78125\n",
      "[Train]: Step: 23800, loss: 1.00326, acc: 0.68750\n",
      "[Train]: Step: 23900, loss: 0.75531, acc: 0.71875\n",
      "[Train]: Step: 24000, loss: 0.85288, acc: 0.71875\n",
      "[Train]: Step: 24100, loss: 0.71206, acc: 0.78125\n",
      "[Train]: Step: 24200, loss: 0.72912, acc: 0.78125\n",
      "[Train]: Step: 24300, loss: 0.47864, acc: 0.87500\n",
      "[Train]: Step: 24400, loss: 1.15485, acc: 0.59375\n",
      "[Train]: Step: 24500, loss: 1.06661, acc: 0.68750\n",
      "[Train]: Step: 24600, loss: 1.06578, acc: 0.68750\n",
      "[Train]: Step: 24700, loss: 0.72038, acc: 0.78125\n",
      "[Train]: Step: 24800, loss: 0.96695, acc: 0.65625\n",
      "[Train]: Step: 24900, loss: 0.92946, acc: 0.71875\n",
      "[Train]: Step: 25000, loss: 0.50228, acc: 0.84375\n",
      "[Test]: Step: 25000, acc: 0.53125\n",
      "[Train]: Step: 25100, loss: 0.70812, acc: 0.81250\n",
      "[Train]: Step: 25200, loss: 0.65185, acc: 0.71875\n",
      "[Train]: Step: 25300, loss: 1.06162, acc: 0.59375\n",
      "[Train]: Step: 25400, loss: 0.26290, acc: 0.93750\n",
      "[Train]: Step: 25500, loss: 0.85390, acc: 0.59375\n",
      "[Train]: Step: 25600, loss: 0.58859, acc: 0.81250\n",
      "[Train]: Step: 25700, loss: 0.66193, acc: 0.71875\n",
      "[Train]: Step: 25800, loss: 0.64910, acc: 0.75000\n",
      "[Train]: Step: 25900, loss: 0.64190, acc: 0.78125\n",
      "[Train]: Step: 26000, loss: 0.82960, acc: 0.75000\n",
      "[Train]: Step: 26100, loss: 0.70453, acc: 0.68750\n",
      "[Train]: Step: 26200, loss: 0.44783, acc: 0.87500\n",
      "[Train]: Step: 26300, loss: 0.63157, acc: 0.81250\n",
      "[Train]: Step: 26400, loss: 0.70311, acc: 0.68750\n",
      "[Train]: Step: 26500, loss: 0.54725, acc: 0.84375\n",
      "[Train]: Step: 26600, loss: 0.69176, acc: 0.71875\n",
      "[Train]: Step: 26700, loss: 0.99759, acc: 0.56250\n",
      "[Train]: Step: 26800, loss: 0.85687, acc: 0.68750\n",
      "[Train]: Step: 26900, loss: 0.68750, acc: 0.78125\n",
      "[Train]: Step: 27000, loss: 0.20483, acc: 0.93750\n",
      "[Train]: Step: 27100, loss: 0.75117, acc: 0.68750\n",
      "[Train]: Step: 27200, loss: 0.50946, acc: 0.84375\n",
      "[Train]: Step: 27300, loss: 0.96405, acc: 0.75000\n",
      "[Train]: Step: 27400, loss: 0.49778, acc: 0.81250\n",
      "[Train]: Step: 27500, loss: 0.80602, acc: 0.81250\n",
      "[Train]: Step: 27600, loss: 0.26082, acc: 0.90625\n",
      "[Train]: Step: 27700, loss: 1.01361, acc: 0.71875\n",
      "[Train]: Step: 27800, loss: 0.61500, acc: 0.81250\n",
      "[Train]: Step: 27900, loss: 0.78568, acc: 0.78125\n",
      "[Train]: Step: 28000, loss: 0.97097, acc: 0.68750\n",
      "[Train]: Step: 28100, loss: 1.08466, acc: 0.71875\n",
      "[Train]: Step: 28200, loss: 0.66602, acc: 0.71875\n",
      "[Train]: Step: 28300, loss: 0.35094, acc: 0.84375\n",
      "[Train]: Step: 28400, loss: 0.53835, acc: 0.87500\n",
      "[Train]: Step: 28500, loss: 0.55519, acc: 0.75000\n",
      "[Train]: Step: 28600, loss: 0.58840, acc: 0.84375\n",
      "[Train]: Step: 28700, loss: 0.55386, acc: 0.81250\n",
      "[Train]: Step: 28800, loss: 0.60591, acc: 0.78125\n",
      "[Train]: Step: 28900, loss: 0.72297, acc: 0.71875\n",
      "[Train]: Step: 29000, loss: 0.67391, acc: 0.71875\n",
      "[Train]: Step: 29100, loss: 0.65780, acc: 0.78125\n",
      "[Train]: Step: 29200, loss: 0.47276, acc: 0.90625\n",
      "[Train]: Step: 29300, loss: 0.80139, acc: 0.75000\n",
      "[Train]: Step: 29400, loss: 0.51582, acc: 0.78125\n",
      "[Train]: Step: 29500, loss: 0.52126, acc: 0.84375\n",
      "[Train]: Step: 29600, loss: 0.70528, acc: 0.71875\n",
      "[Train]: Step: 29700, loss: 0.54067, acc: 0.81250\n",
      "[Train]: Step: 29800, loss: 0.88505, acc: 0.68750\n",
      "[Train]: Step: 29900, loss: 0.78940, acc: 0.71875\n",
      "[Train]: Step: 30000, loss: 0.59245, acc: 0.81250\n",
      "[Test]: Step: 30000, acc: 0.50000\n",
      "[Train]: Step: 30100, loss: 0.57257, acc: 0.78125\n",
      "[Train]: Step: 30200, loss: 0.54416, acc: 0.90625\n",
      "[Train]: Step: 30300, loss: 0.69520, acc: 0.81250\n",
      "[Train]: Step: 30400, loss: 0.51824, acc: 0.84375\n",
      "[Train]: Step: 30500, loss: 1.01967, acc: 0.75000\n",
      "[Train]: Step: 30600, loss: 0.98490, acc: 0.68750\n",
      "[Train]: Step: 30700, loss: 0.48785, acc: 0.81250\n",
      "[Train]: Step: 30800, loss: 0.67894, acc: 0.71875\n",
      "[Train]: Step: 30900, loss: 0.63298, acc: 0.78125\n",
      "[Train]: Step: 31000, loss: 0.80655, acc: 0.68750\n",
      "[Train]: Step: 31100, loss: 0.57640, acc: 0.78125\n",
      "[Train]: Step: 31200, loss: 0.48464, acc: 0.78125\n",
      "[Train]: Step: 31300, loss: 0.53271, acc: 0.84375\n",
      "[Train]: Step: 31400, loss: 0.86276, acc: 0.78125\n",
      "[Train]: Step: 31500, loss: 1.02015, acc: 0.68750\n",
      "[Train]: Step: 31600, loss: 0.66939, acc: 0.81250\n",
      "[Train]: Step: 31700, loss: 0.70328, acc: 0.75000\n",
      "[Train]: Step: 31800, loss: 0.57695, acc: 0.84375\n",
      "[Train]: Step: 31900, loss: 0.53185, acc: 0.81250\n",
      "[Train]: Step: 32000, loss: 0.46737, acc: 0.90625\n",
      "[Train]: Step: 32100, loss: 0.44440, acc: 0.84375\n",
      "[Train]: Step: 32200, loss: 0.66703, acc: 0.75000\n",
      "[Train]: Step: 32300, loss: 0.94369, acc: 0.65625\n",
      "[Train]: Step: 32400, loss: 0.64204, acc: 0.78125\n",
      "[Train]: Step: 32500, loss: 0.82515, acc: 0.75000\n",
      "[Train]: Step: 32600, loss: 1.08373, acc: 0.71875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train]: Step: 32700, loss: 0.40349, acc: 0.84375\n",
      "[Train]: Step: 32800, loss: 0.34219, acc: 0.87500\n",
      "[Train]: Step: 32900, loss: 0.50396, acc: 0.84375\n",
      "[Train]: Step: 33000, loss: 0.25953, acc: 0.93750\n",
      "[Train]: Step: 33100, loss: 0.71534, acc: 0.84375\n",
      "[Train]: Step: 33200, loss: 0.64201, acc: 0.81250\n",
      "[Train]: Step: 33300, loss: 0.85494, acc: 0.75000\n",
      "[Train]: Step: 33400, loss: 0.72005, acc: 0.78125\n",
      "[Train]: Step: 33500, loss: 0.74293, acc: 0.75000\n",
      "[Train]: Step: 33600, loss: 0.61200, acc: 0.81250\n",
      "[Train]: Step: 33700, loss: 0.61776, acc: 0.78125\n",
      "[Train]: Step: 33800, loss: 0.66543, acc: 0.75000\n",
      "[Train]: Step: 33900, loss: 0.76453, acc: 0.65625\n",
      "[Train]: Step: 34000, loss: 0.75840, acc: 0.71875\n",
      "[Train]: Step: 34100, loss: 0.34086, acc: 0.90625\n",
      "[Train]: Step: 34200, loss: 0.30243, acc: 0.87500\n",
      "[Train]: Step: 34300, loss: 0.64321, acc: 0.78125\n",
      "[Train]: Step: 34400, loss: 0.62564, acc: 0.75000\n",
      "[Train]: Step: 34500, loss: 0.59507, acc: 0.78125\n",
      "[Train]: Step: 34600, loss: 0.36636, acc: 0.90625\n",
      "[Train]: Step: 34700, loss: 0.59533, acc: 0.81250\n",
      "[Train]: Step: 34800, loss: 0.48905, acc: 0.87500\n",
      "[Train]: Step: 34900, loss: 0.33235, acc: 0.84375\n",
      "[Train]: Step: 35000, loss: 0.53252, acc: 0.81250\n",
      "[Test]: Step: 35000, acc: 0.56250\n",
      "[Train]: Step: 35100, loss: 0.79436, acc: 0.68750\n",
      "[Train]: Step: 35200, loss: 0.27681, acc: 0.93750\n",
      "[Train]: Step: 35300, loss: 0.63277, acc: 0.75000\n",
      "[Train]: Step: 35400, loss: 0.90768, acc: 0.71875\n",
      "[Train]: Step: 35500, loss: 0.61744, acc: 0.75000\n",
      "[Train]: Step: 35600, loss: 0.75216, acc: 0.81250\n",
      "[Train]: Step: 35700, loss: 0.55408, acc: 0.71875\n",
      "[Train]: Step: 35800, loss: 0.33952, acc: 0.90625\n",
      "[Train]: Step: 35900, loss: 0.61885, acc: 0.78125\n",
      "[Train]: Step: 36000, loss: 0.72610, acc: 0.71875\n",
      "[Train]: Step: 36100, loss: 0.45061, acc: 0.81250\n",
      "[Train]: Step: 36200, loss: 0.48745, acc: 0.84375\n",
      "[Train]: Step: 36300, loss: 0.85061, acc: 0.71875\n",
      "[Train]: Step: 36400, loss: 0.60150, acc: 0.84375\n",
      "[Train]: Step: 36500, loss: 0.84087, acc: 0.84375\n",
      "[Train]: Step: 36600, loss: 0.57849, acc: 0.81250\n",
      "[Train]: Step: 36700, loss: 0.76027, acc: 0.75000\n",
      "[Train]: Step: 36800, loss: 0.65168, acc: 0.71875\n",
      "[Train]: Step: 36900, loss: 0.90363, acc: 0.62500\n",
      "[Train]: Step: 37000, loss: 1.15869, acc: 0.65625\n",
      "[Train]: Step: 37100, loss: 0.72332, acc: 0.71875\n",
      "[Train]: Step: 37200, loss: 0.45727, acc: 0.87500\n",
      "[Train]: Step: 37300, loss: 0.34029, acc: 0.84375\n",
      "[Train]: Step: 37400, loss: 0.57632, acc: 0.75000\n",
      "[Train]: Step: 37500, loss: 0.98969, acc: 0.59375\n",
      "[Train]: Step: 37600, loss: 0.58572, acc: 0.75000\n",
      "[Train]: Step: 37700, loss: 0.35551, acc: 0.90625\n",
      "[Train]: Step: 37800, loss: 0.24550, acc: 0.93750\n",
      "[Train]: Step: 37900, loss: 0.35683, acc: 0.93750\n",
      "[Train]: Step: 38000, loss: 0.50513, acc: 0.84375\n",
      "[Train]: Step: 38100, loss: 0.33562, acc: 0.93750\n",
      "[Train]: Step: 38200, loss: 0.59392, acc: 0.75000\n",
      "[Train]: Step: 38300, loss: 0.62767, acc: 0.78125\n",
      "[Train]: Step: 38400, loss: 0.39361, acc: 0.84375\n",
      "[Train]: Step: 38500, loss: 0.32453, acc: 0.90625\n",
      "[Train]: Step: 38600, loss: 0.60313, acc: 0.75000\n",
      "[Train]: Step: 38700, loss: 1.05299, acc: 0.71875\n",
      "[Train]: Step: 38800, loss: 0.55286, acc: 0.78125\n",
      "[Train]: Step: 38900, loss: 0.80326, acc: 0.78125\n",
      "[Train]: Step: 39000, loss: 0.32972, acc: 0.87500\n",
      "[Train]: Step: 39100, loss: 0.72834, acc: 0.78125\n",
      "[Train]: Step: 39200, loss: 0.56434, acc: 0.81250\n",
      "[Train]: Step: 39300, loss: 0.28774, acc: 0.87500\n",
      "[Train]: Step: 39400, loss: 0.56071, acc: 0.78125\n",
      "[Train]: Step: 39500, loss: 0.41723, acc: 0.78125\n",
      "[Train]: Step: 39600, loss: 0.43050, acc: 0.87500\n",
      "[Train]: Step: 39700, loss: 0.66557, acc: 0.84375\n",
      "[Train]: Step: 39800, loss: 0.37348, acc: 0.93750\n",
      "[Train]: Step: 39900, loss: 0.49638, acc: 0.84375\n",
      "[Train]: Step: 40000, loss: 0.73842, acc: 0.78125\n",
      "[Test]: Step: 40000, acc: 0.50000\n",
      "[Train]: Step: 40100, loss: 0.63155, acc: 0.71875\n",
      "[Train]: Step: 40200, loss: 0.73846, acc: 0.71875\n",
      "[Train]: Step: 40300, loss: 0.44782, acc: 0.87500\n",
      "[Train]: Step: 40400, loss: 0.38420, acc: 0.81250\n",
      "[Train]: Step: 40500, loss: 0.81919, acc: 0.75000\n",
      "[Train]: Step: 40600, loss: 0.60237, acc: 0.87500\n",
      "[Train]: Step: 40700, loss: 0.37727, acc: 0.84375\n",
      "[Train]: Step: 40800, loss: 0.40053, acc: 0.90625\n",
      "[Train]: Step: 40900, loss: 0.60908, acc: 0.81250\n",
      "[Train]: Step: 41000, loss: 0.56117, acc: 0.87500\n",
      "[Train]: Step: 41100, loss: 0.28046, acc: 0.87500\n",
      "[Train]: Step: 41200, loss: 0.62179, acc: 0.81250\n",
      "[Train]: Step: 41300, loss: 0.54471, acc: 0.81250\n",
      "[Train]: Step: 41400, loss: 0.34416, acc: 0.90625\n",
      "[Train]: Step: 41500, loss: 0.78799, acc: 0.75000\n",
      "[Train]: Step: 41600, loss: 0.52293, acc: 0.78125\n",
      "[Train]: Step: 41700, loss: 0.78273, acc: 0.68750\n",
      "[Train]: Step: 41800, loss: 0.33461, acc: 0.87500\n",
      "[Train]: Step: 41900, loss: 0.54387, acc: 0.84375\n",
      "[Train]: Step: 42000, loss: 0.66383, acc: 0.78125\n",
      "[Train]: Step: 42100, loss: 0.61121, acc: 0.78125\n",
      "[Train]: Step: 42200, loss: 0.58158, acc: 0.87500\n",
      "[Train]: Step: 42300, loss: 0.65843, acc: 0.71875\n",
      "[Train]: Step: 42400, loss: 0.45865, acc: 0.87500\n",
      "[Train]: Step: 42500, loss: 0.53016, acc: 0.81250\n",
      "[Train]: Step: 42600, loss: 0.58345, acc: 0.84375\n",
      "[Train]: Step: 42700, loss: 0.53140, acc: 0.78125\n",
      "[Train]: Step: 42800, loss: 0.28836, acc: 0.93750\n",
      "[Train]: Step: 42900, loss: 0.80380, acc: 0.75000\n",
      "[Train]: Step: 43000, loss: 0.81765, acc: 0.65625\n",
      "[Train]: Step: 43100, loss: 0.54646, acc: 0.81250\n",
      "[Train]: Step: 43200, loss: 0.26865, acc: 0.84375\n",
      "[Train]: Step: 43300, loss: 0.26998, acc: 0.90625\n",
      "[Train]: Step: 43400, loss: 0.34558, acc: 0.87500\n",
      "[Train]: Step: 43500, loss: 0.56961, acc: 0.81250\n",
      "[Train]: Step: 43600, loss: 0.52040, acc: 0.84375\n",
      "[Train]: Step: 43700, loss: 0.63066, acc: 0.78125\n",
      "[Train]: Step: 43800, loss: 0.51631, acc: 0.87500\n",
      "[Train]: Step: 43900, loss: 0.40616, acc: 0.93750\n",
      "[Train]: Step: 44000, loss: 0.63359, acc: 0.75000\n",
      "[Train]: Step: 44100, loss: 0.64700, acc: 0.75000\n",
      "[Train]: Step: 44200, loss: 0.42394, acc: 0.78125\n",
      "[Train]: Step: 44300, loss: 0.46275, acc: 0.78125\n",
      "[Train]: Step: 44400, loss: 0.58795, acc: 0.78125\n",
      "[Train]: Step: 44500, loss: 0.21455, acc: 0.93750\n",
      "[Train]: Step: 44600, loss: 0.70608, acc: 0.81250\n",
      "[Train]: Step: 44700, loss: 0.74737, acc: 0.84375\n",
      "[Train]: Step: 44800, loss: 0.40354, acc: 0.87500\n",
      "[Train]: Step: 44900, loss: 0.12197, acc: 1.00000\n",
      "[Train]: Step: 45000, loss: 0.42033, acc: 0.81250\n",
      "[Test]: Step: 45000, acc: 0.50000\n",
      "[Train]: Step: 45100, loss: 0.29460, acc: 0.87500\n",
      "[Train]: Step: 45200, loss: 0.65975, acc: 0.71875\n",
      "[Train]: Step: 45300, loss: 0.55080, acc: 0.78125\n",
      "[Train]: Step: 45400, loss: 0.39106, acc: 0.84375\n",
      "[Train]: Step: 45500, loss: 0.59676, acc: 0.75000\n",
      "[Train]: Step: 45600, loss: 0.38745, acc: 0.87500\n",
      "[Train]: Step: 45700, loss: 0.46687, acc: 0.78125\n",
      "[Train]: Step: 45800, loss: 0.66537, acc: 0.81250\n",
      "[Train]: Step: 45900, loss: 0.60169, acc: 0.78125\n",
      "[Train]: Step: 46000, loss: 0.39829, acc: 0.87500\n",
      "[Train]: Step: 46100, loss: 0.49530, acc: 0.84375\n",
      "[Train]: Step: 46200, loss: 0.79128, acc: 0.75000\n",
      "[Train]: Step: 46300, loss: 0.93672, acc: 0.75000\n",
      "[Train]: Step: 46400, loss: 0.44064, acc: 0.81250\n",
      "[Train]: Step: 46500, loss: 0.57172, acc: 0.75000\n",
      "[Train]: Step: 46600, loss: 0.68128, acc: 0.81250\n",
      "[Train]: Step: 46700, loss: 0.70884, acc: 0.75000\n",
      "[Train]: Step: 46800, loss: 0.52178, acc: 0.84375\n",
      "[Train]: Step: 46900, loss: 0.51835, acc: 0.78125\n",
      "[Train]: Step: 47000, loss: 0.48431, acc: 0.84375\n",
      "[Train]: Step: 47100, loss: 0.45541, acc: 0.84375\n",
      "[Train]: Step: 47200, loss: 0.74481, acc: 0.75000\n",
      "[Train]: Step: 47300, loss: 0.28883, acc: 0.87500\n",
      "[Train]: Step: 47400, loss: 0.33609, acc: 0.90625\n",
      "[Train]: Step: 47500, loss: 0.61844, acc: 0.75000\n",
      "[Train]: Step: 47600, loss: 0.35330, acc: 0.90625\n",
      "[Train]: Step: 47700, loss: 0.64964, acc: 0.87500\n",
      "[Train]: Step: 47800, loss: 0.46733, acc: 0.75000\n",
      "[Train]: Step: 47900, loss: 0.30590, acc: 0.87500\n",
      "[Train]: Step: 48000, loss: 0.65180, acc: 0.75000\n",
      "[Train]: Step: 48100, loss: 0.97835, acc: 0.75000\n",
      "[Train]: Step: 48200, loss: 0.34433, acc: 0.90625\n",
      "[Train]: Step: 48300, loss: 0.60120, acc: 0.84375\n",
      "[Train]: Step: 48400, loss: 0.35926, acc: 0.90625\n",
      "[Train]: Step: 48500, loss: 0.48349, acc: 0.84375\n",
      "[Train]: Step: 48600, loss: 0.67105, acc: 0.71875\n",
      "[Train]: Step: 48700, loss: 0.22385, acc: 0.93750\n",
      "[Train]: Step: 48800, loss: 0.24679, acc: 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train]: Step: 48900, loss: 0.57578, acc: 0.78125\n",
      "[Train]: Step: 49000, loss: 0.90389, acc: 0.71875\n",
      "[Train]: Step: 49100, loss: 0.28273, acc: 0.87500\n",
      "[Train]: Step: 49200, loss: 0.49929, acc: 0.81250\n",
      "[Train]: Step: 49300, loss: 0.34779, acc: 0.90625\n",
      "[Train]: Step: 49400, loss: 0.39375, acc: 0.87500\n",
      "[Train]: Step: 49500, loss: 0.09471, acc: 1.00000\n",
      "[Train]: Step: 49600, loss: 0.36960, acc: 0.90625\n",
      "[Train]: Step: 49700, loss: 0.54113, acc: 0.78125\n",
      "[Train]: Step: 49800, loss: 0.40483, acc: 0.84375\n",
      "[Train]: Step: 49900, loss: 0.35830, acc: 0.84375\n",
      "[Train]: Step: 50000, loss: 0.78756, acc: 0.78125\n",
      "[Test]: Step: 50000, acc: 0.81250\n",
      "[Train]: Step: 50100, loss: 0.28073, acc: 0.93750\n",
      "[Train]: Step: 50200, loss: 0.29085, acc: 0.93750\n",
      "[Train]: Step: 50300, loss: 0.39211, acc: 0.90625\n",
      "[Train]: Step: 50400, loss: 0.67992, acc: 0.78125\n",
      "[Train]: Step: 50500, loss: 0.34814, acc: 0.87500\n",
      "[Train]: Step: 50600, loss: 0.80440, acc: 0.81250\n",
      "[Train]: Step: 50700, loss: 0.72868, acc: 0.75000\n",
      "[Train]: Step: 50800, loss: 0.33019, acc: 0.90625\n",
      "[Train]: Step: 50900, loss: 0.62640, acc: 0.81250\n",
      "[Train]: Step: 51000, loss: 0.30211, acc: 0.93750\n",
      "[Train]: Step: 51100, loss: 0.34127, acc: 0.96875\n",
      "[Train]: Step: 51200, loss: 0.30720, acc: 0.93750\n",
      "[Train]: Step: 51300, loss: 0.24138, acc: 0.90625\n",
      "[Train]: Step: 51400, loss: 0.26682, acc: 0.90625\n",
      "[Train]: Step: 51500, loss: 0.86491, acc: 0.75000\n",
      "[Train]: Step: 51600, loss: 0.86052, acc: 0.75000\n",
      "[Train]: Step: 51700, loss: 0.44028, acc: 0.84375\n",
      "[Train]: Step: 51800, loss: 0.32014, acc: 0.90625\n",
      "[Train]: Step: 51900, loss: 0.64313, acc: 0.84375\n",
      "[Train]: Step: 52000, loss: 0.36603, acc: 0.87500\n",
      "[Train]: Step: 52100, loss: 0.44306, acc: 0.84375\n",
      "[Train]: Step: 52200, loss: 0.67482, acc: 0.81250\n",
      "[Train]: Step: 52300, loss: 0.44137, acc: 0.90625\n",
      "[Train]: Step: 52400, loss: 0.38759, acc: 0.84375\n",
      "[Train]: Step: 52500, loss: 0.80158, acc: 0.65625\n",
      "[Train]: Step: 52600, loss: 0.45394, acc: 0.78125\n",
      "[Train]: Step: 52700, loss: 0.35225, acc: 0.84375\n",
      "[Train]: Step: 52800, loss: 0.43148, acc: 0.87500\n",
      "[Train]: Step: 52900, loss: 0.35643, acc: 0.90625\n",
      "[Train]: Step: 53000, loss: 0.37335, acc: 0.87500\n",
      "[Train]: Step: 53100, loss: 0.52115, acc: 0.81250\n",
      "[Train]: Step: 53200, loss: 0.65493, acc: 0.81250\n",
      "[Train]: Step: 53300, loss: 0.48026, acc: 0.81250\n",
      "[Train]: Step: 53400, loss: 0.78857, acc: 0.71875\n",
      "[Train]: Step: 53500, loss: 0.32381, acc: 0.84375\n",
      "[Train]: Step: 53600, loss: 0.56923, acc: 0.71875\n",
      "[Train]: Step: 53700, loss: 0.47695, acc: 0.87500\n",
      "[Train]: Step: 53800, loss: 0.30528, acc: 0.93750\n",
      "[Train]: Step: 53900, loss: 0.69250, acc: 0.84375\n",
      "[Train]: Step: 54000, loss: 0.45082, acc: 0.84375\n",
      "[Train]: Step: 54100, loss: 0.31612, acc: 0.87500\n",
      "[Train]: Step: 54200, loss: 0.56951, acc: 0.84375\n",
      "[Train]: Step: 54300, loss: 0.58045, acc: 0.81250\n",
      "[Train]: Step: 54400, loss: 0.56507, acc: 0.78125\n",
      "[Train]: Step: 54500, loss: 0.40838, acc: 0.84375\n",
      "[Train]: Step: 54600, loss: 0.57126, acc: 0.84375\n",
      "[Train]: Step: 54700, loss: 0.62815, acc: 0.75000\n",
      "[Train]: Step: 54800, loss: 0.62554, acc: 0.78125\n",
      "[Train]: Step: 54900, loss: 0.93935, acc: 0.71875\n",
      "[Train]: Step: 55000, loss: 0.44943, acc: 0.90625\n",
      "[Test]: Step: 55000, acc: 0.68750\n",
      "[Train]: Step: 55100, loss: 0.25390, acc: 0.90625\n",
      "[Train]: Step: 55200, loss: 0.76769, acc: 0.81250\n",
      "[Train]: Step: 55300, loss: 0.62251, acc: 0.78125\n",
      "[Train]: Step: 55400, loss: 0.29349, acc: 0.90625\n",
      "[Train]: Step: 55500, loss: 0.50238, acc: 0.81250\n",
      "[Train]: Step: 55600, loss: 0.33096, acc: 0.87500\n",
      "[Train]: Step: 55700, loss: 0.45631, acc: 0.81250\n",
      "[Train]: Step: 55800, loss: 0.42634, acc: 0.84375\n",
      "[Train]: Step: 55900, loss: 0.42081, acc: 0.84375\n",
      "[Train]: Step: 56000, loss: 0.59918, acc: 0.81250\n",
      "[Train]: Step: 56100, loss: 0.34858, acc: 0.90625\n",
      "[Train]: Step: 56200, loss: 0.48386, acc: 0.87500\n",
      "[Train]: Step: 56300, loss: 0.41811, acc: 0.90625\n",
      "[Train]: Step: 56400, loss: 0.14101, acc: 0.93750\n",
      "[Train]: Step: 56500, loss: 0.33596, acc: 0.84375\n",
      "[Train]: Step: 56600, loss: 0.63595, acc: 0.81250\n",
      "[Train]: Step: 56700, loss: 0.49141, acc: 0.84375\n",
      "[Train]: Step: 56800, loss: 0.35794, acc: 0.90625\n",
      "[Train]: Step: 56900, loss: 0.44878, acc: 0.87500\n",
      "[Train]: Step: 57000, loss: 0.76594, acc: 0.78125\n",
      "[Train]: Step: 57100, loss: 0.26053, acc: 0.90625\n",
      "[Train]: Step: 57200, loss: 0.54590, acc: 0.87500\n",
      "[Train]: Step: 57300, loss: 0.55532, acc: 0.84375\n",
      "[Train]: Step: 57400, loss: 0.17746, acc: 0.96875\n",
      "[Train]: Step: 57500, loss: 0.62343, acc: 0.87500\n",
      "[Train]: Step: 57600, loss: 0.49803, acc: 0.87500\n",
      "[Train]: Step: 57700, loss: 0.29650, acc: 0.84375\n",
      "[Train]: Step: 57800, loss: 0.36495, acc: 0.84375\n",
      "[Train]: Step: 57900, loss: 0.64042, acc: 0.78125\n",
      "[Train]: Step: 58000, loss: 0.27220, acc: 0.87500\n",
      "[Train]: Step: 58100, loss: 0.65772, acc: 0.81250\n",
      "[Train]: Step: 58200, loss: 0.42125, acc: 0.87500\n",
      "[Train]: Step: 58300, loss: 0.35287, acc: 0.87500\n",
      "[Train]: Step: 58400, loss: 0.48953, acc: 0.87500\n",
      "[Train]: Step: 58500, loss: 0.44662, acc: 0.90625\n",
      "[Train]: Step: 58600, loss: 0.70110, acc: 0.75000\n",
      "[Train]: Step: 58700, loss: 0.51698, acc: 0.84375\n",
      "[Train]: Step: 58800, loss: 0.20544, acc: 0.90625\n",
      "[Train]: Step: 58900, loss: 0.85145, acc: 0.68750\n",
      "[Train]: Step: 59000, loss: 0.46524, acc: 0.87500\n",
      "[Train]: Step: 59100, loss: 0.12420, acc: 0.96875\n",
      "[Train]: Step: 59200, loss: 0.13102, acc: 0.96875\n",
      "[Train]: Step: 59300, loss: 0.38347, acc: 0.87500\n",
      "[Train]: Step: 59400, loss: 0.42136, acc: 0.84375\n",
      "[Train]: Step: 59500, loss: 0.28140, acc: 0.81250\n",
      "[Train]: Step: 59600, loss: 0.72495, acc: 0.84375\n",
      "[Train]: Step: 59700, loss: 0.34622, acc: 0.84375\n",
      "[Train]: Step: 59800, loss: 0.40639, acc: 0.87500\n",
      "[Train]: Step: 59900, loss: 0.55475, acc: 0.81250\n",
      "[Train]: Step: 60000, loss: 0.18571, acc: 0.90625\n",
      "[Test]: Step: 60000, acc: 0.75000\n",
      "[Train]: Step: 60100, loss: 0.17111, acc: 0.93750\n",
      "[Train]: Step: 60200, loss: 0.27209, acc: 0.93750\n",
      "[Train]: Step: 60300, loss: 0.46760, acc: 0.81250\n",
      "[Train]: Step: 60400, loss: 0.39209, acc: 0.84375\n",
      "[Train]: Step: 60500, loss: 0.18965, acc: 0.96875\n",
      "[Train]: Step: 60600, loss: 0.35292, acc: 0.84375\n",
      "[Train]: Step: 60700, loss: 0.21243, acc: 0.93750\n",
      "[Train]: Step: 60800, loss: 0.62539, acc: 0.84375\n",
      "[Train]: Step: 60900, loss: 0.20367, acc: 0.93750\n",
      "[Train]: Step: 61000, loss: 0.48447, acc: 0.81250\n",
      "[Train]: Step: 61100, loss: 0.18307, acc: 0.90625\n",
      "[Train]: Step: 61200, loss: 0.37801, acc: 0.81250\n",
      "[Train]: Step: 61300, loss: 0.70857, acc: 0.84375\n",
      "[Train]: Step: 61400, loss: 0.39381, acc: 0.84375\n",
      "[Train]: Step: 61500, loss: 0.15332, acc: 0.96875\n",
      "[Train]: Step: 61600, loss: 0.58483, acc: 0.84375\n",
      "[Train]: Step: 61700, loss: 0.24035, acc: 0.93750\n",
      "[Train]: Step: 61800, loss: 0.49015, acc: 0.81250\n",
      "[Train]: Step: 61900, loss: 0.60093, acc: 0.78125\n",
      "[Train]: Step: 62000, loss: 0.34964, acc: 0.87500\n",
      "[Train]: Step: 62100, loss: 0.24542, acc: 0.87500\n",
      "[Train]: Step: 62200, loss: 0.33785, acc: 0.87500\n",
      "[Train]: Step: 62300, loss: 0.39225, acc: 0.81250\n",
      "[Train]: Step: 62400, loss: 0.38481, acc: 0.87500\n",
      "[Train]: Step: 62500, loss: 0.56858, acc: 0.78125\n",
      "[Train]: Step: 62600, loss: 0.19040, acc: 0.93750\n",
      "[Train]: Step: 62700, loss: 0.25081, acc: 0.96875\n",
      "[Train]: Step: 62800, loss: 0.65283, acc: 0.75000\n",
      "[Train]: Step: 62900, loss: 0.42428, acc: 0.90625\n",
      "[Train]: Step: 63000, loss: 0.48442, acc: 0.87500\n",
      "[Train]: Step: 63100, loss: 0.29159, acc: 0.90625\n",
      "[Train]: Step: 63200, loss: 0.46336, acc: 0.84375\n",
      "[Train]: Step: 63300, loss: 0.47044, acc: 0.78125\n",
      "[Train]: Step: 63400, loss: 0.43622, acc: 0.87500\n",
      "[Train]: Step: 63500, loss: 0.18730, acc: 0.93750\n",
      "[Train]: Step: 63600, loss: 0.24592, acc: 0.93750\n",
      "[Train]: Step: 63700, loss: 0.34164, acc: 0.84375\n",
      "[Train]: Step: 63800, loss: 0.32585, acc: 0.87500\n",
      "[Train]: Step: 63900, loss: 0.40243, acc: 0.90625\n",
      "[Train]: Step: 64000, loss: 0.23141, acc: 0.90625\n",
      "[Train]: Step: 64100, loss: 0.63590, acc: 0.78125\n",
      "[Train]: Step: 64200, loss: 0.29733, acc: 0.87500\n",
      "[Train]: Step: 64300, loss: 0.39447, acc: 0.87500\n",
      "[Train]: Step: 64400, loss: 0.43255, acc: 0.84375\n",
      "[Train]: Step: 64500, loss: 0.70555, acc: 0.71875\n",
      "[Train]: Step: 64600, loss: 0.74964, acc: 0.75000\n",
      "[Train]: Step: 64700, loss: 0.19118, acc: 0.96875\n",
      "[Train]: Step: 64800, loss: 0.19614, acc: 0.90625\n",
      "[Train]: Step: 64900, loss: 0.48516, acc: 0.81250\n",
      "[Train]: Step: 65000, loss: 0.36933, acc: 0.87500\n",
      "[Test]: Step: 65000, acc: 0.68750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train]: Step: 65100, loss: 0.36980, acc: 0.84375\n",
      "[Train]: Step: 65200, loss: 0.36569, acc: 0.90625\n",
      "[Train]: Step: 65300, loss: 0.56026, acc: 0.78125\n",
      "[Train]: Step: 65400, loss: 0.34312, acc: 0.87500\n",
      "[Train]: Step: 65500, loss: 0.42252, acc: 0.87500\n",
      "[Train]: Step: 65600, loss: 0.10810, acc: 0.93750\n",
      "[Train]: Step: 65700, loss: 0.84083, acc: 0.75000\n",
      "[Train]: Step: 65800, loss: 0.36714, acc: 0.93750\n",
      "[Train]: Step: 65900, loss: 0.44557, acc: 0.87500\n",
      "[Train]: Step: 66000, loss: 0.45806, acc: 0.84375\n",
      "[Train]: Step: 66100, loss: 0.41657, acc: 0.84375\n",
      "[Train]: Step: 66200, loss: 0.35467, acc: 0.81250\n",
      "[Train]: Step: 66300, loss: 0.53282, acc: 0.81250\n",
      "[Train]: Step: 66400, loss: 0.14799, acc: 0.93750\n",
      "[Train]: Step: 66500, loss: 0.52683, acc: 0.84375\n",
      "[Train]: Step: 66600, loss: 0.29128, acc: 0.90625\n",
      "[Train]: Step: 66700, loss: 0.45344, acc: 0.81250\n",
      "[Train]: Step: 66800, loss: 0.37899, acc: 0.81250\n",
      "[Train]: Step: 66900, loss: 0.32316, acc: 0.87500\n",
      "[Train]: Step: 67000, loss: 0.56950, acc: 0.75000\n",
      "[Train]: Step: 67100, loss: 0.24525, acc: 0.90625\n",
      "[Train]: Step: 67200, loss: 0.37094, acc: 0.87500\n",
      "[Train]: Step: 67300, loss: 0.24609, acc: 0.90625\n",
      "[Train]: Step: 67400, loss: 0.61984, acc: 0.81250\n",
      "[Train]: Step: 67500, loss: 0.46895, acc: 0.87500\n",
      "[Train]: Step: 67600, loss: 0.32813, acc: 0.93750\n",
      "[Train]: Step: 67700, loss: 0.56656, acc: 0.81250\n",
      "[Train]: Step: 67800, loss: 0.28482, acc: 0.87500\n",
      "[Train]: Step: 67900, loss: 0.19104, acc: 0.90625\n",
      "[Train]: Step: 68000, loss: 0.39764, acc: 0.87500\n",
      "[Train]: Step: 68100, loss: 0.40119, acc: 0.87500\n",
      "[Train]: Step: 68200, loss: 0.11649, acc: 1.00000\n",
      "[Train]: Step: 68300, loss: 0.22431, acc: 0.96875\n",
      "[Train]: Step: 68400, loss: 0.52188, acc: 0.81250\n",
      "[Train]: Step: 68500, loss: 0.15108, acc: 0.96875\n",
      "[Train]: Step: 68600, loss: 0.23599, acc: 0.93750\n",
      "[Train]: Step: 68700, loss: 0.30246, acc: 0.87500\n",
      "[Train]: Step: 68800, loss: 0.25248, acc: 0.90625\n",
      "[Train]: Step: 68900, loss: 0.10741, acc: 0.93750\n",
      "[Train]: Step: 69000, loss: 0.20425, acc: 0.93750\n",
      "[Train]: Step: 69100, loss: 0.83528, acc: 0.81250\n",
      "[Train]: Step: 69200, loss: 0.30020, acc: 0.87500\n",
      "[Train]: Step: 69300, loss: 0.43642, acc: 0.87500\n",
      "[Train]: Step: 69400, loss: 0.42640, acc: 0.87500\n",
      "[Train]: Step: 69500, loss: 0.30638, acc: 0.90625\n",
      "[Train]: Step: 69600, loss: 0.48651, acc: 0.78125\n",
      "[Train]: Step: 69700, loss: 0.15953, acc: 0.96875\n",
      "[Train]: Step: 69800, loss: 0.22013, acc: 0.87500\n",
      "[Train]: Step: 69900, loss: 0.25753, acc: 0.90625\n",
      "[Train]: Step: 70000, loss: 0.17539, acc: 0.93750\n",
      "[Test]: Step: 70000, acc: 0.56250\n",
      "[Train]: Step: 70100, loss: 0.59344, acc: 0.84375\n",
      "[Train]: Step: 70200, loss: 0.26119, acc: 0.93750\n",
      "[Train]: Step: 70300, loss: 0.50697, acc: 0.90625\n",
      "[Train]: Step: 70400, loss: 0.38501, acc: 0.87500\n",
      "[Train]: Step: 70500, loss: 0.05576, acc: 1.00000\n",
      "[Train]: Step: 70600, loss: 0.27867, acc: 0.87500\n",
      "[Train]: Step: 70700, loss: 0.33638, acc: 0.81250\n",
      "[Train]: Step: 70800, loss: 0.60904, acc: 0.84375\n",
      "[Train]: Step: 70900, loss: 0.61618, acc: 0.81250\n",
      "[Train]: Step: 71000, loss: 0.17723, acc: 0.93750\n",
      "[Train]: Step: 71100, loss: 0.24193, acc: 0.90625\n",
      "[Train]: Step: 71200, loss: 0.25028, acc: 0.90625\n",
      "[Train]: Step: 71300, loss: 0.37389, acc: 0.87500\n",
      "[Train]: Step: 71400, loss: 0.30683, acc: 0.84375\n",
      "[Train]: Step: 71500, loss: 0.41570, acc: 0.87500\n",
      "[Train]: Step: 71600, loss: 0.66006, acc: 0.78125\n",
      "[Train]: Step: 71700, loss: 0.34992, acc: 0.87500\n",
      "[Train]: Step: 71800, loss: 0.42008, acc: 0.84375\n",
      "[Train]: Step: 71900, loss: 0.53323, acc: 0.87500\n",
      "[Train]: Step: 72000, loss: 0.28646, acc: 0.93750\n",
      "[Train]: Step: 72100, loss: 0.26528, acc: 0.90625\n",
      "[Train]: Step: 72200, loss: 0.45953, acc: 0.84375\n",
      "[Train]: Step: 72300, loss: 0.37329, acc: 0.87500\n",
      "[Train]: Step: 72400, loss: 0.56968, acc: 0.78125\n",
      "[Train]: Step: 72500, loss: 0.34864, acc: 0.84375\n",
      "[Train]: Step: 72600, loss: 0.23064, acc: 0.90625\n",
      "[Train]: Step: 72700, loss: 0.43096, acc: 0.84375\n",
      "[Train]: Step: 72800, loss: 0.86355, acc: 0.81250\n",
      "[Train]: Step: 72900, loss: 0.65276, acc: 0.78125\n",
      "[Train]: Step: 73000, loss: 0.18922, acc: 0.93750\n",
      "[Train]: Step: 73100, loss: 0.15509, acc: 0.96875\n",
      "[Train]: Step: 73200, loss: 0.21038, acc: 0.93750\n",
      "[Train]: Step: 73300, loss: 0.20182, acc: 0.93750\n",
      "[Train]: Step: 73400, loss: 0.43023, acc: 0.84375\n",
      "[Train]: Step: 73500, loss: 0.46500, acc: 0.81250\n",
      "[Train]: Step: 73600, loss: 0.46053, acc: 0.90625\n",
      "[Train]: Step: 73700, loss: 0.27510, acc: 0.87500\n",
      "[Train]: Step: 73800, loss: 0.24492, acc: 0.90625\n",
      "[Train]: Step: 73900, loss: 0.65647, acc: 0.78125\n",
      "[Train]: Step: 74000, loss: 0.32576, acc: 0.90625\n",
      "[Train]: Step: 74100, loss: 0.16525, acc: 0.96875\n",
      "[Train]: Step: 74200, loss: 0.43350, acc: 0.87500\n",
      "[Train]: Step: 74300, loss: 0.88255, acc: 0.71875\n",
      "[Train]: Step: 74400, loss: 0.50665, acc: 0.78125\n",
      "[Train]: Step: 74500, loss: 0.06997, acc: 0.96875\n",
      "[Train]: Step: 74600, loss: 1.15516, acc: 0.71875\n",
      "[Train]: Step: 74700, loss: 0.20521, acc: 0.90625\n",
      "[Train]: Step: 74800, loss: 0.52892, acc: 0.84375\n",
      "[Train]: Step: 74900, loss: 0.27485, acc: 0.90625\n",
      "[Train]: Step: 75000, loss: 0.17180, acc: 0.93750\n",
      "[Test]: Step: 75000, acc: 0.59375\n",
      "[Train]: Step: 75100, loss: 0.22011, acc: 0.93750\n",
      "[Train]: Step: 75200, loss: 0.29621, acc: 0.90625\n",
      "[Train]: Step: 75300, loss: 0.32340, acc: 0.90625\n",
      "[Train]: Step: 75400, loss: 0.08290, acc: 1.00000\n",
      "[Train]: Step: 75500, loss: 0.51278, acc: 0.90625\n",
      "[Train]: Step: 75600, loss: 0.12394, acc: 0.90625\n",
      "[Train]: Step: 75700, loss: 0.40084, acc: 0.90625\n",
      "[Train]: Step: 75800, loss: 0.56965, acc: 0.87500\n",
      "[Train]: Step: 75900, loss: 0.39607, acc: 0.84375\n",
      "[Train]: Step: 76000, loss: 0.20927, acc: 0.90625\n",
      "[Train]: Step: 76100, loss: 0.33052, acc: 0.90625\n",
      "[Train]: Step: 76200, loss: 0.13400, acc: 0.96875\n",
      "[Train]: Step: 76300, loss: 0.29705, acc: 0.90625\n",
      "[Train]: Step: 76400, loss: 0.29280, acc: 0.93750\n",
      "[Train]: Step: 76500, loss: 0.24769, acc: 0.90625\n",
      "[Train]: Step: 76600, loss: 0.21414, acc: 0.87500\n",
      "[Train]: Step: 76700, loss: 0.42323, acc: 0.81250\n",
      "[Train]: Step: 76800, loss: 0.47273, acc: 0.87500\n",
      "[Train]: Step: 76900, loss: 0.57884, acc: 0.90625\n",
      "[Train]: Step: 77000, loss: 0.06792, acc: 0.96875\n",
      "[Train]: Step: 77100, loss: 0.45539, acc: 0.84375\n",
      "[Train]: Step: 77200, loss: 0.51914, acc: 0.81250\n",
      "[Train]: Step: 77300, loss: 0.22794, acc: 0.90625\n",
      "[Train]: Step: 77400, loss: 0.19629, acc: 0.93750\n",
      "[Train]: Step: 77500, loss: 0.33609, acc: 0.90625\n",
      "[Train]: Step: 77600, loss: 0.18325, acc: 0.87500\n",
      "[Train]: Step: 77700, loss: 0.48473, acc: 0.84375\n",
      "[Train]: Step: 77800, loss: 0.35421, acc: 0.93750\n",
      "[Train]: Step: 77900, loss: 0.33905, acc: 0.87500\n",
      "[Train]: Step: 78000, loss: 0.14927, acc: 0.96875\n",
      "[Train]: Step: 78100, loss: 0.39922, acc: 0.84375\n",
      "[Train]: Step: 78200, loss: 0.15081, acc: 0.93750\n",
      "[Train]: Step: 78300, loss: 0.33706, acc: 0.90625\n",
      "[Train]: Step: 78400, loss: 0.56287, acc: 0.87500\n",
      "[Train]: Step: 78500, loss: 0.37712, acc: 0.90625\n",
      "[Train]: Step: 78600, loss: 0.52754, acc: 0.81250\n",
      "[Train]: Step: 78700, loss: 0.12191, acc: 0.96875\n",
      "[Train]: Step: 78800, loss: 0.31935, acc: 0.93750\n",
      "[Train]: Step: 78900, loss: 0.38467, acc: 0.84375\n",
      "[Train]: Step: 79000, loss: 0.12322, acc: 0.96875\n",
      "[Train]: Step: 79100, loss: 0.21725, acc: 0.90625\n",
      "[Train]: Step: 79200, loss: 0.07884, acc: 1.00000\n",
      "[Train]: Step: 79300, loss: 0.19316, acc: 0.93750\n",
      "[Train]: Step: 79400, loss: 0.38703, acc: 0.90625\n",
      "[Train]: Step: 79500, loss: 0.22779, acc: 0.87500\n",
      "[Train]: Step: 79600, loss: 0.34693, acc: 0.87500\n",
      "[Train]: Step: 79700, loss: 0.20717, acc: 0.93750\n",
      "[Train]: Step: 79800, loss: 0.13587, acc: 1.00000\n",
      "[Train]: Step: 79900, loss: 0.06884, acc: 1.00000\n",
      "[Train]: Step: 80000, loss: 0.08554, acc: 0.96875\n",
      "[Test]: Step: 80000, acc: 0.78125\n",
      "[Train]: Step: 80100, loss: 0.52155, acc: 0.90625\n",
      "[Train]: Step: 80200, loss: 0.16280, acc: 0.90625\n",
      "[Train]: Step: 80300, loss: 0.39320, acc: 0.84375\n",
      "[Train]: Step: 80400, loss: 0.34751, acc: 0.84375\n",
      "[Train]: Step: 80500, loss: 0.27116, acc: 0.87500\n",
      "[Train]: Step: 80600, loss: 0.82522, acc: 0.78125\n",
      "[Train]: Step: 80700, loss: 0.32012, acc: 0.90625\n",
      "[Train]: Step: 80800, loss: 0.35748, acc: 0.84375\n",
      "[Train]: Step: 80900, loss: 0.35172, acc: 0.93750\n",
      "[Train]: Step: 81000, loss: 0.14695, acc: 0.90625\n",
      "[Train]: Step: 81100, loss: 0.17092, acc: 0.90625\n",
      "[Train]: Step: 81200, loss: 0.32500, acc: 0.87500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train]: Step: 81300, loss: 0.19558, acc: 0.96875\n",
      "[Train]: Step: 81400, loss: 0.29990, acc: 0.87500\n",
      "[Train]: Step: 81500, loss: 0.36765, acc: 0.93750\n",
      "[Train]: Step: 81600, loss: 0.24128, acc: 0.93750\n",
      "[Train]: Step: 81700, loss: 0.48309, acc: 0.87500\n",
      "[Train]: Step: 81800, loss: 0.24142, acc: 0.90625\n",
      "[Train]: Step: 81900, loss: 0.11068, acc: 0.96875\n",
      "[Train]: Step: 82000, loss: 0.22546, acc: 0.87500\n",
      "[Train]: Step: 82100, loss: 0.32371, acc: 0.84375\n",
      "[Train]: Step: 82200, loss: 0.34415, acc: 0.87500\n",
      "[Train]: Step: 82300, loss: 0.27575, acc: 0.90625\n",
      "[Train]: Step: 82400, loss: 0.48307, acc: 0.81250\n",
      "[Train]: Step: 82500, loss: 0.27970, acc: 0.93750\n",
      "[Train]: Step: 82600, loss: 0.26343, acc: 0.93750\n",
      "[Train]: Step: 82700, loss: 0.29341, acc: 0.87500\n",
      "[Train]: Step: 82800, loss: 0.07105, acc: 0.96875\n",
      "[Train]: Step: 82900, loss: 0.34823, acc: 0.90625\n",
      "[Train]: Step: 83000, loss: 0.11928, acc: 0.96875\n",
      "[Train]: Step: 83100, loss: 0.27847, acc: 0.87500\n",
      "[Train]: Step: 83200, loss: 0.10664, acc: 0.96875\n",
      "[Train]: Step: 83300, loss: 0.46791, acc: 0.84375\n",
      "[Train]: Step: 83400, loss: 0.47915, acc: 0.78125\n",
      "[Train]: Step: 83500, loss: 0.17037, acc: 0.96875\n",
      "[Train]: Step: 83600, loss: 0.25271, acc: 0.90625\n",
      "[Train]: Step: 83700, loss: 0.34824, acc: 0.84375\n",
      "[Train]: Step: 83800, loss: 0.48269, acc: 0.87500\n",
      "[Train]: Step: 83900, loss: 0.37634, acc: 0.84375\n",
      "[Train]: Step: 84000, loss: 0.48744, acc: 0.90625\n",
      "[Train]: Step: 84100, loss: 0.33688, acc: 0.78125\n",
      "[Train]: Step: 84200, loss: 0.27982, acc: 0.90625\n",
      "[Train]: Step: 84300, loss: 0.32802, acc: 0.90625\n",
      "[Train]: Step: 84400, loss: 0.19594, acc: 0.93750\n",
      "[Train]: Step: 84500, loss: 0.21280, acc: 0.90625\n",
      "[Train]: Step: 84600, loss: 0.23284, acc: 0.93750\n",
      "[Train]: Step: 84700, loss: 0.27894, acc: 0.93750\n",
      "[Train]: Step: 84800, loss: 0.43524, acc: 0.90625\n",
      "[Train]: Step: 84900, loss: 0.43883, acc: 0.87500\n",
      "[Train]: Step: 85000, loss: 0.11069, acc: 0.96875\n",
      "[Test]: Step: 85000, acc: 0.75000\n",
      "[Train]: Step: 85100, loss: 0.18874, acc: 0.96875\n",
      "[Train]: Step: 85200, loss: 0.34749, acc: 0.87500\n",
      "[Train]: Step: 85300, loss: 0.31822, acc: 0.93750\n",
      "[Train]: Step: 85400, loss: 0.27310, acc: 0.87500\n",
      "[Train]: Step: 85500, loss: 0.13737, acc: 0.93750\n",
      "[Train]: Step: 85600, loss: 0.32298, acc: 0.90625\n",
      "[Train]: Step: 85700, loss: 0.31896, acc: 0.87500\n",
      "[Train]: Step: 85800, loss: 0.23639, acc: 0.90625\n",
      "[Train]: Step: 85900, loss: 0.32038, acc: 0.90625\n",
      "[Train]: Step: 86000, loss: 0.24920, acc: 0.90625\n",
      "[Train]: Step: 86100, loss: 0.25781, acc: 0.90625\n",
      "[Train]: Step: 86200, loss: 0.22559, acc: 0.93750\n",
      "[Train]: Step: 86300, loss: 0.10286, acc: 1.00000\n",
      "[Train]: Step: 86400, loss: 0.28045, acc: 0.90625\n",
      "[Train]: Step: 86500, loss: 0.37496, acc: 0.84375\n",
      "[Train]: Step: 86600, loss: 0.47595, acc: 0.84375\n",
      "[Train]: Step: 86700, loss: 0.47763, acc: 0.84375\n",
      "[Train]: Step: 86800, loss: 0.52471, acc: 0.87500\n",
      "[Train]: Step: 86900, loss: 0.32738, acc: 0.87500\n",
      "[Train]: Step: 87000, loss: 0.42451, acc: 0.87500\n",
      "[Train]: Step: 87100, loss: 0.60517, acc: 0.81250\n",
      "[Train]: Step: 87200, loss: 0.23905, acc: 0.93750\n",
      "[Train]: Step: 87300, loss: 0.11692, acc: 0.93750\n",
      "[Train]: Step: 87400, loss: 0.32443, acc: 0.90625\n",
      "[Train]: Step: 87500, loss: 0.30934, acc: 0.84375\n",
      "[Train]: Step: 87600, loss: 0.31492, acc: 0.90625\n",
      "[Train]: Step: 87700, loss: 0.31494, acc: 0.90625\n",
      "[Train]: Step: 87800, loss: 0.16855, acc: 0.93750\n",
      "[Train]: Step: 87900, loss: 0.56586, acc: 0.84375\n",
      "[Train]: Step: 88000, loss: 0.15827, acc: 0.93750\n",
      "[Train]: Step: 88100, loss: 0.03661, acc: 1.00000\n",
      "[Train]: Step: 88200, loss: 0.33262, acc: 0.84375\n",
      "[Train]: Step: 88300, loss: 0.22439, acc: 0.93750\n",
      "[Train]: Step: 88400, loss: 0.30416, acc: 0.90625\n",
      "[Train]: Step: 88500, loss: 0.63540, acc: 0.78125\n",
      "[Train]: Step: 88600, loss: 0.39276, acc: 0.87500\n",
      "[Train]: Step: 88700, loss: 0.34864, acc: 0.87500\n",
      "[Train]: Step: 88800, loss: 0.63133, acc: 0.78125\n",
      "[Train]: Step: 88900, loss: 0.25587, acc: 0.90625\n",
      "[Train]: Step: 89000, loss: 0.38220, acc: 0.87500\n",
      "[Train]: Step: 89100, loss: 0.09563, acc: 0.96875\n",
      "[Train]: Step: 89200, loss: 0.60808, acc: 0.81250\n",
      "[Train]: Step: 89300, loss: 0.74859, acc: 0.68750\n",
      "[Train]: Step: 89400, loss: 0.44955, acc: 0.87500\n",
      "[Train]: Step: 89500, loss: 0.17657, acc: 0.93750\n",
      "[Train]: Step: 89600, loss: 0.17844, acc: 0.93750\n",
      "[Train]: Step: 89700, loss: 0.13510, acc: 0.96875\n",
      "[Train]: Step: 89800, loss: 0.18894, acc: 0.90625\n",
      "[Train]: Step: 89900, loss: 0.36443, acc: 0.87500\n",
      "[Train]: Step: 90000, loss: 0.21958, acc: 0.87500\n",
      "[Test]: Step: 90000, acc: 0.78125\n",
      "[Train]: Step: 90100, loss: 0.15951, acc: 0.93750\n",
      "[Train]: Step: 90200, loss: 0.69441, acc: 0.84375\n",
      "[Train]: Step: 90300, loss: 0.17631, acc: 0.90625\n",
      "[Train]: Step: 90400, loss: 0.55163, acc: 0.84375\n",
      "[Train]: Step: 90500, loss: 0.26042, acc: 0.93750\n",
      "[Train]: Step: 90600, loss: 0.46918, acc: 0.87500\n",
      "[Train]: Step: 90700, loss: 0.32079, acc: 0.93750\n",
      "[Train]: Step: 90800, loss: 0.07822, acc: 1.00000\n",
      "[Train]: Step: 90900, loss: 0.21092, acc: 0.93750\n",
      "[Train]: Step: 91000, loss: 0.87696, acc: 0.90625\n",
      "[Train]: Step: 91100, loss: 0.38368, acc: 0.87500\n",
      "[Train]: Step: 91200, loss: 0.45138, acc: 0.84375\n",
      "[Train]: Step: 91300, loss: 0.30845, acc: 0.84375\n",
      "[Train]: Step: 91400, loss: 0.05969, acc: 0.96875\n",
      "[Train]: Step: 91500, loss: 0.43322, acc: 0.87500\n",
      "[Train]: Step: 91600, loss: 0.25248, acc: 0.90625\n",
      "[Train]: Step: 91700, loss: 0.74296, acc: 0.81250\n",
      "[Train]: Step: 91800, loss: 0.20477, acc: 0.93750\n",
      "[Train]: Step: 91900, loss: 0.44306, acc: 0.87500\n",
      "[Train]: Step: 92000, loss: 0.40944, acc: 0.87500\n",
      "[Train]: Step: 92100, loss: 0.20716, acc: 0.90625\n",
      "[Train]: Step: 92200, loss: 0.29736, acc: 0.90625\n",
      "[Train]: Step: 92300, loss: 0.06358, acc: 0.96875\n",
      "[Train]: Step: 92400, loss: 0.27167, acc: 0.87500\n",
      "[Train]: Step: 92500, loss: 0.27887, acc: 0.96875\n",
      "[Train]: Step: 92600, loss: 0.03530, acc: 1.00000\n",
      "[Train]: Step: 92700, loss: 0.75227, acc: 0.84375\n",
      "[Train]: Step: 92800, loss: 0.40930, acc: 0.93750\n",
      "[Train]: Step: 92900, loss: 0.54237, acc: 0.81250\n",
      "[Train]: Step: 93000, loss: 0.31540, acc: 0.93750\n",
      "[Train]: Step: 93100, loss: 0.11620, acc: 0.96875\n",
      "[Train]: Step: 93200, loss: 0.06020, acc: 1.00000\n",
      "[Train]: Step: 93300, loss: 0.11043, acc: 1.00000\n",
      "[Train]: Step: 93400, loss: 0.37080, acc: 0.87500\n",
      "[Train]: Step: 93500, loss: 0.30177, acc: 0.87500\n",
      "[Train]: Step: 93600, loss: 0.27532, acc: 0.87500\n",
      "[Train]: Step: 93700, loss: 0.34264, acc: 0.90625\n",
      "[Train]: Step: 93800, loss: 0.20976, acc: 0.90625\n",
      "[Train]: Step: 93900, loss: 0.13155, acc: 0.96875\n",
      "[Train]: Step: 94000, loss: 0.24481, acc: 0.90625\n",
      "[Train]: Step: 94100, loss: 0.32929, acc: 0.84375\n",
      "[Train]: Step: 94200, loss: 0.48159, acc: 0.87500\n",
      "[Train]: Step: 94300, loss: 0.32730, acc: 0.84375\n",
      "[Train]: Step: 94400, loss: 0.16358, acc: 0.93750\n",
      "[Train]: Step: 94500, loss: 0.29193, acc: 0.93750\n",
      "[Train]: Step: 94600, loss: 0.32201, acc: 0.90625\n",
      "[Train]: Step: 94700, loss: 0.24167, acc: 0.90625\n",
      "[Train]: Step: 94800, loss: 0.32461, acc: 0.90625\n",
      "[Train]: Step: 94900, loss: 0.53311, acc: 0.90625\n",
      "[Train]: Step: 95000, loss: 0.22633, acc: 0.93750\n",
      "[Test]: Step: 95000, acc: 0.65625\n",
      "[Train]: Step: 95100, loss: 0.50999, acc: 0.87500\n",
      "[Train]: Step: 95200, loss: 0.22083, acc: 0.93750\n",
      "[Train]: Step: 95300, loss: 0.04864, acc: 1.00000\n",
      "[Train]: Step: 95400, loss: 0.40539, acc: 0.87500\n",
      "[Train]: Step: 95500, loss: 0.33448, acc: 0.87500\n",
      "[Train]: Step: 95600, loss: 0.31223, acc: 0.87500\n",
      "[Train]: Step: 95700, loss: 0.23794, acc: 0.93750\n",
      "[Train]: Step: 95800, loss: 0.15804, acc: 0.93750\n",
      "[Train]: Step: 95900, loss: 0.26663, acc: 0.90625\n",
      "[Train]: Step: 96000, loss: 0.14507, acc: 0.93750\n",
      "[Train]: Step: 96100, loss: 0.10868, acc: 0.96875\n",
      "[Train]: Step: 96200, loss: 0.18308, acc: 0.96875\n",
      "[Train]: Step: 96300, loss: 0.91646, acc: 0.68750\n",
      "[Train]: Step: 96400, loss: 0.29915, acc: 0.90625\n",
      "[Train]: Step: 96500, loss: 0.12108, acc: 0.93750\n",
      "[Train]: Step: 96600, loss: 0.19892, acc: 0.93750\n",
      "[Train]: Step: 96700, loss: 0.30414, acc: 0.87500\n",
      "[Train]: Step: 96800, loss: 0.05904, acc: 1.00000\n",
      "[Train]: Step: 96900, loss: 0.29907, acc: 0.87500\n",
      "[Train]: Step: 97000, loss: 0.17206, acc: 0.93750\n",
      "[Train]: Step: 97100, loss: 0.49691, acc: 0.78125\n",
      "[Train]: Step: 97200, loss: 0.39099, acc: 0.87500\n",
      "[Train]: Step: 97300, loss: 0.03767, acc: 0.96875\n",
      "[Train]: Step: 97400, loss: 0.26247, acc: 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train]: Step: 97500, loss: 0.37255, acc: 0.84375\n",
      "[Train]: Step: 97600, loss: 0.45988, acc: 0.84375\n",
      "[Train]: Step: 97700, loss: 0.09515, acc: 0.96875\n",
      "[Train]: Step: 97800, loss: 0.44781, acc: 0.90625\n",
      "[Train]: Step: 97900, loss: 0.11440, acc: 0.96875\n",
      "[Train]: Step: 98000, loss: 1.36597, acc: 0.65625\n",
      "[Train]: Step: 98100, loss: 0.11504, acc: 0.93750\n",
      "[Train]: Step: 98200, loss: 0.49512, acc: 0.87500\n",
      "[Train]: Step: 98300, loss: 0.54757, acc: 0.84375\n",
      "[Train]: Step: 98400, loss: 0.36190, acc: 0.84375\n",
      "[Train]: Step: 98500, loss: 0.07650, acc: 1.00000\n",
      "[Train]: Step: 98600, loss: 0.30045, acc: 0.90625\n",
      "[Train]: Step: 98700, loss: 0.16539, acc: 0.93750\n",
      "[Train]: Step: 98800, loss: 0.45203, acc: 0.87500\n",
      "[Train]: Step: 98900, loss: 0.08738, acc: 0.96875\n",
      "[Train]: Step: 99000, loss: 0.16851, acc: 0.93750\n",
      "[Train]: Step: 99100, loss: 0.40106, acc: 0.84375\n",
      "[Train]: Step: 99200, loss: 0.28504, acc: 0.90625\n",
      "[Train]: Step: 99300, loss: 0.13728, acc: 0.93750\n",
      "[Train]: Step: 99400, loss: 0.24105, acc: 0.93750\n",
      "[Train]: Step: 99500, loss: 0.25234, acc: 0.87500\n",
      "[Train]: Step: 99600, loss: 0.28167, acc: 0.96875\n",
      "[Train]: Step: 99700, loss: 0.21732, acc: 0.93750\n",
      "[Train]: Step: 99800, loss: 0.16607, acc: 0.93750\n",
      "[Train]: Step: 99900, loss: 0.32394, acc: 0.87500\n",
      "[Train]: Step: 100000, loss: 0.30442, acc: 0.90625\n",
      "[Test]: Step: 100000, acc: 0.62500\n"
     ]
    }
   ],
   "source": [
    "steps = 100000\n",
    "\n",
    "with tf.Session(graph=graph, config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Tensorflow Initialized!')\n",
    "    \n",
    "    for step in range(steps):\n",
    "        batch_data, batch_labels = train_data.next_batch(32)\n",
    "        feed_dict = {input_data: batch_data, y: batch_labels}\n",
    "        #merge = tf.summary.merge_all()\n",
    "        _, loss, acc = sess.run([optimizer, _loss, accuracy], \n",
    "                                feed_dict=feed_dict)\n",
    "        if (step + 1) % 100 == 0:\n",
    "            print('[Train]: Step: %d, loss: %4.5f, acc: %4.5f' \n",
    "                  % (step+1, loss, acc))\n",
    "            \n",
    "        if (step + 1) % 5000 == 0:\n",
    "            test_input, test_labels = test_data.next_batch(32)\n",
    "            feed_dict = {input_data: test_input, y: test_labels}\n",
    "            _, test_acc = sess.run([correct_pred, accuracy], feed_dict=feed_dict)\n",
    "            #print(pred)\n",
    "            #print(type(test_acc))\n",
    "            print('[Test]: Step: %d, acc: %4.5f' \n",
    "                  % (step+1, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file contains helper functions for building the model and for \n",
    "loading model parameters. These helper functions are built to \n",
    "mirror those in the official TensorFlow implementation.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import math\n",
    "import collections\n",
    "from functools import partial\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import model_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the entire model.(stem, all blocks, and head.)\n",
    "GlobalParams = collections.namedtuple('GlobalParams', [\n",
    "    'batch_norm_momentum', 'batch_norm_epsilon', 'dropout_rate',\n",
    "    'num_classes', 'width_coefficient', 'depth_coefficient',\n",
    "    'depth_divisor', 'min_depth', 'drop_connect_rate', 'image_size'])\n",
    "\n",
    "# Parameters for an individual model block.\n",
    "BlockArgs = collections.namedtuple('BlockArgs', [\n",
    "    'kernel_size', 'num_repeat', 'input_filters',\n",
    "    'output_filters', 'expand_ratio', 'id_skip', 'stride', 'se_ratio'])\n",
    "\n",
    "# Change namedtuple defaults\n",
    "GlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\n",
    "BlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dStaticsSamePadding(nn.Module):\n",
    "    \"\"\"\n",
    "    The real keras/tensorflow conv2d same padding\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, bias=True, groups=1, dilation=1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride,\n",
    "                              bias=bias, groups=groups)\n",
    "        self.stride = self.conv.stride\n",
    "        self.kernel_size = self.conv.kernel_size\n",
    "        self.dilation = self.conv.dilation\n",
    "        \n",
    "        if isinstance(self.stride, int):\n",
    "            self.stride = [self.stride] * 2\n",
    "        elif len(self.stride) == 1:\n",
    "            self.stride = [self.stride[0]] * 2\n",
    "        \n",
    "        if isinstance(self.kernel_size, int):\n",
    "            self.kernel_size = [self.kernel_size] * 2\n",
    "        elif len(self.kernel_size) == 1:\n",
    "            self.kernel_size = [self.kernel_size[0]] * 2\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[-2:]\n",
    "        \n",
    "        extra_h = (math.ceil(w / self.stride[1]) - 1) * self.stride[1] - w + self.kernel_size[1]\n",
    "        extra_v = (math.ceil(h / self.stride[0]) - 1) * self.stride[0] - w + self.kernel_size[0]\n",
    "        \n",
    "        left = extra_h // 2\n",
    "        right = extra_h - left\n",
    "        top = extra_v // 2\n",
    "        bottom = extra_v - top\n",
    "        x = F.pad(x, [left, right, top, bottom])\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class MaxPool2dStaticSamePadding(nn.Module):\n",
    "    \"\"\"\n",
    "    The real keras/tensorflow MaxPool2d with same padding.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(*args, **kwargs)\n",
    "        self.stride = self.pool.stride\n",
    "        self.kernel_size = self.pool.kernel_size\n",
    "        \n",
    "        if isinstance(self.stride, int):\n",
    "            self.stride = [self.stride] * 2\n",
    "        elif len(self.stride) == 1:\n",
    "            self.stride = [self.stride[0]] * 2\n",
    "        \n",
    "        if isinstance(self.kernel_size, int):\n",
    "            self.kernel_size = [self.kernel_size] * 2\n",
    "        elif len(self.kernel_size) == 1:\n",
    "            self.kernel_size = [self.kernel_size[0]] * 2\n",
    "            \n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[-2:]\n",
    "        \n",
    "        extra_h = (math.ceil(w / self.stride[1]) - 1) * self.stride[1] - w + self.kernel_size[1]\n",
    "        extra_v = (math.ceil(h / self.stride[0]) - 1) * self.stride[0] - w + self.kernel_size[0]\n",
    "        \n",
    "        left = extra_h // 2\n",
    "        right = extra_h - left\n",
    "        top = extra_v // 2\n",
    "        bottom = extra_v - top\n",
    "        x = F.pad(x, [left, right, top, bottom])\n",
    "        \n",
    "        x = self.pool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dDynamicSamePadding(nn.Conv2d):\n",
    "    \"\"\" 2D Convolutions like TensorFlow, for a dynamic image size \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)\n",
    "        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]] * 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        ih, iw = x.size()[-2:]\n",
    "        kh, kw = self.weight.size()[-2:]\n",
    "        sh, sw = self.stride\n",
    "        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n",
    "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
    "        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            x = F.pad(x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2])\n",
    "        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "    \n",
    "    \n",
    "class Identity(nn.Module):\n",
    "    \n",
    "    def __init__(self,):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return input\n",
    "    \n",
    "    \n",
    "def efficientnet_params(model_name):\n",
    "    \"\"\" Map EfficientNet model name to parameter coefficients. \"\"\"\n",
    "    params_dict = {\n",
    "        # Coefficients:   width,depth,res,dropout\n",
    "        'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n",
    "        'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n",
    "        'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n",
    "        'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n",
    "        'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n",
    "        'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n",
    "        'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n",
    "        'efficientnet-b7': (2.0, 3.1, 600, 0.5),\n",
    "        'efficientnet-b8': (2.2, 3.6, 672, 0.5),\n",
    "        'efficientnet-l2': (4.3, 5.3, 800, 0.5),\n",
    "    }\n",
    "    return params_dict[model_name]\n",
    "\n",
    "\n",
    "class BlockDecoder(object):\n",
    "    \"\"\" Block Decoder for readability, straight from the official Tensorflow repository. \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def _decode_block_string(block_string):\n",
    "        \"\"\" Get a block through a string notation of arguments. \"\"\"\n",
    "        assert isinstance(block_string, str)\n",
    "        \n",
    "        ops = block_string.split('_')\n",
    "        options = {}\n",
    "        for op in ops:\n",
    "            splits = re.split(r'\\d.*', op)\n",
    "            if len(splits) >= 2:\n",
    "                key, value = splits[:2]\n",
    "                options[key] = value\n",
    "                \n",
    "        # Check stride\n",
    "        assert ('s' in options and len(options['s'] == 1) or\n",
    "                (len(options['s']) == 2 and options['s'][0] == options['s'][1]))\n",
    "        \n",
    "        return BlockArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwishImplementation(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i * torch.sigmoid(i)\n",
    "        ctx.save_for_backward(i)\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        i = ctx.saved_variables[0]\n",
    "        sigmoid_i = torch.sigmoid(i)\n",
    "        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n",
    "    \n",
    "    \n",
    "class MemoryEfficientSwish(nn.Module):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return SwishImplementation.apply(x)\n",
    "    \n",
    "\n",
    "class Swish(nn.Module):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "    \n",
    "    \n",
    "def round_filters(filters, global_params):\n",
    "    ''' Calculate and round number of filters based on depth multiplier. '''\n",
    "    multiplier = global_params.width_coefficient\n",
    "    if not multiplier:\n",
    "        return filters\n",
    "    divisor = global_params.depth_divisor\n",
    "    min_depth = global_params.min_depth\n",
    "    filters *= multiplier\n",
    "    min_depth = min_depth or divisor\n",
    "    new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)\n",
    "    if new_filters < 0.9 * filters:\n",
    "        new_filters += divisor\n",
    "    return int(new_filters)\n",
    "\n",
    "\n",
    "def round_repeats(repeats, global_params):\n",
    "    ''' Round number of filters based on depth multiplier. '''\n",
    "    multiplier = global_params.depth_coefficient\n",
    "    if not multiplier:\n",
    "        return repeats\n",
    "    return int(math.ceil(multiplier * repeats))\n",
    "\n",
    "\n",
    "def drop_connect(inputs, p, training):\n",
    "    \"\"\" Drop connect. \"\"\"\n",
    "    if not training: return inputs\n",
    "    batch_size = inputs.shape[0]\n",
    "    keep_prob = 1 - p\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype, device=inputs.device)\n",
    "    binary_tensor = torch.floor(random_tensor)\n",
    "    output = inputs / keep_prob * binary_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_same_padding_conv2d(image_size=None):\n",
    "    \"\"\" Chooses static padding if you have specified an image size, and dynamic padding otherwise.\n",
    "        Static padding is necessary for ONNX exporting of models. \"\"\"\n",
    "    if image_size is None:\n",
    "        return Conv2dDynamicSamePadding\n",
    "    else:\n",
    "        return partial(Conv2dStaticSamePadding, image_size=image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

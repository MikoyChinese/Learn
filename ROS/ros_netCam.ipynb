{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/usr/lib/python2.7/dist-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rospy\n",
    "import cv2\n",
    "import json\n",
    "from sensor_msgs.msg import Image\n",
    "from std_msgs.msg import String\n",
    "from cv_bridge import CvBridge, CvBridgeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DaHuaNetCam:\n",
    "    \n",
    "    def __init__(self, user, pwd, ip, channel):\n",
    "        # Default parameters, can be set.\n",
    "        self._fps = 30\n",
    "        self._netCamId = 1\n",
    "        self._img_size = (1280, 720)\n",
    "        # Get the value from config.\n",
    "        self.user = user\n",
    "        self.pwd = pwd\n",
    "        self.ip = ip\n",
    "        self.channel = channel\n",
    "    \n",
    "    def openNetCam(self):\n",
    "        import cv2\n",
    "        # NetCam URL:\n",
    "        url = 'rtsp://%s:%s@%s/cam/realmonitor?channel=%d&subtype=0' \\\n",
    "              % (self.user, self.pwd, self.ip, self.channel)\n",
    "        # netCam = cv2.VideoCapture(url)\n",
    "        netCam = cv2.VideoCapture(0)\n",
    "        if netCam.isOpened():\n",
    "            return netCam\n",
    "        raise Exception(\"NetCam can't open! [NetCam URL]: %s\" % url)\n",
    "    \n",
    "    def set_fps(self, fps):\n",
    "        self._fps = fps\n",
    "        \n",
    "    def set_netCamId(self, netCamId):\n",
    "        self._netCamId = netCamId\n",
    "        \n",
    "    def set_img_size(self, img_size):\n",
    "        self._img_size = img_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetCam_Converter:\n",
    "    \n",
    "    def __init__(self, image_topic):\n",
    "        self.image_pub = rospy.Publisher(image_topic, Image)\n",
    "        self.bridge = CvBridge()\n",
    "    \n",
    "    def publish(self, image):\n",
    "        try:\n",
    "            image = self.bridge.cv2_to_imgmsg(image, 'bgr8')\n",
    "            self.image_pub.publish(image)\n",
    "        except CvBridgeError as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tf_pose\n",
    "from tf_pose import common\n",
    "print(common.__file__)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user, pwd, ip, channel = ['admin', 'admin2019', '172.16.1.180', 1]\n",
    "netCam = DaHuaNetCam(user, pwd, ip, channel)\n",
    "netCap = netCam.openNetCam()\n",
    "\n",
    "while netCap.isOpened():\n",
    "    ret, frame = netCap.read()\n",
    "    if ret:\n",
    "        cv2.imshow(\"NetCam Frame\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "netCap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0705 18:01:26.456783 140565966112576 deprecation_wrapper.py:119] From /home/commaai-03/Mikoy/work/open_ptrack/tf-pose-estimation/tf_pose/mobilenet/mobilenet.py:369: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging, time\n",
    "import easydict\n",
    "\n",
    "from tf_pose import common\n",
    "from tf_pose.estimator import TfPoseEstimator\n",
    "from tf_pose.networks import model_wh\n",
    "\n",
    "logger = logging.getLogger('TfPoseEstimatorRun')\n",
    "logger.handlers.clear()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "    \"model\": \"/home/commaai-03/Mikoy/work/open_ptrack/tf-pose-estimation/models/graph/mobilenet_thin/graph_opt.pb\",\n",
    "    \"resize\": \"432x368\",\n",
    "    \"resize_out_ratio\": 4.0,\n",
    "    \"folder\": \"/home/commaai-03/Mikoy/work/open_ptrack/tf-pose-estimation/huapu_imgs/\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-05 18:01:40,754] [TfPoseEstimator] [INFO] loading graph from /home/commaai-03/Mikoy/work/open_ptrack/tf-pose-estimation/models/graph/mobilenet_thin/graph_opt.pb(default size=432x368)\n"
     ]
    }
   ],
   "source": [
    "w, h = model_wh(args.resize)\n",
    "if w == 0 or h == 0:\n",
    "    e = TfPoseEstimator(args.model, target_size=(432, 368))\n",
    "else:\n",
    "    e = TfPoseEstimator(args.model, target_size=(w, h))\n",
    "    \n",
    "def draw_boxes(human, npimg, image_w, image_h):\n",
    "    face_box = human.get_face_box(image_w, image_h)\n",
    "    upper_body_box= human.get_upper_body_box(image_w, image_h)\n",
    "\n",
    "    # face_box\n",
    "    if face_box:\n",
    "        x1, y1 = int(face_box['x']-face_box['w']/2), int(face_box['y']-face_box['h']/2)\n",
    "        x2, y2 = int(face_box['x']+face_box['w']/2), int(face_box['y']+face_box['h']/2)\n",
    "        cv2.rectangle(npimg, (x1,y1),(x2,y2),(0,255,0), 2)\n",
    "    # upper_body_box\n",
    "    if upper_body_box:\n",
    "        x3, y3 = int(upper_body_box['x'] - upper_body_box['w']/2), int(upper_body_box['y'] - upper_body_box['h']/2)\n",
    "        x4, y4 = int(upper_body_box['x'] + upper_body_box['w']/2), int(upper_body_box['y'] + upper_body_box['h']/2)\n",
    "        cv2.rectangle(npimg, (x3,y3),(x4,y4),(0,0,255), 2)\n",
    "    return npimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user, pwd, ip, channel = ['admin', 'admin2019', '172.16.1.180', 1]\n",
    "netCam = DaHuaNetCam(user, pwd, ip, channel)\n",
    "netCap = netCam.openNetCam()\n",
    "netCap.set(3, 1280)\n",
    "netCap.set(4, 720)\n",
    "\n",
    "switch = False\n",
    "\n",
    "while netCap.isOpened():\n",
    "    ret, frame = netCap.read()\n",
    "    if ret:\n",
    "        tf_pose_msgs = easydict.EasyDict({'header': {'stamp': {'secs': 0, 'nsecs': 0}, 'frame_id': ''}, 'poses': []})\n",
    "        t = rospy.Time.now()\n",
    "        tf_pose_msgs.header.stamp.secs, tf_pose_msgs.header.stamp.nsecs = t.secs, t.nsecs\n",
    "        image_h, image_w = frame.shape[:2]\n",
    "        humans = e.inference(frame, resize_to_default=(w > 0 and h > 0),\n",
    "                             upsample_size=args.resize_out_ratio)\n",
    "        \n",
    "        tf_pose_msgs = get_msg(tf_pose_msgs, humans, image_w, image_h)\n",
    "        tf_pose_msgs = json.dumps(tf_pose_msgs)\n",
    "        pub.publish(tf_pose_msgs)\n",
    "        \n",
    "        #elapsed = rospy.Time.now() - t\n",
    "        #logger.info('inference image in %.4f seconds.' % elapsed)\n",
    "        \n",
    "        if switch:\n",
    "            for human in humans:\n",
    "                draw_boxes(human, frame, image_w, image_h)\n",
    "            cv2.imshow('ROS Face And Body Box', frame)\n",
    "        else:\n",
    "            frame = TfPoseEstimator.draw_humans(frame, humans, imgcopy=False)\n",
    "            cv2.imshow(\"TF_PoseEstimator Frame\", frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "netCap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Face Box And Upper Body Box Ros Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_msg(tf_pose_msgs, humans, image_w, image_h, need_handle=False):\n",
    "    \n",
    "    for human in humans:\n",
    "        tf_pose_msg = {'face_box': None, 'upper_body_box': None}\n",
    "        face_box = human.get_face_box(image_w,image_h)\n",
    "        upper_body_box= human.get_upper_body_box(image_w, image_h)\n",
    "        if not need_handle:\n",
    "            if face_box:\n",
    "                tf_pose_msg['face_box'] = face_box\n",
    "            if upper_body_box:\n",
    "                tf_pose_msg['upper_body_box'] = upper_body_box\n",
    "        else:\n",
    "            # face_box\n",
    "            if face_box:\n",
    "                x1, y1 = int(face_box['x']-face_box['w']/2), int(face_box['y']-face_box['h']/2)\n",
    "                x2, y2 = int(face_box['x']+face_box['w']/2), int(face_box['y']+face_box['h']/2)\n",
    "                tf_pose_msg['face_box'] = {'x1y1': (x1,y1), 'x2y2': (x2,y2)}\n",
    "            # upper_body_box\n",
    "            if upper_body_box:\n",
    "                x1, y1 = int(upper_body_box['x'] - upper_body_box['w']/2), int(upper_body_box['y'] - upper_body_box['h']/2)\n",
    "                x2, y2 = int(upper_body_box['x'] + upper_body_box['w']/2), int(upper_body_box['y'] + upper_body_box['h']/2)\n",
    "                tf_pose_msg['face_box'] = {'x1y1': (x1,y1), 'x2y2': (x2,y2)}\n",
    "\n",
    "        tf_pose_msgs.poses.append(tf_pose_msg)\n",
    "    return tf_pose_msgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ros Msg Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwhile not rospy.is_shutdown():\\n   pub.publish(tf_pose_msgs)\\n   r.sleep()\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import rospy\n",
    "from std_msgs.msg import String\n",
    "\n",
    "pub = rospy.Publisher('Test', String, queue_size=10)\n",
    "rospy.init_node('TF_POSE')\n",
    "r = rospy.Rate(10) # 10hz\n",
    "\n",
    "'''\n",
    "while not rospy.is_shutdown():\n",
    "   pub.publish(tf_pose_msgs)\n",
    "   r.sleep()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
